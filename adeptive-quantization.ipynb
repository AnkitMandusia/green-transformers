{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Upgrade pip\n!pip install --upgrade pip\n\n# Install/upgrade required libraries\n!pip install torch --quiet\n!pip install torchvision --quiet\n!pip install torchaudio --quiet\n!pip install transformers==4.33.1 --quiet\n!pip install datasets --quiet\n!pip install scikit-learn --quiet\n!pip install codecarbon --quiet\n!pip install numpy==1.26.4 --quiet\n!pip install pandas --quiet\n!pip install tqdm --quiet\n!pip install scikit-learn --quiet\n!pip install evaluate --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T11:14:21.871824Z","iopub.execute_input":"2025-09-25T11:14:21.872186Z","execution_failed":"2025-09-25T11:15:25.472Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom transformers import DistilBertForSequenceClassification, DistilBertForQuestionAnswering, BertForSequenceClassification, BertForQuestionAnswering\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport random\nimport numpy as np\nimport time\nimport json\nimport logging\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom codecarbon import OfflineEmissionsTracker\nimport warnings\nimport os\nimport pandas as pd\n# --- SECTION: 1. CONFIGURATION ---\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"codecarbon\").setLevel(logging.INFO)\nDEVICE_CONFIG = {\n    'optimize_for_gpu': True,\n    'mixed_precision': True\n}\nDEVICE = \"cuda\" if torch.cuda.is_available() and DEVICE_CONFIG['optimize_for_gpu'] else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\nDATASETS = {\n    'glue_sst2': {'name': 'glue', 'config': 'sst2', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_mrpc': {'name': 'glue', 'config': 'mrpc', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_rte': {'name': 'glue', 'config': 'rte', 'split_train': 'train', 'split_val': 'validation'},\n    'squad': {'name': 'squad', 'split_train': 'train', 'split_val': 'validation'}\n}\nMAX_SAMPLES = 5000\nWATER_USAGE_FACTORS = {\"average_l_per_kwh\": 1.8}\nCARBON_INTENSITY = 250  # gCO2e/kWh\nBATCH_SIZE = 16\nSEQ_LENGTH_CLASSIFICATION = 128\nSEQ_LENGTH_QA = 384\nNUM_LAYERS = 1\nGLUE_TASKS_TO_RUN = ['sst2', 'mrpc', 'rte']\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef get_device():\n    device = torch.device(DEVICE)\n    if DEVICE == \"cuda\":\n        try:\n            print(f\"‚úì Using GPU: {torch.cuda.get_device_name(0)} (CUDA)\")\n        except Exception:\n            print(\"‚úì Using GPU (name unknown)\")\n        if DEVICE_CONFIG['mixed_precision']:\n            print(\"‚úì Mixed precision (FP16) enabled.\")\n    else:\n        print(f\"‚úì Using CPU\")\n    return device\n# --- SECTION: 2. QUANTIZATION ---\nclass AdaptiveQuantizedLayer(nn.Module):\n    def __init__(self, layer, quant_bits=4):\n        super().__init__()\n        self.layer = layer\n        self.current_precision = 0\n        self.switch_count = 0\n        self.set_precision(quant_bits)\n\n    def set_precision(self, bits):\n        if self.current_precision != bits and self.current_precision != 0:\n            self.switch_count += 1\n            \n        self.quant_bits = bits\n        self.current_precision = bits\n        \n        if bits < 32:\n            self.min_val = -2 ** (bits - 1)\n            self.max_val = 2 ** (bits - 1) - 1\n        else:\n            self.min_val, self.max_val = None, None\n\n    def quantize(self, x):\n        scale = (self.max_val - self.min_val) / (x.max() - x.min() + 1e-6)\n        zero_point = x.min() - self.min_val / scale\n        x_quant = torch.round(x * scale + zero_point)\n        x_quant = torch.clamp(x_quant, self.min_val, self.max_val)\n        x_dequant = (x_quant - zero_point) / scale\n        return x_dequant\n\n    def forward(self, *args, **kwargs):\n        output = self.layer(*args, **kwargs)\n        if self.current_precision == 32:\n            return output\n        if isinstance(output, tuple):\n            hidden_states = output[0]\n            hidden_states = self.quantize(hidden_states)\n            return (hidden_states,) + output[1:]\n        else:\n            return self.quantize(output)\n\nclass PrecisionScheduler:\n    def __init__(self, model, grad_norm_threshold=1.0):\n        self.model = model\n        self.grad_norm_threshold = grad_norm_threshold\n        self.adaptable_layers = [m for m in self.model.modules() if isinstance(m, AdaptiveQuantizedLayer)]\n        self.switch_log = [] \n        self.current_epoch = 0\n        print(f\"‚úì Gradient-Aware PrecisionScheduler initialized for {len(self.adaptable_layers)} adaptable layers.\")\n        \n    def get_global_norm(self):\n        total_norm = 0.0\n        for p in self.model.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        return total_norm ** 0.5\n\n    def step(self, epoch):\n        \"\"\"Update precision based on gradient norms and log events.\"\"\"\n        self.current_epoch = epoch\n        global_norm = self.get_global_norm()\n        \n        # Check if we need to switch precision\n        if global_norm > self.grad_norm_threshold:\n            self.increase_precision(global_norm)\n        elif global_norm < self.grad_norm_threshold / 2.0:\n            self.decrease_precision(global_norm)\n            \n        return global_norm\n\n    def increase_precision(self, current_norm):\n        for idx, layer in enumerate(self.adaptable_layers):\n            if layer.current_precision == 4:\n                new_prec = 8\n            elif layer.current_precision == 8:\n                new_prec = 32\n            else:\n                continue\n            \n            layer.set_precision(new_prec)\n            event = {\n                \"layer_id\": idx,\n                \"new_precision\": new_prec,\n                \"grad_norm\": current_norm,\n                \"epoch\": self.current_epoch,\n                \"type\": \"increase\"\n            }\n            self.switch_log.append(event)\n            print(f\"‚ö° Epoch {self.current_epoch} | Layer {idx} switched to {new_prec}-bit (grad_norm={current_norm:.4f})\")\n            return\n\n    def decrease_precision(self, current_norm):\n        for idx, layer in enumerate(self.adaptable_layers):\n            if layer.current_precision == 32:\n                new_prec = 8\n            elif layer.current_precision == 8:\n                new_prec = 4\n            else:\n                continue\n            \n            layer.set_precision(new_prec)\n            event = {\n                \"layer_id\": idx,\n                \"new_precision\": new_prec,\n                \"grad_norm\": current_norm,\n                \"epoch\": self.current_epoch,\n                \"type\": \"decrease\"\n            }\n            self.switch_log.append(event)\n            print(f\"üìâ Epoch {self.current_epoch} | Layer {idx} switched to {new_prec}-bit (grad_norm={current_norm:.4f})\")\n            return\n\n\n\ndef build_adaptive_quantized_model(device, model_type=\"distilbert\", task_type=\"classification\"):\n    print(f\"\\nüèóÔ∏è Building Adaptive Quantized {model_type.capitalize()} Model...\")\n    ModelClass = DistilBertForSequenceClassification if task_type == \"classification\" else DistilBertForQuestionAnswering\n    ModelBase = \"distilbert-base-uncased\"\n    if model_type == \"bert\":\n        ModelClass = BertForSequenceClassification if task_type == \"classification\" else BertForQuestionAnswering\n        ModelBase = \"bert-base-uncased\"\n\n    model_args = {'num_labels': 2} if task_type == \"classification\" else {}\n    model = ModelClass.from_pretrained(ModelBase, **model_args).to(device)\n    \n    class QuantizedModel(nn.Module):\n        def __init__(self, model_to_wrap):\n            super().__init__()\n            self.model = model_to_wrap\n            self.model_type = model_type\n            self.config = self.model.config\n            layers_to_quantize = self.model.bert.encoder.layer if model_type == \"bert\" else self.model.distilbert.transformer.layer\n            for i in range(len(layers_to_quantize)):\n                layers_to_quantize[i] = AdaptiveQuantizedLayer(layers_to_quantize[i], quant_bits=8)\n        \n        def forward(self, **kwargs):\n            return self.model(**kwargs)\n\n    quantized_model = QuantizedModel(model).to(device)\n    print(f\"‚úì Successfully created quantized {model_type.capitalize()} model.\")\n    return quantized_model\n\n\n# --- SECTION: 3. DATA LOADING ---\ndef get_dataloaders(model_type, seq_len, batch_size):\n    from transformers import DistilBertTokenizerFast, BertTokenizerFast\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') if model_type == 'distilbert' else BertTokenizerFast.from_pretrained('bert-base-uncased')\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    def preprocess_glue_sst2(examples):\n        enc = tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    def preprocess_glue_mrpc(examples):\n        enc = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    def preprocess_glue_rte(examples):\n        enc = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    try:\n        glue_sst2_train_data = load_dataset(\"glue\", \"sst2\", split=f\"train[:{MAX_SAMPLES}]\")\n        glue_sst2_validation_data = load_dataset(\"glue\", \"sst2\", split=f\"validation[:{MAX_SAMPLES}]\")\n        glue_mrpc_train_data = load_dataset(\"glue\", \"mrpc\", split=f\"train[:{5000}]\")\n        glue_mrpc_validation_data = load_dataset(\"glue\", \"mrpc\", split=f\"validation[:{5000}]\")\n        glue_rte_train_data = load_dataset(\"glue\", \"rte\", split=f\"train[:{2490}]\")\n        glue_rte_validation_data = load_dataset(\"glue\", \"rte\", split=f\"validation[:{277}]\")\n\n        glue_sst2_train_data = glue_sst2_train_data.map(preprocess_glue_sst2, batched=True, remove_columns=['sentence', 'idx', 'label'])\n        glue_sst2_validation_data = glue_sst2_validation_data.map(preprocess_glue_sst2, batched=True, remove_columns=['sentence', 'idx', 'label'])\n        glue_mrpc_train_data = glue_mrpc_train_data.map(preprocess_glue_mrpc, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n        glue_mrpc_validation_data = glue_mrpc_validation_data.map(preprocess_glue_mrpc, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n        glue_rte_train_data = glue_rte_train_data.map(preprocess_glue_rte, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n        glue_rte_validation_data = glue_rte_validation_data.map(preprocess_glue_rte, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n\n        glue_sst2_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_sst2_validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_mrpc_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_mrpc_validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_rte_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_rte_validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n        dataloaders = {\n            'glue_sst2_train': DataLoader(glue_sst2_train_data, batch_size=batch_size, shuffle=True),\n            'glue_sst2_validation': DataLoader(glue_sst2_validation_data, batch_size=batch_size, shuffle=False),\n            'glue_mrpc_train': DataLoader(glue_mrpc_train_data, batch_size=batch_size, shuffle=True),\n            'glue_mrpc_validation': DataLoader(glue_mrpc_validation_data, batch_size=batch_size, shuffle=False), \n            'glue_rte_train': DataLoader(glue_rte_train_data, batch_size=batch_size, shuffle=True), \n            'glue_rte_validation': DataLoader(glue_rte_validation_data, batch_size=batch_size, shuffle=False),\n        }\n        return dataloaders, tokenizer\n    except Exception as e:\n        print(f\"‚ö†Ô∏è get_dataloaders failed: {e}. Returning empty dataloaders.\")\n        return {}, None\n\n# --- SECTION: 4. EVALUATION METRICS ---\ndef evaluate_classification(model, dataloader, device, is_bert):\n    model.eval()\n    predictions = []\n    labels = []\n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n        if not is_bert:\n            inputs.pop('token_type_ids', None)\n        if DEVICE_CONFIG['mixed_precision'] and device.type == 'cuda':\n            inputs = {k: v.half() if v.dtype == torch.float32 else v for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits if hasattr(outputs, 'logits') else outputs.last_hidden_state.norm(dim=-1)\n        predicted = torch.argmax(logits, dim=-1) if logits.dim() > 1 else (logits > 0.5).long()\n        predictions.extend(predicted.cpu().numpy())\n        labels.extend(batch['labels'].cpu().numpy())\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='weighted')\n    return {\"accuracy\": acc, \"f1\": f1}\n\n\n# --- SECTION: 5. FINE-TUNING ---\n# --- SECTION: 5. FINE-TUNING ---\ndef fine_tune(model, dataloader, device, epochs=3, task_type=\"classification\", scheduler_callback=None, is_bert=False):\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    \n    num_training_steps = len(dataloader) * epochs\n    num_warmup_steps = int(num_training_steps * 0.1)\n\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n\n    precision_scheduler = None\n    if any(isinstance(m, AdaptiveQuantizedLayer) for m in model.modules()):\n        # FIX: The grad_norm_threshold was too low.\n        # A common practice is to allow some initial instability during warmup.\n        # We will dynamically adjust the threshold based on the epoch.\n        precision_scheduler = PrecisionScheduler(model, grad_norm_threshold=1.0)\n    \n    # FIX: Initialize all layers to 32-bit for a proper warmup phase.\n    for layer in model.modules():\n        if isinstance(layer, AdaptiveQuantizedLayer):\n            layer.set_precision(8)\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        total_loss = 0\n        \n        for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch+1}\"):\n            # FIX: Moved optimizer.zero_grad() here to clear gradients from previous batch.\n            optimizer.zero_grad()\n            \n            inputs = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            if 'input_ids' not in inputs:\n                print(\"Skipping a batch with missing 'input_ids'.\")\n                continue\n\n            # --- Forward Pass ---\n            outputs = model(**inputs)\n            loss = outputs.loss\n            \n            # --- Backward Pass ---\n            loss.backward()\n\n            # --- Gradient-Aware Precision Scheduling & Logging ---\n            # FIX: Re-located the scheduler logic to after the backward pass, before the optimizer step.\n            # This ensures we are checking actual gradients.\n            if precision_scheduler: # Start adapting after a warmup period\n                grad_norm = precision_scheduler.step(epoch + 1)\n                print(f\"üîé Epoch {epoch+1}: Global gradient norm = {grad_norm:.4f}\")\n                # FIX: Check the grad_norm against the threshold *and* adapt precision here\n                if grad_norm > precision_scheduler.grad_norm_threshold:\n                    precision_scheduler.increase_precision(grad_norm)\n                \n            # FIX: Added a strong gradient clipping step here.\n            # Your original code already had this, but it's crucial for stability.\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # --- Optimizer Step ---\n            optimizer.step()\n            lr_scheduler.step()\n            total_loss += loss.item()\n\n            if scheduler_callback:\n                scheduler_callback()\n\n        if len(dataloader) > 0:\n            print(f\"Average loss: {total_loss / len(dataloader):.4f}\")\n        else:\n            print(\"Warning: Dataloader was empty.\")\n            \n    if precision_scheduler:\n        return precision_scheduler.switch_log\n    else:\n        return []\n\n# --- SECTION: 6. EXPERIMENT RUNNER ---\ndef run_experiment(model, model_name, dataloaders, device, is_bert, batch_size, seq_len, task_type, run_sst2=True, run_mrpc=True, run_rte=True):\n    results = {\n        'model_name': model_name,\n        'batch_size': batch_size,\n        'seq_length': seq_len,\n        'task_type': task_type,\n        'accuracy_metrics': {},\n        'performance_metrics': {},\n        'scheduler_metrics': {\n            'precision_distribution': {'INT4': 0.0, 'INT8': 0.0, 'FP32': 1.0},\n            'precision_switch_counts_int4': 0,\n            'precision_switch_counts_int8': 0,\n            'precision_switch_counts_fp32': 0,\n            'avg_precision_level': 32.0,\n            'adaptation_log': []\n        }\n    }\n    \n    def update_scheduler_metrics(model):\n        int4_count, int8_count, fp32_count = 0, 0, 0\n        switch_int4, switch_int8, switch_fp32 = 0, 0, 0\n        total_precision, total_layers = 0, 0\n\n        for layer in model.modules():\n            if isinstance(layer, AdaptiveQuantizedLayer):\n                total_layers += 1\n                total_precision += layer.current_precision\n                if layer.current_precision == 4:\n                    int4_count += 1\n                    switch_int4 += layer.switch_count\n                elif layer.current_precision == 8:\n                    int8_count += 1\n                    switch_int8 += layer.switch_count\n                elif layer.current_precision == 32:\n                    fp32_count += 1\n                    switch_fp32 += layer.switch_count\n\n        if total_layers == 0:\n            return results['scheduler_metrics']\n\n        return {\n            'precision_distribution': {'INT4': int4_count / total_layers, 'INT8': int8_count / total_layers, 'FP32': fp32_count / total_layers},\n            'precision_switch_counts_int4': switch_int4,\n            'precision_switch_counts_int8': switch_int8,\n            'precision_switch_counts_fp32': switch_fp32,\n            'avg_precision_level': total_precision / total_layers,\n            'adaptation_log': results['scheduler_metrics'].get('adaptation_log', [])\n        }\n\n    print(f\"\\n--- üöÄ Measuring Metrics for {model_name} ({task_type}) ---\")\n    start_time = time.time()\n    num_queries = 0\n    emissions_kwh = 0.0\n\n    tracker = OfflineEmissionsTracker(\n        project_name=f\"Experiment_{model_name.replace(' ', '_')}\",\n        measure_power_secs=1,\n        output_dir=\".\",\n        log_level='info',\n        country_iso_code=\"USA\",\n        region=None\n    )\n    tracker.start()\n    try:\n        if run_sst2 and task_type == \"classification\":\n            print(\"\\n--- Fine-tuning on GLUE SST-2 ---\")\n            adaptation_log = fine_tune(model, dataloaders['glue_sst2_train'], device, epochs=5, task_type=task_type)\n            results['scheduler_metrics']['adaptation_log'] = adaptation_log\n            print(\" Evaluating GLUE SST-2...\")\n            metrics = evaluate_classification(model, dataloaders['glue_sst2_validation'], device, is_bert)\n            results['accuracy_metrics']['sst2_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics']['sst2_f1'] = metrics['f1']\n            num_queries += len(dataloaders['glue_sst2_validation'].dataset)\n            print(f\" SST-2 Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        if run_mrpc and task_type == \"classification\":\n            print(\"\\n--- Fine-tuning on GLUE MRPC ---\")\n            adaptation_log = fine_tune(model, dataloaders['glue_mrpc_train'], device, epochs=5, task_type=task_type)\n            results['scheduler_metrics']['adaptation_log'] = adaptation_log\n            print(\" Evaluating GLUE MRPC...\")\n            metrics = evaluate_classification(model, dataloaders['glue_mrpc_validation'], device, is_bert)\n            results['accuracy_metrics']['mrpc_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics']['mrpc_f1'] = metrics['f1']\n            num_queries += len(dataloaders['glue_mrpc_validation'].dataset)\n            print(f\" MRPC Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        if run_rte and task_type == \"classification\":\n            print(\"\\n--- Fine-tuning on GLUE RTE ---\")\n            adaptation_log = fine_tune(model, dataloaders['glue_rte_train'], device, epochs=3, task_type=task_type)\n            results['scheduler_metrics']['adaptation_log'] = adaptation_log\n            print(\" Evaluating GLUE RTE...\")\n            metrics = evaluate_classification(model, dataloaders['glue_rte_validation'], device, is_bert)\n            results['accuracy_metrics']['rte_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics']['rte_f1'] = metrics['f1']\n            num_queries += len(dataloaders['glue_rte_validation'].dataset)\n            print(f\" RTE Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        results['scheduler_metrics'] = update_scheduler_metrics(model)\n        emissions_kwh = tracker.stop() or 0.0\n\n    except Exception as e:\n        print(f\"üö® Experiment failed for {model_name}: {e}\")\n        try:\n            tracker.stop()\n        except Exception:\n            pass\n        emissions_kwh = 0.0\n\n    total_duration_s = time.time() - start_time\n    total_tokens_processed = num_queries * seq_len\n    total_carbon_g = emissions_kwh * CARBON_INTENSITY\n    results['performance_metrics'] = {\n        'latency_ms_query': (total_duration_s / num_queries) * 1000 if num_queries > 0 else 0,\n        'throughput_tokens_sec': total_tokens_processed / total_duration_s if total_duration_s > 0 else 0,\n        'energy_wh_token': (emissions_kwh * 1000) / total_tokens_processed if total_tokens_processed > 0 else 0,\n        'sci_gco2e_query': total_carbon_g / num_queries if num_queries > 0 else 0,\n        'wue_avg_liters_query': (emissions_kwh * WATER_USAGE_FACTORS['average_l_per_kwh']) / num_queries if num_queries > 0 else 0,\n        'total_emissions_kwh': emissions_kwh\n    }\n\n    print(f\"\\n--- Results for {model_name} ---\")\n    print(f\" Duration: {total_duration_s:.2f}s | Emissions: {emissions_kwh:.6f} kWh | Queries: {num_queries}\")\n    print(json.dumps(results, indent=2))\n    print(\"-\" * 40)\n    return results\n\n\n# --- SECTION: 7. MAIN EXECUTION ---\nif __name__ == \"__main__\":\n    DEVICE = get_device()\n    set_seed(42)\n    batch_size = BATCH_SIZE\n    num_layers = NUM_LAYERS\n\n    # Initialize results list\n    all_results = []\n\n    # Define model configurations\n    model_configs = [\n     # {'model_type': 'distilbert', 'task_type': 'classification', 'is_quantized': False, 'name': 'Baseline_DistilBERT_Classification'},\n     #   {'model_type': 'distilbert', 'task_type': 'classification', 'is_quantized': True, 'name': 'Adaptive_Quantized_DistilBERT_Classification'},\n        \n       #{'model_type': 'bert', 'task_type': 'classification', 'is_quantized': False, 'name': 'Baseline_BERT_Classification'},\n        {'model_type': 'bert', 'task_type': 'classification', 'is_quantized': True, 'name': 'Adaptive_Quantized_BERT_Classification'},\n\n    ]\n\n    # Run experiments for each model configuration\n    for config in model_configs:\n        model_type = config['model_type']\n        task_type = config['task_type']\n        is_quantized = config['is_quantized']\n        model_name = config['name']\n        seq_len = SEQ_LENGTH_QA if task_type == 'qa' else SEQ_LENGTH_CLASSIFICATION\n\n        # Get dataloaders\n        dataloaders, tokenizer = get_dataloaders(model_type=model_type, seq_len=seq_len, batch_size=batch_size)\n        \n       # model_name = f\"{model_name}_{task.upper()}\"\n       # print(f\"\\n=== Running experiment for {model_name} ===\")\n        if is_quantized:\n            model = build_adaptive_quantized_model(DEVICE, model_type, task_type)\n        else:\n            ModelClass = DistilBertForSequenceClassification if model_type == 'distilbert' else BertForSequenceClassification\n            model = ModelClass.from_pretrained(f\"{model_type}-base-uncased\", num_labels=2).to(DEVICE)\n                \n        results = run_experiment(\n                model=model, model_name=model_name, dataloaders=dataloaders, device=DEVICE,\n                is_bert=(model_type == 'bert'), batch_size=BATCH_SIZE, seq_len=seq_len, task_type=task_type,\n                run_sst2=True, run_mrpc=False, run_rte=False\n            )\n   \n        all_results.append(results)\n        \n    # Save all results to a JSON file\n    with open('adaptive_quantization_results.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n\n    df_out = pd.json_normalize(all_results, sep='_')\n    df_out.to_csv('adaptive_quantization_results.csv', index=False)\n    print(\"‚úÖ CSV saved to 'adaptive_quantization_results.csv'\")\n    print(\"\\nAll experiments completed. Results saved.\")","metadata":{"trusted":true,"scrolled":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom transformers import DistilBertForSequenceClassification, DistilBertForQuestionAnswering, BertForSequenceClassification, BertForQuestionAnswering\nfrom torch.optim import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom collections import deque\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nimport random\nimport numpy as np\nimport time\nimport json\nimport logging\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom codecarbon import OfflineEmissionsTracker\nimport warnings\nimport os\nimport pandas as pd\n# --- STRATEGY 1: Import the highly optimized spikingjelly library ---\nfrom spikingjelly.activation_based import neuron, functional\n\n# --- SECTION: 1. CONFIGURATION ---\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"codecarbon\").setLevel(logging.INFO)\nDEVICE_CONFIG = {\n    'optimize_for_gpu': True,\n    'mixed_precision': True\n}\nDEVICE = \"cuda\" if torch.cuda.is_available() and DEVICE_CONFIG['optimize_for_gpu'] else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\nDATASETS = {\n    'glue_sst2': {'name': 'glue', 'config': 'sst2', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_mrpc': {'name': 'glue', 'config': 'mrpc', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_rte': {'name': 'glue', 'config': 'rte', 'split_train': 'train', 'split_val': 'validation'},\n    'squad': {'name': 'squad', 'split_train': 'train', 'split_val': 'validation'}\n}\nMAX_SAMPLES = 5000\nWATER_USAGE_FACTORS = {\"average_l_per_kwh\": 1.8}\nCARBON_INTENSITY = 250  # gCO2e/kWh\nBATCH_SIZE = 16\nSEQ_LENGTH_CLASSIFICATION = 128\nSEQ_LENGTH_QA = 384\nNUM_LAYERS = 1\nGLUE_TASKS_TO_RUN = ['sst2', 'mrpc', 'rte']\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef get_device():\n    device = torch.device(DEVICE)\n    if DEVICE == \"cuda\":\n        try:\n            print(f\"‚úì Using GPU: {torch.cuda.get_device_name(0)} (CUDA)\")\n        except Exception:\n            print(\"‚úì Using GPU (name unknown)\")\n        if DEVICE_CONFIG['mixed_precision']:\n            print(\"‚úì Mixed precision (FP16) enabled.\")\n    else:\n        print(f\"‚úì Using CPU\")\n    return device\n# --- SECTION: 2. QUANTIZATION ---\n# --- SECTION: 2. QUANTIZATION ---\n# --- SECTION: 2. QUANTIZATION ---\n# --- SECTION: 2. QUANTIZATION ---\nclass AdaptiveQuantizedLayer(nn.Module):\n    def __init__(self, layer, quant_bits=32):\n        super().__init__()\n        self.layer = layer\n        self.current_precision = 0\n        self.switch_count = {'INT4': 0, 'INT8': 0, 'FP32': 0}\n        self.set_precision(quant_bits)\n\n    def set_precision(self, bits):\n        if self.current_precision != bits and self.current_precision != 0:\n            self.switch_count['INT' + str(bits) if bits < 32 else 'FP32'] += 1\n        self.quant_bits = bits\n        self.current_precision = bits\n        if bits < 32:\n            self.min_val = -2 ** (bits - 1)\n            self.max_val = 2 ** (bits - 1) - 1\n        else:\n            self.min_val, self.max_val = None, None\n\n    def quantize(self, x):\n        x_max, x_min = x.max(), x.min()\n        if x_max == x_min:  # Prevent division by zero\n            return x\n        scale = (self.max_val - self.min_val) / (x_max - x_min + 1e-8)\n        zero_point = x_min - self.min_val / scale\n        x_quant = torch.round(x * scale + zero_point)\n        x_quant = torch.clamp(x_quant, self.min_val, self.max_val)\n        x_dequant = (x_quant - zero_point) / scale\n        return x_dequant\n\n    def forward(self, *args, **kwargs):\n        output = self.layer(*args, **kwargs)\n        if self.current_precision == 32 or not self.training:\n            return output\n        if isinstance(output, tuple):\n            hidden_states = output[0]\n            hidden_states = self.quantize(hidden_states)\n            return (hidden_states,) + output[1:]\n        else:\n            return self.quantize(output)\n\nclass PrecisionScheduler:\n    def __init__(self, model, initial_grad_norm_threshold=10.0, ma_window_size=20, cooldown_steps=20, warmup_steps=1000, alpha=0.9, task_name='sst2'):\n        self.model = model\n        self.initial_grad_norm_threshold = initial_grad_norm_threshold\n        self.current_threshold = initial_grad_norm_threshold\n        self.alpha = alpha\n        self.ma_window_size = ma_window_size\n        self.cooldown_steps = cooldown_steps\n        self.warmup_steps = 1000 if task_name == 'sst2' else 500  # Longer warmup for SST-2\n        self.adaptable_layers = [m for m in self.model.modules() if isinstance(m, AdaptiveQuantizedLayer)]\n        self.grad_norm_history = deque(maxlen=ma_window_size)\n        self.layer_cooldowns = [0] * len(self.adaptable_layers)\n        self.switch_log = []\n        self.current_epoch = 0\n        self.step_count = 0\n        print(f\"‚úì Robust PrecisionScheduler initialized for {len(self.adaptable_layers)} layers (Window: {ma_window_size}, Cooldown: {cooldown_steps}, Warmup: {self.warmup_steps}).\")\n\n    def get_global_norm(self):\n        total_norm = 0.0\n        param_count = 0\n        for name, p in self.model.named_parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                if not torch.isfinite(param_norm):  # Skip non-finite norms\n                    print(f\"Warning: Non-finite gradient norm in {name}: {param_norm.item()}\")\n                    continue\n                total_norm += param_norm.item() ** 2\n                param_count += 1\n        total_norm = total_norm ** 0.5 if param_count > 0 else 0.0\n        print(f\"Parameters with gradients: {param_count}, Global norm: {total_norm:.4f}\")\n        return total_norm\n\n    def update_threshold(self, grad_norm):\n        if not torch.isfinite(torch.tensor(grad_norm)):  # Skip infinite norms\n            print(f\"Warning: Skipping threshold update due to infinite grad_norm: {grad_norm}\")\n            return\n        self.grad_norm_history.append(grad_norm)\n        ma_grad_norm = sum(self.grad_norm_history) / len(self.grad_norm_history)\n        clipped_norm = min(max(ma_grad_norm, 0.1), 100.0)  # Tighter upper bound\n        self.current_threshold = max(1.0, min(50.0, clipped_norm * self.alpha + (1 - self.alpha) * self.current_threshold))\n        print(f\"Updated grad_norm_threshold: {self.current_threshold:.4f} (MA grad_norm: {ma_grad_norm:.4f})\")\n\n    def step(self, epoch, global_norm):\n        self.current_epoch = epoch\n        self.step_count += 1\n\n        # Warmup: Keep all layers at FP32\n        if self.step_count <= self.warmup_steps:\n            for idx, layer in enumerate(self.adaptable_layers):\n                if layer.current_precision != 32:\n                    layer.set_precision(32)\n                    self._log_switch(idx, 32, global_norm, \"increase\")\n                    print(f\"‚ö° Warmup | Layer {idx} set to 32-bit (grad_norm={global_norm:.4f})\")\n            return global_norm\n\n        # Update threshold and cooldowns\n        self.update_threshold(global_norm)\n        for i in range(len(self.layer_cooldowns)):\n            if self.layer_cooldowns[i] > 0:\n                self.layer_cooldowns[i] -= 1\n\n        # Adjust precision for all layers\n        ma_grad_norm = sum(self.grad_norm_history) / len(self.grad_norm_history) if self.grad_norm_history else global_norm\n        for idx, layer in enumerate(self.adaptable_layers):\n            if self.layer_cooldowns[idx] > 0:\n                continue\n            current_precision = layer.current_precision\n            new_precision = current_precision\n\n            # Increase precision if gradients are high\n            if torch.isfinite(torch.tensor(ma_grad_norm)) and ma_grad_norm > self.current_threshold * 2.0:  # Stricter increase\n                if current_precision == 4:\n                    new_precision = 8\n                elif current_precision == 8:\n                    new_precision = 32\n            # Decrease precision if gradients are stable\n            elif torch.isfinite(torch.tensor(ma_grad_norm)) and ma_grad_norm < self.current_threshold * 0.3:  # Stricter decrease\n                if current_precision == 32:\n                    new_precision = 8\n                elif current_precision == 8:\n                    new_precision = 4\n            # Periodic attempt to decrease precision to INT8 only\n            elif self.step_count % 500 == 0 and current_precision > 8:\n                new_precision = 8\n\n            if new_precision != current_precision:\n                layer.set_precision(new_precision)\n                self.layer_cooldowns[idx] = self.cooldown_steps\n                self._log_switch(idx, new_precision, ma_grad_norm, \"increase\" if new_precision > current_precision else \"decrease\")\n                print(f\"‚ö° Epoch {self.current_epoch} | Layer {idx} switched to {new_precision}-bit (MA grad_norm={ma_grad_norm:.4f})\")\n\n        return global_norm\n\n    def _log_switch(self, layer_idx, new_prec, norm, switch_type):\n        event = {\"layer_id\": layer_idx, \"new_precision\": new_prec, \"grad_norm\": norm, \"epoch\": self.current_epoch, \"type\": switch_type}\n        self.switch_log.append(event)\n\ndef build_adaptive_quantized_model(device, model_type, task_type):\n    print(f\"\\nüèóÔ∏è Building Adaptive Quantized {model_type.capitalize()} Model...\")\n    ModelClass = BertForSequenceClassification if model_type == 'bert' else DistilBertForSequenceClassification\n    ModelBase = \"bert-base-uncased\" if model_type == 'bert' else \"distilbert-base-uncased\"\n    model = ModelClass.from_pretrained(ModelBase, num_labels=2).to(device)\n\n    class QuantizedModel(nn.Module):\n        def __init__(self, model_to_wrap, model_type_local):\n            super().__init__()\n            self.model = model_to_wrap\n            self.config = self.model.config\n            layers_to_quantize = self.model.bert.encoder.layer if model_type_local == \"bert\" else self.model.distilbert.transformer.layer\n            for i in range(len(layers_to_quantize)):\n                if i in [0, len(layers_to_quantize)-1]:\n                    continue\n                layers_to_quantize[i] = AdaptiveQuantizedLayer(layers_to_quantize[i], quant_bits=32)\n        def forward(self, **kwargs):\n            return self.model(**kwargs)\n\n    quantized_model = QuantizedModel(model, model_type).to(device)\n    print(f\"‚úì Successfully created quantized {model_type.capitalize()} model.\")\n    return quantized_model\n# --- SECTION: 3. DATA LOADING ---\ndef get_dataloaders(model_type, seq_len, batch_size):\n    from transformers import DistilBertTokenizerFast, BertTokenizerFast\n    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased') if model_type == 'distilbert' else BertTokenizerFast.from_pretrained('bert-base-uncased')\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    def preprocess_glue_sst2(examples):\n        enc = tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    def preprocess_glue_mrpc(examples):\n        enc = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    def preprocess_glue_rte(examples):\n        enc = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    try:\n        glue_sst2_train_data = load_dataset(\"glue\", \"sst2\", split=f\"train[:{MAX_SAMPLES}]\")\n        glue_sst2_validation_data = load_dataset(\"glue\", \"sst2\", split=f\"validation[:{MAX_SAMPLES}]\")\n        glue_mrpc_train_data = load_dataset(\"glue\", \"mrpc\", split=f\"train[:{5000}]\")\n        glue_mrpc_validation_data = load_dataset(\"glue\", \"mrpc\", split=f\"validation[:{5000}]\")\n        glue_rte_train_data = load_dataset(\"glue\", \"rte\", split=f\"train[:{2490}]\")\n        glue_rte_validation_data = load_dataset(\"glue\", \"rte\", split=f\"validation[:{277}]\")\n\n        glue_sst2_train_data = glue_sst2_train_data.map(preprocess_glue_sst2, batched=True, remove_columns=['sentence', 'idx', 'label'])\n        glue_sst2_validation_data = glue_sst2_validation_data.map(preprocess_glue_sst2, batched=True, remove_columns=['sentence', 'idx', 'label'])\n        glue_mrpc_train_data = glue_mrpc_train_data.map(preprocess_glue_mrpc, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n        glue_mrpc_validation_data = glue_mrpc_validation_data.map(preprocess_glue_mrpc, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n        glue_rte_train_data = glue_rte_train_data.map(preprocess_glue_rte, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n        glue_rte_validation_data = glue_rte_validation_data.map(preprocess_glue_rte, batched=True, remove_columns=['sentence1', 'sentence2', 'idx', 'label'])\n\n        glue_sst2_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_sst2_validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_mrpc_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_mrpc_validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_rte_train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n        glue_rte_validation_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n        dataloaders = {\n            'glue_sst2_train': DataLoader(glue_sst2_train_data, batch_size=batch_size, shuffle=True),\n            'glue_sst2_validation': DataLoader(glue_sst2_validation_data, batch_size=batch_size, shuffle=False),\n            'glue_mrpc_train': DataLoader(glue_mrpc_train_data, batch_size=batch_size, shuffle=True),\n            'glue_mrpc_validation': DataLoader(glue_mrpc_validation_data, batch_size=batch_size, shuffle=False),\n            'glue_rte_train': DataLoader(glue_rte_train_data, batch_size=batch_size, shuffle=True),\n            'glue_rte_validation': DataLoader(glue_rte_validation_data, batch_size=batch_size, shuffle=False),\n        }\n        return dataloaders, tokenizer\n    except Exception as e:\n        print(f\"‚ö†Ô∏è get_dataloaders failed: {e}. Returning empty dataloaders.\")\n        return {}, None\n\ndef evaluate_classification(model, dataloader, device, is_bert):\n    model.eval()\n    predictions, labels = [], []\n    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n        if not is_bert:\n            inputs.pop('token_type_ids', None)\n        if DEVICE_CONFIG['mixed_precision'] and device.type == 'cuda':\n            inputs = {k: v.half() if v.dtype == torch.float32 else v for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        logits = outputs.logits\n        predicted = torch.argmax(logits, dim=-1)\n        predictions.extend(predicted.cpu().numpy())\n        labels.extend(batch['labels'].cpu().numpy())\n    return {\"accuracy\": accuracy_score(labels, predictions), \"f1\": f1_score(labels, predictions, average='weighted')}\n\n# --- SECTION: 5. FINE-TUNING ---\ndef fine_tune(model, train_dataloader, validation_dataloader, device, epochs=5, task_type=\"classification\", is_bert=False):\n    from torch.cuda.amp import autocast, GradScaler\n    scaler = GradScaler() if DEVICE_CONFIG['mixed_precision'] and device.type == 'cuda' else None\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)  # Added weight decay\n    num_training_steps = len(train_dataloader) * epochs\n    num_warmup_steps = int(num_training_steps * 0.2)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n\n    precision_scheduler = None\n    if any(isinstance(m, AdaptiveQuantizedLayer) for m in model.modules()):\n        precision_scheduler = PrecisionScheduler(model, initial_grad_norm_threshold=10.0, ma_window_size=20, cooldown_steps=20, warmup_steps=1000 if task_type == 'sst2' else 500, task_name=task_type)\n\n    for layer in model.modules():\n        if isinstance(layer, AdaptiveQuantizedLayer):\n            layer.set_precision(32)\n\n    best_f1 = 0.0\n    patience = 3\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        total_loss = 0\n        for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\", leave=False):\n            optimizer.zero_grad()\n            inputs = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            if 'input_ids' not in inputs:\n                print(\"Skipping a batch with missing 'input_ids'.\")\n                continue\n            with autocast() if scaler else torch.no_grad():\n                outputs = model(**inputs)\n            loss = outputs.loss\n            total_loss += loss.item()\n            if scaler:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                unclipped_grad_norm = precision_scheduler.get_global_norm() if precision_scheduler else 0.0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n                if precision_scheduler:\n                    precision_scheduler.step(epoch + 1, unclipped_grad_norm)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                unclipped_grad_norm = precision_scheduler.get_global_norm() if precision_scheduler else 0.0\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n                if precision_scheduler:\n                    precision_scheduler.step(epoch + 1, unclipped_grad_norm)\n                optimizer.step()\n            lr_scheduler.step()\n\n        print(f\"Average loss: {total_loss / len(train_dataloader):.4f}\")\n\n        # Validation after each epoch\n        metrics = evaluate_classification(model, validation_dataloader, device, is_bert)\n        print(f\"Validation - Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        # Early stopping\n        if metrics['f1'] > best_f1:\n            best_f1 = metrics['f1']\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n    return precision_scheduler.switch_log if precision_scheduler else []\n\n# --- SECTION 6: SINGLE TASK EXPERIMENT RUNNER ---\ndef run_single_task_experiment(model_config, task_name, device):\n    model_type = model_config['model_type']\n    is_quantized = model_config['is_quantized']\n    model_name_prefix = model_config['name']\n\n    model_full_name = f\"{model_name_prefix}_{task_name.upper()}\"\n    print(f\"\\n{'='*20} Running Experiment: {model_full_name} {'='*20}\")\n\n    dataloaders, tokenizer = get_dataloaders(model_type, SEQ_LENGTH_CLASSIFICATION, BATCH_SIZE)\n    train_dataloader = dataloaders.get(f'glue_{task_name}_train')\n    validation_dataloader = dataloaders.get(f'glue_{task_name}_validation')\n    if not train_dataloader or not validation_dataloader:\n        print(f\"‚ö†Ô∏è Dataloaders for task '{task_name}' not found. Skipping.\")\n        return None\n\n    if is_quantized:\n        model = build_adaptive_quantized_model(device, model_type, task_name)\n    else:\n        ModelClass = DistilBertForSequenceClassification if model_type == 'distilbert' else BertForSequenceClassification\n        model = ModelClass.from_pretrained(f\"{model_type}-base-uncased\", num_labels=2).to(device)\n\n    start_time = time.time()\n    tracker = OfflineEmissionsTracker(project_name=model_full_name, country_iso_code=\"USA\")\n    tracker.start()\n\n    epochs = 5 if task_name == 'rte' else 5\n    adaptation_log = fine_tune(model, train_dataloader, validation_dataloader, device, epochs=epochs, task_type=task_name, is_bert=(model_type=='bert'))\n\n    metrics = evaluate_classification(model, validation_dataloader, device, is_bert=(model_type=='bert'))\n\n    emissions_kwh = tracker.stop() or 0.0\n    total_duration_s = time.time() - start_time\n\n    results = {'model_name': model_full_name, 'dataset': task_name.upper()}\n    results.update(metrics)\n\n    def get_scheduler_summary(model, log):\n        summary = {\n            'adaptation_log': log,\n            'precision_switch_counts_int4': 0,\n            'precision_switch_counts_int8': 0,\n            'precision_switch_counts_fp32': 0,\n            'avg_precision_level': 32.0,\n            'precision_distribution': {'INT4': 0.0, 'INT8': 0.0, 'FP32': 1.0}\n        }\n        quant_layers = [m for m in model.modules() if isinstance(m, AdaptiveQuantizedLayer)]\n        if not quant_layers:\n            return summary\n        total_layers = len(quant_layers)\n        int4_count = sum(1 for l in quant_layers if l.current_precision == 4)\n        int8_count = sum(1 for l in quant_layers if l.current_precision == 8)\n        fp32_count = sum(1 for l in quant_layers if l.current_precision == 32)\n        summary['avg_precision_level'] = sum(l.current_precision for l in quant_layers) / total_layers\n        summary['precision_distribution'] = {\n            'INT4': int4_count / total_layers,\n            'INT8': int8_count / total_layers,\n            'FP32': fp32_count / total_layers\n        }\n        summary['precision_switch_counts_int4'] = sum(l.switch_count['INT4'] for l in quant_layers)\n        summary['precision_switch_counts_int8'] = sum(l.switch_count['INT8'] for l in quant_layers)\n        summary['precision_switch_counts_fp32'] = sum(l.switch_count['FP32'] for l in quant_layers)\n        return summary\n\n    if is_quantized:\n        results['scheduler_metrics'] = get_scheduler_summary(model, adaptation_log)\n\n    num_queries = len(validation_dataloader.dataset)\n    total_tokens_processed = num_queries * SEQ_LENGTH_CLASSIFICATION\n    total_carbon_g = emissions_kwh * CARBON_INTENSITY\n    results['performance_metrics'] = {\n        'total_duration_s': total_duration_s,\n        'total_emissions_kwh': emissions_kwh,\n        'latency_ms_query': (total_duration_s / num_queries) * 1000 if num_queries > 0 else 0,\n        'throughput_tokens_sec': total_tokens_processed / total_duration_s if total_duration_s > 0 else 0,\n        'energy_wh_token': (emissions_kwh * 1000) / total_tokens_processed if total_tokens_processed > 0 else 0,\n        'sci_gco2e_query': total_carbon_g / num_queries if num_queries > 0 else 0,\n        'wue_avg_liters_query': (emissions_kwh * WATER_USAGE_FACTORS['average_l_per_kwh']) / num_queries if num_queries > 0 else 0\n    }\n\n    print(\"\\n--- Final Results ---\")\n    print(json.dumps(results, indent=2))\n    return results\n\n# --- SECTION: 7. MAIN EXECUTION ---\nif __name__ == \"__main__\":\n    DEVICE = get_device()\n    set_seed(42)\n\n    all_results = []\n\n    model_configs = [\n       # {'model_type': 'distilbert', 'task_type': 'classification', 'is_quantized': True, 'name': 'Adaptive_Quantized_DistilBERT'},\n       # {'model_type': 'distilbert', 'task_type': 'classification', 'is_quantized': False, 'name': 'Baseline_DistilBERT'},\n     {'model_type': 'bert', 'task_type': 'classification', 'is_quantized': True, 'name': 'Adaptive_Quantized_BERT'},\n        # {'model_type': 'bert', 'task_type': 'classification', 'is_quantized': False, 'name': 'Baseline_BERT'},\n    ]\n\n    tasks_to_run = ['sst2', 'mrpc', 'rte']\n\n    for config in model_configs:\n        for task in tasks_to_run:\n            result = run_single_task_experiment(model_config=config, task_name=task, device=DEVICE)\n            if result:\n                all_results.append(result)\n\n    if all_results:\n        results_df = pd.json_normalize(all_results, sep='_')\n        all_cols = {col for res in all_results for col in pd.json_normalize(res, sep='_').columns}\n        results_df = results_df.reindex(columns=list(all_cols))\n        results_df.to_csv(\"all_task_results.csv\", index=False)\n        with open('adaptive_quantization_results.json', 'w') as f:\n            json.dump(all_results, f, indent=2)\n        print(\"\\n‚úÖ All experiments complete. Results saved to 'all_task_results.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:06:40.674376Z","iopub.execute_input":"2025-09-25T08:06:40.674896Z","iopub.status.idle":"2025-09-25T08:17:31.174297Z","shell.execute_reply.started":"2025-09-25T08:06:40.674869Z","shell.execute_reply":"2025-09-25T08:17:31.173692Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"2025-09-25 08:06:43.537533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758787603.560816    1243 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758787603.568078    1243 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  torch.utils._pytree._register_pytree_node(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n‚úì Using GPU: Tesla T4 (CUDA)\n‚úì Mixed precision (FP16) enabled.\n\n==================== Running Experiment: Adaptive_Quantized_BERT_SST2 ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e761dee1bdd4da99311a9b442e74f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6667e3b2f2fa435892557069c243ff0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031540d2d91d4c79a35c4a82ab884091"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9f4a30e142941839c7abc00fd22c6ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f0bad2108984c498d2e5585241d3305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0250ad35ab4c4e30b7263da94f9178b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee907e6aa42c45b39626dc8b497a90d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c6cdcc834f41efa18d6289af19299a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d14d58b8ac240ce8a1a77a48ddc913a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"016c5266f3e14d87bade1d8fe551f114"}},"metadata":{}},{"name":"stdout","text":"\nüèóÔ∏è Building Adaptive Quantized Bert Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78e518f7c8ae4c58b4774f33ce6144dc"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[codecarbon INFO @ 08:07:02] offline tracker init\n[codecarbon WARNING @ 08:07:02] Multiple instances of codecarbon are allowed to run at the same time.\n[codecarbon INFO @ 08:07:02] [setup] RAM Tracking...\n[codecarbon INFO @ 08:07:02] [setup] CPU Tracking...\n","output_type":"stream"},{"name":"stdout","text":"‚úì Successfully created quantized Bert model.\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon WARNING @ 08:07:03] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon WARNING @ 08:07:03] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n\n[codecarbon INFO @ 08:07:03] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon WARNING @ 08:07:03] No CPU tracking mode found. Falling back on CPU constant mode.\n[codecarbon INFO @ 08:07:03] [setup] GPU Tracking...\n[codecarbon INFO @ 08:07:03] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 08:07:03] The below tracking methods have been set up:\n                RAM Tracking Method: RAM power estimation model\n                CPU Tracking Method: global constant\n                GPU Tracking Method: pynvml\n            \n[codecarbon INFO @ 08:07:03] >>> Tracker's metadata:\n[codecarbon INFO @ 08:07:03]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 08:07:03]   Python version: 3.11.13\n[codecarbon INFO @ 08:07:03]   CodeCarbon version: 3.0.5\n[codecarbon INFO @ 08:07:03]   Available RAM : 31.350 GB\n[codecarbon INFO @ 08:07:03]   CPU count: 4 thread(s) in 1 physical CPU(s)\n[codecarbon INFO @ 08:07:03]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 08:07:03]   GPU count: 2\n[codecarbon INFO @ 08:07:03]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 08:07:03] Emissions data (if any) will be saved to file /kaggle/working/emissions.csv\n","output_type":"stream"},{"name":"stdout","text":"‚úì Robust PrecisionScheduler initialized for 10 layers (Window: 20, Cooldown: 20, Warmup: 1000).\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 2.5485\nParameters with gradients: 201, Global norm: 2.5513\nParameters with gradients: 201, Global norm: 4.3030\nParameters with gradients: 201, Global norm: 2.6177\nParameters with gradients: 201, Global norm: 4.9532\nParameters with gradients: 201, Global norm: 1.9514\nParameters with gradients: 201, Global norm: 2.2987\nParameters with gradients: 201, Global norm: 3.6862\nParameters with gradients: 201, Global norm: 3.2303\nParameters with gradients: 201, Global norm: 4.0498\nParameters with gradients: 201, Global norm: 2.4910\nParameters with gradients: 201, Global norm: 4.1905\nParameters with gradients: 201, Global norm: 2.7589\nParameters with gradients: 201, Global norm: 3.8373\nParameters with gradients: 201, Global norm: 6.1075\nParameters with gradients: 201, Global norm: 3.4892\nParameters with gradients: 201, Global norm: 4.8482\nParameters with gradients: 201, Global norm: 3.7262\nParameters with gradients: 201, Global norm: 4.5293\nParameters with gradients: 201, Global norm: 5.3514\nParameters with gradients: 201, Global norm: 6.1364\nParameters with gradients: 201, Global norm: 2.2167\nParameters with gradients: 201, Global norm: 3.0957\nParameters with gradients: 201, Global norm: 3.4889\nParameters with gradients: 201, Global norm: 2.6628\nParameters with gradients: 201, Global norm: 2.7863\nParameters with gradients: 201, Global norm: 2.3679\nParameters with gradients: 201, Global norm: 3.4191\nParameters with gradients: 201, Global norm: 2.9429\nParameters with gradients: 201, Global norm: 2.3436\nParameters with gradients: 201, Global norm: 3.7822\nParameters with gradients: 201, Global norm: 2.5564\nParameters with gradients: 201, Global norm: 3.9679\nParameters with gradients: 201, Global norm: 3.8760\nParameters with gradients: 201, Global norm: 1.8419\nParameters with gradients: 201, Global norm: 2.6429\nParameters with gradients: 201, Global norm: 4.6275\nParameters with gradients: 201, Global norm: 3.1155\nParameters with gradients: 201, Global norm: 2.1002\nParameters with gradients: 201, Global norm: 2.2940\nParameters with gradients: 201, Global norm: 2.1200\nParameters with gradients: 201, Global norm: 2.8226\nParameters with gradients: 201, Global norm: 3.1926\nParameters with gradients: 201, Global norm: 3.7450\nParameters with gradients: 201, Global norm: 3.2087\nParameters with gradients: 201, Global norm: 2.8594\nParameters with gradients: 201, Global norm: 5.8438\nParameters with gradients: 201, Global norm: 3.0647\nParameters with gradients: 201, Global norm: 4.1495\nParameters with gradients: 201, Global norm: 1.8136\nParameters with gradients: 201, Global norm: 2.7678\nParameters with gradients: 201, Global norm: 4.3669\nParameters with gradients: 201, Global norm: 1.9981\nParameters with gradients: 201, Global norm: 2.2427\nParameters with gradients: 201, Global norm: 3.4316\nParameters with gradients: 201, Global norm: 3.0648\nParameters with gradients: 201, Global norm: 3.0723\nParameters with gradients: 201, Global norm: 4.3414\nParameters with gradients: 201, Global norm: 3.6836\nParameters with gradients: 201, Global norm: 5.4554\nParameters with gradients: 201, Global norm: 3.0091\nParameters with gradients: 201, Global norm: 2.8898\nParameters with gradients: 201, Global norm: 5.1169\nParameters with gradients: 201, Global norm: 2.0184\nParameters with gradients: 201, Global norm: 1.8977\nParameters with gradients: 201, Global norm: 3.5098\nParameters with gradients: 201, Global norm: 3.1093\nParameters with gradients: 201, Global norm: 4.0844\nParameters with gradients: 201, Global norm: 2.7303\nParameters with gradients: 201, Global norm: 2.0663\nParameters with gradients: 201, Global norm: 3.7636\nParameters with gradients: 201, Global norm: 4.4064\nParameters with gradients: 201, Global norm: 4.8624\nParameters with gradients: 201, Global norm: 4.6860\nParameters with gradients: 201, Global norm: 4.3583\nParameters with gradients: 201, Global norm: 3.4142\nParameters with gradients: 201, Global norm: 2.3773\nParameters with gradients: 201, Global norm: 3.2792\nParameters with gradients: 201, Global norm: 5.2287\nParameters with gradients: 201, Global norm: 6.3425\nParameters with gradients: 201, Global norm: 2.2674\nParameters with gradients: 201, Global norm: 4.9863\nParameters with gradients: 201, Global norm: 2.2911\nParameters with gradients: 201, Global norm: 2.9327\nParameters with gradients: 201, Global norm: 4.7933\nParameters with gradients: 201, Global norm: 3.2537\nParameters with gradients: 201, Global norm: 3.0834\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:07:18] Energy consumed for RAM : 0.000083 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:07:18] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:07:18] Energy consumed for All CPU : 0.000177 kWh\n[codecarbon INFO @ 08:07:18] Energy consumed for all GPUs : 0.000297 kWh. Total GPU Power : 71.13313909547337 W\n[codecarbon INFO @ 08:07:18] 0.000557 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.7555\nParameters with gradients: 201, Global norm: 2.8886\nParameters with gradients: 201, Global norm: 1.9804\nParameters with gradients: 201, Global norm: 3.6694\nParameters with gradients: 201, Global norm: 3.4135\nParameters with gradients: 201, Global norm: 6.1977\nParameters with gradients: 201, Global norm: 3.1413\nParameters with gradients: 201, Global norm: 3.6426\nParameters with gradients: 201, Global norm: 4.7672\nParameters with gradients: 201, Global norm: 3.5424\nParameters with gradients: 201, Global norm: 3.0826\nParameters with gradients: 201, Global norm: 2.9247\nParameters with gradients: 201, Global norm: 3.3965\nParameters with gradients: 201, Global norm: 3.2014\nParameters with gradients: 201, Global norm: 3.4861\nParameters with gradients: 201, Global norm: 3.2482\nParameters with gradients: 201, Global norm: 4.3074\nParameters with gradients: 201, Global norm: 3.1565\nParameters with gradients: 201, Global norm: 3.6883\nParameters with gradients: 201, Global norm: 4.7349\nParameters with gradients: 201, Global norm: 3.5482\nParameters with gradients: 201, Global norm: 4.5897\nParameters with gradients: 201, Global norm: 3.3205\nParameters with gradients: 201, Global norm: 5.3086\nParameters with gradients: 201, Global norm: 3.4075\nParameters with gradients: 201, Global norm: 3.7270\nParameters with gradients: 201, Global norm: 4.0766\nParameters with gradients: 201, Global norm: 3.9293\nParameters with gradients: 201, Global norm: 3.6184\nParameters with gradients: 201, Global norm: 5.1437\nParameters with gradients: 201, Global norm: 3.8649\nParameters with gradients: 201, Global norm: 5.4960\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.intermediate.dense.weight: inf\nParameters with gradients: 200, Global norm: 5.3213\nParameters with gradients: 201, Global norm: 4.8887\nParameters with gradients: 201, Global norm: 3.7238\nParameters with gradients: 201, Global norm: 6.1685\nParameters with gradients: 201, Global norm: 4.0494\nParameters with gradients: 201, Global norm: 3.4327\nParameters with gradients: 201, Global norm: 4.3509\nParameters with gradients: 201, Global norm: 3.3695\nParameters with gradients: 201, Global norm: 3.8378\nParameters with gradients: 201, Global norm: 5.4278\nParameters with gradients: 201, Global norm: 3.7364\nParameters with gradients: 201, Global norm: 7.2506\nParameters with gradients: 201, Global norm: 6.2926\nParameters with gradients: 201, Global norm: 3.9923\nParameters with gradients: 201, Global norm: 7.2283\nParameters with gradients: 201, Global norm: 4.6472\nParameters with gradients: 201, Global norm: 6.9182\nParameters with gradients: 201, Global norm: 6.0776\nParameters with gradients: 201, Global norm: 7.5645\nParameters with gradients: 201, Global norm: 12.2604\nParameters with gradients: 201, Global norm: 4.7387\nParameters with gradients: 201, Global norm: 5.6629\nParameters with gradients: 201, Global norm: 4.6120\nParameters with gradients: 201, Global norm: 9.9897\nParameters with gradients: 201, Global norm: 4.2681\nParameters with gradients: 201, Global norm: 14.2126\nParameters with gradients: 201, Global norm: 11.4068\nParameters with gradients: 201, Global norm: 7.0262\nParameters with gradients: 201, Global norm: 3.0069\nParameters with gradients: 201, Global norm: 5.1311\nParameters with gradients: 201, Global norm: 9.4191\nParameters with gradients: 201, Global norm: 8.3142\nParameters with gradients: 201, Global norm: 11.2183\nParameters with gradients: 201, Global norm: 10.4185\nParameters with gradients: 201, Global norm: 5.6278\nParameters with gradients: 201, Global norm: 8.6019\nParameters with gradients: 201, Global norm: 14.3470\nParameters with gradients: 201, Global norm: 7.9851\nParameters with gradients: 201, Global norm: 18.3741\nParameters with gradients: 201, Global norm: 8.1388\nParameters with gradients: 201, Global norm: 15.9460\nParameters with gradients: 201, Global norm: 6.4163\nParameters with gradients: 201, Global norm: 5.7133\nWarning: Non-finite gradient norm in model.bert.encoder.layer.7.layer.intermediate.dense.weight: inf\nWarning: Non-finite gradient norm in model.bert.encoder.layer.8.layer.intermediate.dense.weight: inf\nParameters with gradients: 199, Global norm: 23.1013\nParameters with gradients: 201, Global norm: 5.1424\nParameters with gradients: 201, Global norm: 16.3650\nParameters with gradients: 201, Global norm: 13.9178\nParameters with gradients: 201, Global norm: 5.0038\nParameters with gradients: 201, Global norm: 18.6421\nParameters with gradients: 201, Global norm: 9.1416\nParameters with gradients: 201, Global norm: 13.6474\nParameters with gradients: 201, Global norm: 11.9143\nParameters with gradients: 201, Global norm: 13.1657\nParameters with gradients: 201, Global norm: 9.0505\nParameters with gradients: 201, Global norm: 16.8194\nParameters with gradients: 201, Global norm: 6.2779\nParameters with gradients: 201, Global norm: 5.0292\nParameters with gradients: 201, Global norm: 9.2781\nParameters with gradients: 201, Global norm: 17.6962\nParameters with gradients: 201, Global norm: 7.0185\nParameters with gradients: 201, Global norm: 20.1830\nParameters with gradients: 201, Global norm: 34.4079\nParameters with gradients: 201, Global norm: 11.2729\nParameters with gradients: 201, Global norm: 13.9039\nParameters with gradients: 201, Global norm: 10.7441\nParameters with gradients: 201, Global norm: 11.6182\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:07:33] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:07:33] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:07:33] Energy consumed for All CPU : 0.000354 kWh\n[codecarbon INFO @ 08:07:33] Energy consumed for all GPUs : 0.000611 kWh. Total GPU Power : 75.49141277620168 W\n[codecarbon INFO @ 08:07:33] 0.001132 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 6.5811\nParameters with gradients: 201, Global norm: 14.3819\nParameters with gradients: 201, Global norm: 10.5013\nParameters with gradients: 201, Global norm: 7.4028\nParameters with gradients: 201, Global norm: 11.9702\nParameters with gradients: 201, Global norm: 12.9963\nParameters with gradients: 201, Global norm: 6.7228\nParameters with gradients: 201, Global norm: 10.7873\nParameters with gradients: 201, Global norm: 6.8126\nParameters with gradients: 201, Global norm: 8.4171\nParameters with gradients: 201, Global norm: 5.0625\nParameters with gradients: 201, Global norm: 12.0687\nParameters with gradients: 201, Global norm: 10.0476\nParameters with gradients: 201, Global norm: 5.9760\nParameters with gradients: 201, Global norm: 4.9901\nParameters with gradients: 201, Global norm: 4.6716\nParameters with gradients: 201, Global norm: 5.0960\nParameters with gradients: 201, Global norm: 7.0181\nParameters with gradients: 201, Global norm: 8.5742\nParameters with gradients: 201, Global norm: 8.9566\nParameters with gradients: 201, Global norm: 10.8809\nParameters with gradients: 201, Global norm: 8.3078\nParameters with gradients: 201, Global norm: 15.4962\nParameters with gradients: 201, Global norm: 8.9268\nParameters with gradients: 201, Global norm: 21.7349\nParameters with gradients: 201, Global norm: 11.3291\nParameters with gradients: 201, Global norm: 14.0404\nParameters with gradients: 201, Global norm: 8.4075\nParameters with gradients: 201, Global norm: 10.4947\nParameters with gradients: 201, Global norm: 7.5722\nParameters with gradients: 201, Global norm: 9.9356\nParameters with gradients: 201, Global norm: 13.0878\nParameters with gradients: 201, Global norm: 9.0367\nParameters with gradients: 201, Global norm: 11.2127\nParameters with gradients: 201, Global norm: 15.5976\nParameters with gradients: 201, Global norm: 8.1563\nParameters with gradients: 201, Global norm: 12.9751\nParameters with gradients: 201, Global norm: 7.9037\nParameters with gradients: 201, Global norm: 6.9795\nParameters with gradients: 201, Global norm: 8.8159\nParameters with gradients: 201, Global norm: 16.8522\nParameters with gradients: 201, Global norm: 7.4897\nParameters with gradients: 201, Global norm: 6.0310\nParameters with gradients: 201, Global norm: 11.1946\nWarning: Non-finite gradient norm in model.bert.embeddings.word_embeddings.weight: nan\nWarning: Non-finite gradient norm in model.bert.embeddings.position_embeddings.weight: nan\nWarning: Non-finite gradient norm in model.bert.embeddings.token_type_embeddings.weight: nan\nWarning: Non-finite gradient norm in model.bert.embeddings.LayerNorm.weight: nan\nWarning: Non-finite gradient norm in model.bert.embeddings.LayerNorm.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.self.query.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.self.query.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.self.key.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.self.key.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.self.value.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.self.value.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.output.dense.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.output.dense.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.output.LayerNorm.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.attention.output.LayerNorm.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.intermediate.dense.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.intermediate.dense.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.output.dense.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.output.dense.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.output.LayerNorm.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.0.output.LayerNorm.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.1.layer.attention.self.query.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.1.layer.attention.self.query.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.1.layer.attention.self.key.weight: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.1.layer.attention.self.key.bias: nan\nWarning: Non-finite gradient norm in model.bert.encoder.layer.1.layer.intermediate.dense.weight: inf\nParameters with gradients: 175, Global norm: 36.6243\nParameters with gradients: 201, Global norm: 7.2290\nParameters with gradients: 201, Global norm: 9.7317\nParameters with gradients: 201, Global norm: 10.0701\nParameters with gradients: 201, Global norm: 15.0141\nParameters with gradients: 201, Global norm: 4.8853\nParameters with gradients: 201, Global norm: 6.8741\nParameters with gradients: 201, Global norm: 9.0044\nParameters with gradients: 201, Global norm: 7.1987\nParameters with gradients: 201, Global norm: 7.9316\nParameters with gradients: 201, Global norm: 11.2204\nParameters with gradients: 201, Global norm: 16.7938\nParameters with gradients: 201, Global norm: 9.7443\nParameters with gradients: 201, Global norm: 8.9125\nParameters with gradients: 201, Global norm: 6.1359\nParameters with gradients: 201, Global norm: 22.3507\nParameters with gradients: 201, Global norm: 8.2321\nParameters with gradients: 201, Global norm: 17.2470\nParameters with gradients: 201, Global norm: 12.1773\nParameters with gradients: 201, Global norm: 7.3191\nParameters with gradients: 201, Global norm: 5.1621\nParameters with gradients: 201, Global norm: 5.8850\nParameters with gradients: 201, Global norm: 11.1504\nParameters with gradients: 201, Global norm: 9.1968\nParameters with gradients: 201, Global norm: 9.2336\nParameters with gradients: 201, Global norm: 12.3102\nParameters with gradients: 201, Global norm: 6.8363\nParameters with gradients: 201, Global norm: 6.3576\nParameters with gradients: 201, Global norm: 14.6474\nParameters with gradients: 201, Global norm: 12.6563\nParameters with gradients: 201, Global norm: 9.4920\nParameters with gradients: 201, Global norm: 11.0929\nParameters with gradients: 201, Global norm: 7.9543\nParameters with gradients: 201, Global norm: 6.2623\nParameters with gradients: 201, Global norm: 11.2844\nParameters with gradients: 201, Global norm: 7.9502\nParameters with gradients: 201, Global norm: 22.7082\nParameters with gradients: 201, Global norm: 9.4509\nParameters with gradients: 201, Global norm: 5.7318\nParameters with gradients: 201, Global norm: 4.5231\nParameters with gradients: 201, Global norm: 9.2214\nParameters with gradients: 201, Global norm: 11.1302\nParameters with gradients: 201, Global norm: 7.1311\nParameters with gradients: 201, Global norm: 5.9476\nParameters with gradients: 201, Global norm: 9.9805\nParameters with gradients: 201, Global norm: 10.6303\nParameters with gradients: 201, Global norm: 5.0615\nParameters with gradients: 201, Global norm: 12.9162\nParameters with gradients: 201, Global norm: 6.6865\nParameters with gradients: 201, Global norm: 8.3991\nParameters with gradients: 201, Global norm: 14.1944\nParameters with gradients: 201, Global norm: 8.5932\nParameters with gradients: 201, Global norm: 8.8773\nParameters with gradients: 201, Global norm: 7.8561\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:07:48] Energy consumed for RAM : 0.000250 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:07:48] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:07:48] Energy consumed for All CPU : 0.000531 kWh\n[codecarbon INFO @ 08:07:48] Energy consumed for all GPUs : 0.000925 kWh. Total GPU Power : 75.5261526982569 W\n[codecarbon INFO @ 08:07:48] 0.001706 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 8.8752\nParameters with gradients: 201, Global norm: 14.4958\nParameters with gradients: 201, Global norm: 4.2480\nParameters with gradients: 201, Global norm: 4.6809\nParameters with gradients: 201, Global norm: 11.6111\nParameters with gradients: 201, Global norm: 13.8423\nParameters with gradients: 201, Global norm: 10.2935\nParameters with gradients: 201, Global norm: 13.0934\nParameters with gradients: 201, Global norm: 15.7154\nParameters with gradients: 201, Global norm: 9.9652\nParameters with gradients: 201, Global norm: 7.2032\nParameters with gradients: 201, Global norm: 18.0947\nParameters with gradients: 201, Global norm: 18.7728\nParameters with gradients: 201, Global norm: 19.3461\nParameters with gradients: 201, Global norm: 14.0055\nParameters with gradients: 201, Global norm: 7.8249\nParameters with gradients: 201, Global norm: 10.5632\nParameters with gradients: 201, Global norm: 21.9339\nParameters with gradients: 201, Global norm: 10.1152\nParameters with gradients: 201, Global norm: 3.8633\nParameters with gradients: 201, Global norm: 5.9225\nParameters with gradients: 201, Global norm: 6.7580\nParameters with gradients: 201, Global norm: 6.2914\nParameters with gradients: 201, Global norm: 5.7103\nParameters with gradients: 201, Global norm: 13.0368\nParameters with gradients: 201, Global norm: 13.4134\nParameters with gradients: 201, Global norm: 2.6913\nParameters with gradients: 201, Global norm: 6.7257\nParameters with gradients: 201, Global norm: 7.8992\nParameters with gradients: 201, Global norm: 7.6185\nAverage loss: 0.4711\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.8727, F1: 0.8723\n\nEpoch 2/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 2:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 4.5759\nParameters with gradients: 201, Global norm: 4.4878\nParameters with gradients: 201, Global norm: 4.2421\nParameters with gradients: 201, Global norm: 4.8151\nParameters with gradients: 201, Global norm: 3.1142\nParameters with gradients: 201, Global norm: 3.8129\nParameters with gradients: 201, Global norm: 7.6865\nParameters with gradients: 201, Global norm: 1.6587\nParameters with gradients: 201, Global norm: 5.1493\nParameters with gradients: 201, Global norm: 10.5432\nParameters with gradients: 201, Global norm: 18.3132\nParameters with gradients: 201, Global norm: 13.8463\nParameters with gradients: 201, Global norm: 10.8007\nParameters with gradients: 201, Global norm: 4.7471\nParameters with gradients: 201, Global norm: 3.0092\nParameters with gradients: 201, Global norm: 2.4236\nParameters with gradients: 201, Global norm: 4.3556\nParameters with gradients: 201, Global norm: 17.1898\nParameters with gradients: 201, Global norm: 5.1521\nParameters with gradients: 201, Global norm: 7.5647\nParameters with gradients: 201, Global norm: 3.3072\nParameters with gradients: 201, Global norm: 17.0123\nParameters with gradients: 201, Global norm: 6.3130\nParameters with gradients: 201, Global norm: 14.1122\nParameters with gradients: 201, Global norm: 14.6997\nParameters with gradients: 201, Global norm: 4.6927\nParameters with gradients: 201, Global norm: 10.3403\nParameters with gradients: 201, Global norm: 5.3874\nParameters with gradients: 201, Global norm: 4.5904\nParameters with gradients: 201, Global norm: 13.6015\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:08:03] Energy consumed for RAM : 0.000333 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:08:03] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:08:03] Energy consumed for All CPU : 0.000708 kWh\n[codecarbon INFO @ 08:08:03] Energy consumed for all GPUs : 0.001243 kWh. Total GPU Power : 76.2943046515599 W\n[codecarbon INFO @ 08:08:03] 0.002284 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 2.5942\nParameters with gradients: 201, Global norm: 13.6933\nParameters with gradients: 201, Global norm: 2.6331\nParameters with gradients: 201, Global norm: 4.7612\nParameters with gradients: 201, Global norm: 4.6337\nParameters with gradients: 201, Global norm: 3.2078\nParameters with gradients: 201, Global norm: 5.0646\nParameters with gradients: 201, Global norm: 24.6550\nParameters with gradients: 201, Global norm: 11.0486\nParameters with gradients: 201, Global norm: 2.8514\nParameters with gradients: 201, Global norm: 12.2030\nParameters with gradients: 201, Global norm: 23.0611\nParameters with gradients: 201, Global norm: 8.5520\nParameters with gradients: 201, Global norm: 2.4026\nParameters with gradients: 201, Global norm: 3.6839\nParameters with gradients: 201, Global norm: 3.4798\nParameters with gradients: 201, Global norm: 3.4904\nParameters with gradients: 201, Global norm: 10.9166\nParameters with gradients: 201, Global norm: 6.1615\nParameters with gradients: 201, Global norm: 4.0713\nParameters with gradients: 201, Global norm: 3.9578\nParameters with gradients: 201, Global norm: 9.4753\nParameters with gradients: 201, Global norm: 15.2077\nParameters with gradients: 201, Global norm: 15.8276\nParameters with gradients: 201, Global norm: 9.1753\nParameters with gradients: 201, Global norm: 3.8753\nParameters with gradients: 201, Global norm: 5.8902\nParameters with gradients: 201, Global norm: 9.3286\nParameters with gradients: 201, Global norm: 2.3700\nParameters with gradients: 201, Global norm: 2.9279\nParameters with gradients: 201, Global norm: 7.1683\nParameters with gradients: 201, Global norm: 5.9067\nParameters with gradients: 201, Global norm: 3.5292\nParameters with gradients: 201, Global norm: 5.3984\nParameters with gradients: 201, Global norm: 1.3325\nParameters with gradients: 201, Global norm: 12.3036\nParameters with gradients: 201, Global norm: 5.6174\nParameters with gradients: 201, Global norm: 2.8256\nParameters with gradients: 201, Global norm: 3.3402\nParameters with gradients: 201, Global norm: 4.3010\nParameters with gradients: 201, Global norm: 19.4458\nParameters with gradients: 201, Global norm: 4.1568\nParameters with gradients: 201, Global norm: 8.4042\nParameters with gradients: 201, Global norm: 7.6553\nParameters with gradients: 201, Global norm: 19.5701\nParameters with gradients: 201, Global norm: 3.9229\nParameters with gradients: 201, Global norm: 4.3398\nParameters with gradients: 201, Global norm: 3.8854\nParameters with gradients: 201, Global norm: 3.8043\nParameters with gradients: 201, Global norm: 0.9430\nParameters with gradients: 201, Global norm: 4.6966\nParameters with gradients: 201, Global norm: 15.0723\nParameters with gradients: 201, Global norm: 2.4571\nParameters with gradients: 201, Global norm: 17.2845\nParameters with gradients: 201, Global norm: 2.4479\nParameters with gradients: 201, Global norm: 5.4802\nParameters with gradients: 201, Global norm: 9.5622\nParameters with gradients: 201, Global norm: 3.0938\nParameters with gradients: 201, Global norm: 17.8718\nParameters with gradients: 201, Global norm: 7.6178\nParameters with gradients: 201, Global norm: 7.5035\nParameters with gradients: 201, Global norm: 2.9244\nParameters with gradients: 201, Global norm: 15.7809\nParameters with gradients: 201, Global norm: 1.5336\nParameters with gradients: 201, Global norm: 5.5166\nParameters with gradients: 201, Global norm: 6.4085\nParameters with gradients: 201, Global norm: 8.4096\nParameters with gradients: 201, Global norm: 14.7457\nParameters with gradients: 201, Global norm: 7.2568\nParameters with gradients: 201, Global norm: 7.3221\nParameters with gradients: 201, Global norm: 7.7928\nParameters with gradients: 201, Global norm: 16.9181\nParameters with gradients: 201, Global norm: 13.4738\nParameters with gradients: 201, Global norm: 8.3454\nParameters with gradients: 201, Global norm: 1.5734\nParameters with gradients: 201, Global norm: 8.3276\nParameters with gradients: 201, Global norm: 3.3002\nParameters with gradients: 201, Global norm: 5.7066\nParameters with gradients: 201, Global norm: 3.5602\nParameters with gradients: 201, Global norm: 4.3157\nParameters with gradients: 201, Global norm: 4.0922\nParameters with gradients: 201, Global norm: 11.3188\nParameters with gradients: 201, Global norm: 8.7548\nParameters with gradients: 201, Global norm: 6.9684\nParameters with gradients: 201, Global norm: 5.9629\nParameters with gradients: 201, Global norm: 3.0194\nParameters with gradients: 201, Global norm: 0.7110\nParameters with gradients: 201, Global norm: 6.6342\nParameters with gradients: 201, Global norm: 15.7105\nParameters with gradients: 201, Global norm: 5.5374\nParameters with gradients: 201, Global norm: 13.6293\nParameters with gradients: 201, Global norm: 14.6374\nParameters with gradients: 201, Global norm: 4.2231\nParameters with gradients: 201, Global norm: 5.0790\nParameters with gradients: 201, Global norm: 17.8527\nParameters with gradients: 201, Global norm: 5.6188\nParameters with gradients: 201, Global norm: 0.9008\nParameters with gradients: 201, Global norm: 13.0462\nParameters with gradients: 201, Global norm: 16.6189\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:08:18] Energy consumed for RAM : 0.000416 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:08:18] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:08:18] Energy consumed for All CPU : 0.000885 kWh\n[codecarbon INFO @ 08:08:18] Energy consumed for all GPUs : 0.001558 kWh. Total GPU Power : 75.6963297734707 W\n[codecarbon INFO @ 08:08:18] 0.002859 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 12.4255\nParameters with gradients: 201, Global norm: 9.6229\nParameters with gradients: 201, Global norm: 0.2501\nParameters with gradients: 201, Global norm: 20.2500\nParameters with gradients: 201, Global norm: 0.4302\nParameters with gradients: 201, Global norm: 8.5951\nParameters with gradients: 201, Global norm: 13.4446\nParameters with gradients: 201, Global norm: 4.8367\nParameters with gradients: 201, Global norm: 10.9249\nParameters with gradients: 201, Global norm: 9.2171\nParameters with gradients: 201, Global norm: 6.8776\nParameters with gradients: 201, Global norm: 1.7996\nParameters with gradients: 201, Global norm: 8.0882\nParameters with gradients: 201, Global norm: 11.8342\nParameters with gradients: 201, Global norm: 3.4570\nParameters with gradients: 201, Global norm: 6.5996\nParameters with gradients: 201, Global norm: 7.1909\nParameters with gradients: 201, Global norm: 3.7062\nParameters with gradients: 201, Global norm: 0.7811\nParameters with gradients: 201, Global norm: 2.5122\nParameters with gradients: 201, Global norm: 6.2796\nParameters with gradients: 201, Global norm: 7.6671\nParameters with gradients: 201, Global norm: 1.9642\nParameters with gradients: 201, Global norm: 6.0194\nParameters with gradients: 201, Global norm: 10.8223\nParameters with gradients: 201, Global norm: 3.8478\nParameters with gradients: 201, Global norm: 1.7008\nParameters with gradients: 201, Global norm: 5.7084\nParameters with gradients: 201, Global norm: 9.0517\nParameters with gradients: 201, Global norm: 6.8752\nParameters with gradients: 201, Global norm: 1.2425\nParameters with gradients: 201, Global norm: 9.6609\nParameters with gradients: 201, Global norm: 3.2531\nParameters with gradients: 201, Global norm: 1.2214\nParameters with gradients: 201, Global norm: 13.0292\nParameters with gradients: 201, Global norm: 7.0220\nParameters with gradients: 201, Global norm: 2.4071\nParameters with gradients: 201, Global norm: 16.5907\nParameters with gradients: 201, Global norm: 6.5342\nParameters with gradients: 201, Global norm: 6.7620\nParameters with gradients: 201, Global norm: 2.1031\nParameters with gradients: 201, Global norm: 11.0046\nParameters with gradients: 201, Global norm: 5.9451\nParameters with gradients: 201, Global norm: 3.8502\nParameters with gradients: 201, Global norm: 6.8912\nParameters with gradients: 201, Global norm: 2.4186\nParameters with gradients: 201, Global norm: 9.6485\nParameters with gradients: 201, Global norm: 3.8491\nParameters with gradients: 201, Global norm: 2.3587\nParameters with gradients: 201, Global norm: 3.0393\nParameters with gradients: 201, Global norm: 7.4186\nParameters with gradients: 201, Global norm: 9.6927\nParameters with gradients: 201, Global norm: 4.1297\nParameters with gradients: 201, Global norm: 5.2033\nParameters with gradients: 201, Global norm: 11.9730\nParameters with gradients: 201, Global norm: 3.8324\nParameters with gradients: 201, Global norm: 8.2491\nParameters with gradients: 201, Global norm: 7.0900\nParameters with gradients: 201, Global norm: 11.0380\nParameters with gradients: 201, Global norm: 1.9856\nParameters with gradients: 201, Global norm: 5.3025\nParameters with gradients: 201, Global norm: 12.7469\nParameters with gradients: 201, Global norm: 3.5569\nParameters with gradients: 201, Global norm: 19.1059\nParameters with gradients: 201, Global norm: 11.8130\nParameters with gradients: 201, Global norm: 20.3264\nParameters with gradients: 201, Global norm: 3.2976\nParameters with gradients: 201, Global norm: 4.7292\nParameters with gradients: 201, Global norm: 5.8191\nParameters with gradients: 201, Global norm: 11.0869\nParameters with gradients: 201, Global norm: 15.8122\nParameters with gradients: 201, Global norm: 6.7413\nParameters with gradients: 201, Global norm: 3.0128\nParameters with gradients: 201, Global norm: 9.3527\nParameters with gradients: 201, Global norm: 8.6283\nParameters with gradients: 201, Global norm: 4.0399\nParameters with gradients: 201, Global norm: 6.3999\nParameters with gradients: 201, Global norm: 3.5601\nParameters with gradients: 201, Global norm: 7.4033\nParameters with gradients: 201, Global norm: 3.0014\nParameters with gradients: 201, Global norm: 1.8090\nParameters with gradients: 201, Global norm: 7.0850\nParameters with gradients: 201, Global norm: 6.0775\nParameters with gradients: 201, Global norm: 6.1459\nParameters with gradients: 201, Global norm: 2.2255\nParameters with gradients: 201, Global norm: 3.8582\nParameters with gradients: 201, Global norm: 10.1327\nParameters with gradients: 201, Global norm: 3.4526\nParameters with gradients: 201, Global norm: 14.5446\nParameters with gradients: 201, Global norm: 2.4786\nParameters with gradients: 201, Global norm: 1.6424\nParameters with gradients: 201, Global norm: 10.7316\nParameters with gradients: 201, Global norm: 2.8661\nParameters with gradients: 201, Global norm: 2.8577\nParameters with gradients: 201, Global norm: 2.1123\nParameters with gradients: 201, Global norm: 4.0636\nParameters with gradients: 201, Global norm: 4.5284\nParameters with gradients: 201, Global norm: 3.0471\nParameters with gradients: 201, Global norm: 1.2656\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:08:33] Energy consumed for RAM : 0.000500 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:08:33] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:08:33] Energy consumed for All CPU : 0.001062 kWh\n[codecarbon INFO @ 08:08:33] Energy consumed for all GPUs : 0.001873 kWh. Total GPU Power : 75.55201002933805 W\n[codecarbon INFO @ 08:08:33] 0.003434 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 6.9830\nParameters with gradients: 201, Global norm: 8.7291\nParameters with gradients: 201, Global norm: 9.0353\nParameters with gradients: 201, Global norm: 14.1748\nParameters with gradients: 201, Global norm: 7.3503\nParameters with gradients: 201, Global norm: 14.4054\nParameters with gradients: 201, Global norm: 17.4051\nParameters with gradients: 201, Global norm: 8.7176\nParameters with gradients: 201, Global norm: 15.8415\nParameters with gradients: 201, Global norm: 1.4711\nParameters with gradients: 201, Global norm: 3.9390\nParameters with gradients: 201, Global norm: 14.2936\nParameters with gradients: 201, Global norm: 4.1043\nParameters with gradients: 201, Global norm: 2.9298\nParameters with gradients: 201, Global norm: 4.9781\nParameters with gradients: 201, Global norm: 3.7700\nParameters with gradients: 201, Global norm: 2.2870\nParameters with gradients: 201, Global norm: 4.8706\nParameters with gradients: 201, Global norm: 6.1233\nParameters with gradients: 201, Global norm: 8.9374\nParameters with gradients: 201, Global norm: 7.0657\nParameters with gradients: 201, Global norm: 18.9737\nParameters with gradients: 201, Global norm: 1.1849\nParameters with gradients: 201, Global norm: 2.9901\nParameters with gradients: 201, Global norm: 7.4392\nParameters with gradients: 201, Global norm: 8.1775\nParameters with gradients: 201, Global norm: 7.8551\nParameters with gradients: 201, Global norm: 2.9054\nParameters with gradients: 201, Global norm: 5.9888\nParameters with gradients: 201, Global norm: 2.6207\nParameters with gradients: 201, Global norm: 12.8752\nParameters with gradients: 201, Global norm: 7.4273\nParameters with gradients: 201, Global norm: 4.3815\nParameters with gradients: 201, Global norm: 7.7416\nParameters with gradients: 201, Global norm: 5.2436\nParameters with gradients: 201, Global norm: 20.2139\nParameters with gradients: 201, Global norm: 12.5273\nParameters with gradients: 201, Global norm: 2.8072\nParameters with gradients: 201, Global norm: 5.1246\nParameters with gradients: 201, Global norm: 4.3409\nParameters with gradients: 201, Global norm: 2.5583\nParameters with gradients: 201, Global norm: 2.5263\nParameters with gradients: 201, Global norm: 9.3016\nParameters with gradients: 201, Global norm: 9.0344\nParameters with gradients: 201, Global norm: 26.6176\nParameters with gradients: 201, Global norm: 4.9112\nParameters with gradients: 201, Global norm: 2.1487\nParameters with gradients: 201, Global norm: 2.9039\nParameters with gradients: 201, Global norm: 6.3059\nParameters with gradients: 201, Global norm: 4.0760\nParameters with gradients: 201, Global norm: 4.9521\nParameters with gradients: 201, Global norm: 2.6616\nParameters with gradients: 201, Global norm: 7.3610\nParameters with gradients: 201, Global norm: 8.3812\nParameters with gradients: 201, Global norm: 2.1313\nParameters with gradients: 201, Global norm: 4.1302\nParameters with gradients: 201, Global norm: 17.8689\nParameters with gradients: 201, Global norm: 12.8037\nParameters with gradients: 201, Global norm: 3.3896\nParameters with gradients: 201, Global norm: 3.4906\nParameters with gradients: 201, Global norm: 5.4137\nParameters with gradients: 201, Global norm: 2.7626\nParameters with gradients: 201, Global norm: 4.4066\nParameters with gradients: 201, Global norm: 3.6653\nParameters with gradients: 201, Global norm: 3.0906\nParameters with gradients: 201, Global norm: 4.9725\nParameters with gradients: 201, Global norm: 2.6672\nParameters with gradients: 201, Global norm: 5.8432\nParameters with gradients: 201, Global norm: 2.6919\nParameters with gradients: 201, Global norm: 6.1457\nParameters with gradients: 201, Global norm: 33.3536\nParameters with gradients: 201, Global norm: 4.7032\nParameters with gradients: 201, Global norm: 36.7526\nParameters with gradients: 201, Global norm: 10.9271\nParameters with gradients: 201, Global norm: 4.2334\nParameters with gradients: 201, Global norm: 2.9616\nParameters with gradients: 201, Global norm: 13.3056\nParameters with gradients: 201, Global norm: 8.5930\nParameters with gradients: 201, Global norm: 3.2449\nParameters with gradients: 201, Global norm: 3.0943\nParameters with gradients: 201, Global norm: 6.4031\nParameters with gradients: 201, Global norm: 1.2899\nParameters with gradients: 201, Global norm: 6.2371\nParameters with gradients: 201, Global norm: 1.1326\nParameters with gradients: 201, Global norm: 8.0430\nAverage loss: 0.1717\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:08:48] Energy consumed for RAM : 0.000583 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:08:48] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:08:48] Energy consumed for All CPU : 0.001239 kWh\n[codecarbon INFO @ 08:08:48] Energy consumed for all GPUs : 0.002190 kWh. Total GPU Power : 76.15649158964281 W\n[codecarbon INFO @ 08:08:48] 0.004012 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Validation - Accuracy: 0.9094, F1: 0.9094\n\nEpoch 3/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 3:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 1.4521\nParameters with gradients: 201, Global norm: 7.1683\nParameters with gradients: 201, Global norm: 0.7686\nParameters with gradients: 201, Global norm: 9.0633\nParameters with gradients: 201, Global norm: 1.7850\nParameters with gradients: 201, Global norm: 0.5192\nParameters with gradients: 201, Global norm: 6.7486\nParameters with gradients: 201, Global norm: 1.1857\nParameters with gradients: 201, Global norm: 1.2938\nParameters with gradients: 201, Global norm: 3.6401\nParameters with gradients: 201, Global norm: 1.7940\nParameters with gradients: 201, Global norm: 0.6646\nParameters with gradients: 201, Global norm: 1.6123\nParameters with gradients: 201, Global norm: 0.2087\nParameters with gradients: 201, Global norm: 1.2297\nParameters with gradients: 201, Global norm: 1.4792\nParameters with gradients: 201, Global norm: 3.8871\nParameters with gradients: 201, Global norm: 0.3761\nParameters with gradients: 201, Global norm: 0.3715\nParameters with gradients: 201, Global norm: 1.6994\nParameters with gradients: 201, Global norm: 5.4237\nParameters with gradients: 201, Global norm: 16.5142\nParameters with gradients: 201, Global norm: 3.3806\nParameters with gradients: 201, Global norm: 0.5815\nParameters with gradients: 201, Global norm: 0.5701\nParameters with gradients: 201, Global norm: 3.3881\nParameters with gradients: 201, Global norm: 6.1233\nParameters with gradients: 201, Global norm: 2.3785\nParameters with gradients: 201, Global norm: 0.1413\nParameters with gradients: 201, Global norm: 0.1499\nParameters with gradients: 201, Global norm: 0.6628\nParameters with gradients: 201, Global norm: 1.0419\nParameters with gradients: 201, Global norm: 19.5524\nParameters with gradients: 201, Global norm: 0.1418\nParameters with gradients: 201, Global norm: 1.7332\nParameters with gradients: 201, Global norm: 0.3600\nParameters with gradients: 201, Global norm: 1.4431\nParameters with gradients: 201, Global norm: 0.5641\nParameters with gradients: 201, Global norm: 0.1503\nParameters with gradients: 201, Global norm: 5.3586\nParameters with gradients: 201, Global norm: 0.6348\nParameters with gradients: 201, Global norm: 0.8447\nParameters with gradients: 201, Global norm: 12.3280\nParameters with gradients: 201, Global norm: 0.5478\nParameters with gradients: 201, Global norm: 0.7446\nParameters with gradients: 201, Global norm: 2.9481\nParameters with gradients: 201, Global norm: 0.5705\nParameters with gradients: 201, Global norm: 0.3064\nParameters with gradients: 201, Global norm: 0.1517\nParameters with gradients: 201, Global norm: 0.1349\nParameters with gradients: 201, Global norm: 0.9704\nParameters with gradients: 201, Global norm: 0.1759\nParameters with gradients: 201, Global norm: 0.2368\nParameters with gradients: 201, Global norm: 2.6247\nParameters with gradients: 201, Global norm: 0.0651\nParameters with gradients: 201, Global norm: 0.1281\nParameters with gradients: 201, Global norm: 4.7031\nParameters with gradients: 201, Global norm: 0.4570\nParameters with gradients: 201, Global norm: 0.8338\nParameters with gradients: 201, Global norm: 0.0813\nParameters with gradients: 201, Global norm: 0.0818\nParameters with gradients: 201, Global norm: 1.1231\nParameters with gradients: 201, Global norm: 0.1455\nParameters with gradients: 201, Global norm: 1.1927\nParameters with gradients: 201, Global norm: 0.1604\nParameters with gradients: 201, Global norm: 2.8519\nParameters with gradients: 201, Global norm: 0.3013\nParameters with gradients: 201, Global norm: 0.9452\nParameters with gradients: 201, Global norm: 0.1283\nParameters with gradients: 201, Global norm: 0.1101\nParameters with gradients: 201, Global norm: 0.0786\nParameters with gradients: 201, Global norm: 4.8552\nParameters with gradients: 201, Global norm: 0.2890\nParameters with gradients: 201, Global norm: 4.4885\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:09:03] Energy consumed for RAM : 0.000666 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:09:03] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:09:03] Energy consumed for All CPU : 0.001416 kWh\n[codecarbon INFO @ 08:09:03] Energy consumed for all GPUs : 0.002507 kWh. Total GPU Power : 76.09268559471151 W\n[codecarbon INFO @ 08:09:03] 0.004589 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:09:03] 0.014126 g.CO2eq/s mean an estimation of 445.478906716598 kg.CO2eq/year\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.0563\nParameters with gradients: 201, Global norm: 0.0762\nParameters with gradients: 201, Global norm: 0.0919\nParameters with gradients: 201, Global norm: 0.0764\nParameters with gradients: 201, Global norm: 0.0784\nParameters with gradients: 201, Global norm: 8.0131\nParameters with gradients: 201, Global norm: 18.1449\nParameters with gradients: 201, Global norm: 3.5891\nParameters with gradients: 201, Global norm: 0.0899\nParameters with gradients: 201, Global norm: 0.0794\nParameters with gradients: 201, Global norm: 0.1184\nParameters with gradients: 201, Global norm: 0.4157\nParameters with gradients: 201, Global norm: 0.1560\nParameters with gradients: 201, Global norm: 0.0708\nParameters with gradients: 201, Global norm: 0.0617\nParameters with gradients: 201, Global norm: 0.1137\nParameters with gradients: 201, Global norm: 5.8907\nParameters with gradients: 201, Global norm: 0.2083\nParameters with gradients: 201, Global norm: 0.1300\nParameters with gradients: 201, Global norm: 4.5558\nParameters with gradients: 201, Global norm: 0.0659\nParameters with gradients: 201, Global norm: 0.0995\nParameters with gradients: 201, Global norm: 0.1637\nParameters with gradients: 201, Global norm: 0.0639\nParameters with gradients: 201, Global norm: 6.2194\nParameters with gradients: 201, Global norm: 0.1448\nParameters with gradients: 201, Global norm: 0.0458\nParameters with gradients: 201, Global norm: 1.7712\nParameters with gradients: 201, Global norm: 0.3098\nParameters with gradients: 201, Global norm: 0.0583\nParameters with gradients: 201, Global norm: 0.1835\nParameters with gradients: 201, Global norm: 0.2346\nParameters with gradients: 201, Global norm: 1.8088\nParameters with gradients: 201, Global norm: 6.6386\nParameters with gradients: 201, Global norm: 0.0645\nParameters with gradients: 201, Global norm: 2.4779\nParameters with gradients: 201, Global norm: 0.0834\nParameters with gradients: 201, Global norm: 1.3420\nParameters with gradients: 201, Global norm: 33.0563\nParameters with gradients: 201, Global norm: 0.5744\nParameters with gradients: 201, Global norm: 0.1673\nParameters with gradients: 201, Global norm: 0.2854\nParameters with gradients: 201, Global norm: 2.0049\nParameters with gradients: 201, Global norm: 0.1076\nParameters with gradients: 201, Global norm: 2.5269\nParameters with gradients: 201, Global norm: 2.8671\nParameters with gradients: 201, Global norm: 0.2081\nParameters with gradients: 201, Global norm: 0.1245\nParameters with gradients: 201, Global norm: 0.3891\nParameters with gradients: 201, Global norm: 0.6732\nParameters with gradients: 201, Global norm: 0.3919\nParameters with gradients: 201, Global norm: 0.1806\nParameters with gradients: 201, Global norm: 0.0855\nParameters with gradients: 201, Global norm: 0.1140\nParameters with gradients: 201, Global norm: 0.1259\nParameters with gradients: 201, Global norm: 10.9007\nParameters with gradients: 201, Global norm: 4.3506\nParameters with gradients: 201, Global norm: 13.7107\nParameters with gradients: 201, Global norm: 0.3312\nParameters with gradients: 201, Global norm: 0.5897\nParameters with gradients: 201, Global norm: 0.5560\nParameters with gradients: 201, Global norm: 0.5391\nParameters with gradients: 201, Global norm: 11.0129\nParameters with gradients: 201, Global norm: 0.0915\nParameters with gradients: 201, Global norm: 0.2112\nParameters with gradients: 201, Global norm: 0.1472\nParameters with gradients: 201, Global norm: 0.0864\nParameters with gradients: 201, Global norm: 12.5704\nParameters with gradients: 201, Global norm: 2.3504\nParameters with gradients: 201, Global norm: 0.5193\nParameters with gradients: 201, Global norm: 11.0144\nParameters with gradients: 201, Global norm: 0.2786\nParameters with gradients: 201, Global norm: 0.0796\nParameters with gradients: 201, Global norm: 0.1012\nParameters with gradients: 201, Global norm: 0.6677\nParameters with gradients: 201, Global norm: 0.2494\nParameters with gradients: 201, Global norm: 1.1794\nParameters with gradients: 201, Global norm: 1.5415\nParameters with gradients: 201, Global norm: 0.2468\nParameters with gradients: 201, Global norm: 0.1125\nParameters with gradients: 201, Global norm: 7.5062\nParameters with gradients: 201, Global norm: 0.3299\nParameters with gradients: 201, Global norm: 1.6112\nParameters with gradients: 201, Global norm: 0.1482\nParameters with gradients: 201, Global norm: 5.5979\nParameters with gradients: 201, Global norm: 0.1593\nParameters with gradients: 201, Global norm: 0.2039\nParameters with gradients: 201, Global norm: 0.0473\nParameters with gradients: 201, Global norm: 0.0904\nParameters with gradients: 201, Global norm: 0.5740\nParameters with gradients: 201, Global norm: 0.2963\nParameters with gradients: 201, Global norm: 3.9744\nParameters with gradients: 201, Global norm: 0.0726\nParameters with gradients: 201, Global norm: 0.3320\nParameters with gradients: 201, Global norm: 1.2490\nParameters with gradients: 201, Global norm: 10.3690\nParameters with gradients: 201, Global norm: 2.5477\nParameters with gradients: 201, Global norm: 5.1351\nParameters with gradients: 201, Global norm: 0.5594\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:09:18] Energy consumed for RAM : 0.000749 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:09:18] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:09:18] Energy consumed for All CPU : 0.001593 kWh\n[codecarbon INFO @ 08:09:18] Energy consumed for all GPUs : 0.002822 kWh. Total GPU Power : 75.77922993846036 W\n[codecarbon INFO @ 08:09:18] 0.005165 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.3759\nParameters with gradients: 201, Global norm: 0.1341\nParameters with gradients: 201, Global norm: 0.1655\nParameters with gradients: 201, Global norm: 0.1928\nParameters with gradients: 201, Global norm: 0.2063\nParameters with gradients: 201, Global norm: 1.5107\nParameters with gradients: 201, Global norm: 0.2484\nParameters with gradients: 201, Global norm: 0.5167\nParameters with gradients: 201, Global norm: 1.2986\nParameters with gradients: 201, Global norm: 0.1941\nParameters with gradients: 201, Global norm: 1.5299\nParameters with gradients: 201, Global norm: 4.9709\nParameters with gradients: 201, Global norm: 0.1108\nParameters with gradients: 201, Global norm: 0.9025\nParameters with gradients: 201, Global norm: 0.1317\nParameters with gradients: 201, Global norm: 0.3492\nParameters with gradients: 201, Global norm: 9.0721\nParameters with gradients: 201, Global norm: 0.6872\nParameters with gradients: 201, Global norm: 1.2967\nParameters with gradients: 201, Global norm: 16.7493\nParameters with gradients: 201, Global norm: 0.1734\nParameters with gradients: 201, Global norm: 0.2593\nParameters with gradients: 201, Global norm: 0.1983\nParameters with gradients: 201, Global norm: 0.3418\nParameters with gradients: 201, Global norm: 0.3261\nParameters with gradients: 201, Global norm: 0.0950\nParameters with gradients: 201, Global norm: 6.6858\nParameters with gradients: 201, Global norm: 0.1389\nParameters with gradients: 201, Global norm: 0.0981\nParameters with gradients: 201, Global norm: 5.3176\nParameters with gradients: 201, Global norm: 0.1525\nParameters with gradients: 201, Global norm: 0.9632\nParameters with gradients: 201, Global norm: 0.1187\nParameters with gradients: 201, Global norm: 0.3653\nParameters with gradients: 201, Global norm: 0.1220\nParameters with gradients: 201, Global norm: 2.8631\nParameters with gradients: 201, Global norm: 0.0728\nParameters with gradients: 201, Global norm: 0.1171\nParameters with gradients: 201, Global norm: 15.7408\nParameters with gradients: 201, Global norm: 0.2448\nParameters with gradients: 201, Global norm: 3.5274\nParameters with gradients: 201, Global norm: 4.6313\nParameters with gradients: 201, Global norm: 0.1014\nParameters with gradients: 201, Global norm: 0.4579\nParameters with gradients: 201, Global norm: 19.2809\nParameters with gradients: 201, Global norm: 12.1269\nParameters with gradients: 201, Global norm: 0.0715\nParameters with gradients: 201, Global norm: 0.5029\nParameters with gradients: 201, Global norm: 0.2937\nParameters with gradients: 201, Global norm: 0.2097\nParameters with gradients: 201, Global norm: 3.9935\nParameters with gradients: 201, Global norm: 0.1285\nParameters with gradients: 201, Global norm: 2.4465\nParameters with gradients: 201, Global norm: 0.2075\nParameters with gradients: 201, Global norm: 0.2700\nParameters with gradients: 201, Global norm: 0.2340\nParameters with gradients: 201, Global norm: 2.4877\nParameters with gradients: 201, Global norm: 0.1622\nParameters with gradients: 201, Global norm: 0.2222\nParameters with gradients: 201, Global norm: 0.2295\nParameters with gradients: 201, Global norm: 0.2542\nParameters with gradients: 201, Global norm: 0.4739\nParameters with gradients: 201, Global norm: 0.1551\nParameters with gradients: 201, Global norm: 25.6777\nParameters with gradients: 201, Global norm: 0.1877\nParameters with gradients: 201, Global norm: 0.5722\nParameters with gradients: 201, Global norm: 12.9289\nParameters with gradients: 201, Global norm: 0.1914\nParameters with gradients: 201, Global norm: 2.3151\nParameters with gradients: 201, Global norm: 0.6759\nParameters with gradients: 201, Global norm: 1.9844\nParameters with gradients: 201, Global norm: 0.3511\nParameters with gradients: 201, Global norm: 0.2912\nParameters with gradients: 201, Global norm: 1.0512\nParameters with gradients: 201, Global norm: 9.9195\nParameters with gradients: 201, Global norm: 1.3187\nParameters with gradients: 201, Global norm: 0.1173\nParameters with gradients: 201, Global norm: 1.2174\nParameters with gradients: 201, Global norm: 5.6917\nParameters with gradients: 201, Global norm: 4.6684\nParameters with gradients: 201, Global norm: 0.1442\nParameters with gradients: 201, Global norm: 2.6117\nParameters with gradients: 201, Global norm: 0.1865\nParameters with gradients: 201, Global norm: 3.1249\nParameters with gradients: 201, Global norm: 0.2406\nParameters with gradients: 201, Global norm: 0.2974\nParameters with gradients: 201, Global norm: 0.1444\nParameters with gradients: 201, Global norm: 4.1302\nParameters with gradients: 201, Global norm: 4.1385\nParameters with gradients: 201, Global norm: 0.5329\nParameters with gradients: 201, Global norm: 1.4883\nParameters with gradients: 201, Global norm: 0.2436\nParameters with gradients: 201, Global norm: 0.6278\nParameters with gradients: 201, Global norm: 0.2670\nParameters with gradients: 201, Global norm: 8.6305\nParameters with gradients: 201, Global norm: 0.4160\nParameters with gradients: 201, Global norm: 0.7339\nParameters with gradients: 201, Global norm: 0.2931\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:09:33] Energy consumed for RAM : 0.000833 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:09:33] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:09:33] Energy consumed for All CPU : 0.001770 kWh\n[codecarbon INFO @ 08:09:33] Energy consumed for all GPUs : 0.003138 kWh. Total GPU Power : 75.79189472012956 W\n[codecarbon INFO @ 08:09:33] 0.005740 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 1.0159\nParameters with gradients: 201, Global norm: 0.3116\nParameters with gradients: 201, Global norm: 0.2626\nParameters with gradients: 201, Global norm: 7.4899\nParameters with gradients: 201, Global norm: 0.3155\nParameters with gradients: 201, Global norm: 7.9434\nParameters with gradients: 201, Global norm: 0.2618\nParameters with gradients: 201, Global norm: 10.0468\nParameters with gradients: 201, Global norm: 0.6023\nParameters with gradients: 201, Global norm: 4.7252\nParameters with gradients: 201, Global norm: 0.2951\nParameters with gradients: 201, Global norm: 0.2148\nParameters with gradients: 201, Global norm: 2.5621\nParameters with gradients: 201, Global norm: 1.7473\nParameters with gradients: 201, Global norm: 0.5997\nParameters with gradients: 201, Global norm: 0.2058\nParameters with gradients: 201, Global norm: 0.5900\nParameters with gradients: 201, Global norm: 0.2418\nParameters with gradients: 201, Global norm: 0.4648\nParameters with gradients: 201, Global norm: 0.2333\nParameters with gradients: 201, Global norm: 1.3101\nParameters with gradients: 201, Global norm: 6.3210\nParameters with gradients: 201, Global norm: 0.1524\nParameters with gradients: 201, Global norm: 4.7740\nParameters with gradients: 201, Global norm: 1.5123\nParameters with gradients: 201, Global norm: 10.7884\nParameters with gradients: 201, Global norm: 0.6386\nParameters with gradients: 201, Global norm: 0.6732\nParameters with gradients: 201, Global norm: 0.2950\nParameters with gradients: 201, Global norm: 5.1522\nParameters with gradients: 201, Global norm: 4.7544\nParameters with gradients: 201, Global norm: 0.4778\nParameters with gradients: 201, Global norm: 0.3292\nParameters with gradients: 201, Global norm: 4.5900\nParameters with gradients: 201, Global norm: 0.1508\nParameters with gradients: 201, Global norm: 4.4406\nParameters with gradients: 201, Global norm: 0.3614\nParameters with gradients: 201, Global norm: 11.7530\nParameters with gradients: 201, Global norm: 2.9325\nParameters with gradients: 201, Global norm: 0.1191\nParameters with gradients: 201, Global norm: 0.3784\nParameters with gradients: 201, Global norm: 11.2321\nAverage loss: 0.0381\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.9083, F1: 0.9083\n\nEpoch 4/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 4:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.1697\nParameters with gradients: 201, Global norm: 0.4662\nParameters with gradients: 201, Global norm: 0.2035\nParameters with gradients: 201, Global norm: 0.3241\nParameters with gradients: 201, Global norm: 0.6510\nParameters with gradients: 201, Global norm: 0.1301\nParameters with gradients: 201, Global norm: 0.2515\nParameters with gradients: 201, Global norm: 0.3154\nParameters with gradients: 201, Global norm: 0.2195\nParameters with gradients: 201, Global norm: 0.0629\nParameters with gradients: 201, Global norm: 0.2005\nParameters with gradients: 201, Global norm: 0.2046\nParameters with gradients: 201, Global norm: 0.4637\nParameters with gradients: 201, Global norm: 0.1850\nParameters with gradients: 201, Global norm: 0.7726\nParameters with gradients: 201, Global norm: 0.4011\nParameters with gradients: 201, Global norm: 1.3522\nParameters with gradients: 201, Global norm: 0.0659\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:09:48] Energy consumed for RAM : 0.000916 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:09:48] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:09:48] Energy consumed for All CPU : 0.001947 kWh\n[codecarbon INFO @ 08:09:48] Energy consumed for all GPUs : 0.003456 kWh. Total GPU Power : 76.35073519249377 W\n[codecarbon INFO @ 08:09:48] 0.006319 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.1054\nParameters with gradients: 201, Global norm: 0.1120\nParameters with gradients: 201, Global norm: 0.0781\nParameters with gradients: 201, Global norm: 1.1739\nParameters with gradients: 201, Global norm: 1.4154\nParameters with gradients: 201, Global norm: 0.0837\nParameters with gradients: 201, Global norm: 2.7234\nParameters with gradients: 201, Global norm: 0.9631\nParameters with gradients: 201, Global norm: 0.7684\nParameters with gradients: 201, Global norm: 0.1445\nParameters with gradients: 201, Global norm: 0.0658\nParameters with gradients: 201, Global norm: 1.6426\nParameters with gradients: 201, Global norm: 0.3736\nParameters with gradients: 201, Global norm: 0.5484\nParameters with gradients: 201, Global norm: 0.2267\nParameters with gradients: 201, Global norm: 0.1799\nParameters with gradients: 201, Global norm: 0.3285\nParameters with gradients: 201, Global norm: 0.0920\nParameters with gradients: 201, Global norm: 0.6173\nParameters with gradients: 201, Global norm: 0.5374\nParameters with gradients: 201, Global norm: 0.1553\nParameters with gradients: 201, Global norm: 0.3366\nParameters with gradients: 201, Global norm: 0.0710\nParameters with gradients: 201, Global norm: 0.1578\nParameters with gradients: 201, Global norm: 0.1694\nParameters with gradients: 201, Global norm: 0.9915\nParameters with gradients: 201, Global norm: 0.1952\nParameters with gradients: 201, Global norm: 0.0860\nParameters with gradients: 201, Global norm: 0.1290\nParameters with gradients: 201, Global norm: 0.0939\nParameters with gradients: 201, Global norm: 0.1396\nParameters with gradients: 201, Global norm: 0.0814\nParameters with gradients: 201, Global norm: 0.1192\nParameters with gradients: 201, Global norm: 0.0893\nParameters with gradients: 201, Global norm: 0.0987\nParameters with gradients: 201, Global norm: 0.0462\nParameters with gradients: 201, Global norm: 1.2099\nParameters with gradients: 201, Global norm: 0.0638\nParameters with gradients: 201, Global norm: 0.3708\nParameters with gradients: 201, Global norm: 0.0765\nParameters with gradients: 201, Global norm: 0.0767\nParameters with gradients: 201, Global norm: 0.0815\nParameters with gradients: 201, Global norm: 0.1363\nParameters with gradients: 201, Global norm: 2.7065\nUpdated grad_norm_threshold: 3.4359 (MA grad_norm: 2.7065)\nParameters with gradients: 201, Global norm: 0.0558\nUpdated grad_norm_threshold: 1.5866 (MA grad_norm: 1.3812)\nParameters with gradients: 201, Global norm: 0.1250\nUpdated grad_norm_threshold: 1.0249 (MA grad_norm: 0.9625)\nParameters with gradients: 201, Global norm: 0.0508\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7346)\nParameters with gradients: 201, Global norm: 0.7978\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7472)\nParameters with gradients: 201, Global norm: 2.8133\nUpdated grad_norm_threshold: 1.0824 (MA grad_norm: 1.0915)\nParameters with gradients: 201, Global norm: 0.0535\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9432)\nParameters with gradients: 201, Global norm: 0.1268\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8412)\nParameters with gradients: 201, Global norm: 0.1522\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7646)\nParameters with gradients: 201, Global norm: 3.1428\nUpdated grad_norm_threshold: 1.0022 (MA grad_norm: 1.0025)\nParameters with gradients: 201, Global norm: 0.6966\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9746)\nParameters with gradients: 201, Global norm: 0.1176\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9032)\nParameters with gradients: 201, Global norm: 0.1442\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8448)\nParameters with gradients: 201, Global norm: 0.1074\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7922)\nParameters with gradients: 201, Global norm: 0.3266\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7611)\nParameters with gradients: 201, Global norm: 0.1653\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7239)\nParameters with gradients: 201, Global norm: 0.1557\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6905)\nParameters with gradients: 201, Global norm: 0.0773\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6564)\nParameters with gradients: 201, Global norm: 0.1966\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6322)\nParameters with gradients: 201, Global norm: 0.3398\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6176)\nParameters with gradients: 201, Global norm: 0.0891\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4867)\nParameters with gradients: 201, Global norm: 2.4890\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6084)\nParameters with gradients: 201, Global norm: 0.1815\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6112)\nParameters with gradients: 201, Global norm: 0.2208\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6197)\nParameters with gradients: 201, Global norm: 0.2941\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5945)\nParameters with gradients: 201, Global norm: 0.1464\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4612)\nParameters with gradients: 201, Global norm: 0.1893\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4679)\nParameters with gradients: 201, Global norm: 0.4536\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4843)\nParameters with gradients: 201, Global norm: 0.6172\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5075)\nParameters with gradients: 201, Global norm: 0.2135\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3611)\nParameters with gradients: 201, Global norm: 0.3234\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3424)\nParameters with gradients: 201, Global norm: 0.2139\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3472)\nParameters with gradients: 201, Global norm: 0.2435\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3522)\nParameters with gradients: 201, Global norm: 0.3295\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3633)\nParameters with gradients: 201, Global norm: 0.2152\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3577)\nParameters with gradients: 201, Global norm: 0.4823\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3736)\nParameters with gradients: 201, Global norm: 0.1376\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3727)\nParameters with gradients: 201, Global norm: 0.1784\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3777)\nParameters with gradients: 201, Global norm: 0.3009\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3829)\nParameters with gradients: 201, Global norm: 0.1979\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3759)\nParameters with gradients: 201, Global norm: 0.2053\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3817)\nParameters with gradients: 201, Global norm: 0.0863\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2615)\n‚ö° Epoch 4 | Layer 0 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 1 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 2 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 3 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 4 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 5 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 6 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 7 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 8 switched to 8-bit (MA grad_norm=0.2615)\n‚ö° Epoch 4 | Layer 9 switched to 8-bit (MA grad_norm=0.2615)\nParameters with gradients: 201, Global norm: 0.1226\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2586)\nParameters with gradients: 201, Global norm: 0.1562\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2554)\nParameters with gradients: 201, Global norm: 0.0872\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2450)\nParameters with gradients: 201, Global norm: 3.0778\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3916)\nParameters with gradients: 201, Global norm: 0.0758\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3859)\nParameters with gradients: 201, Global norm: 0.2048\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3735)\nParameters with gradients: 201, Global norm: 0.3130\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3583)\nParameters with gradients: 201, Global norm: 0.3500\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3651)\nParameters with gradients: 201, Global norm: 0.1279\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3553)\nParameters with gradients: 201, Global norm: 0.0868\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3489)\nParameters with gradients: 201, Global norm: 0.1771\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3456)\nParameters with gradients: 201, Global norm: 0.1958\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3389)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:10:03] Energy consumed for RAM : 0.000999 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:10:03] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:10:03] Energy consumed for All CPU : 0.002124 kWh\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.1433\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3354)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:10:03] Energy consumed for all GPUs : 0.003771 kWh. Total GPU Power : 75.70866592586512 W\n[codecarbon INFO @ 08:10:03] 0.006894 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.2062\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3215)\nParameters with gradients: 201, Global norm: 0.3355\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3314)\nParameters with gradients: 201, Global norm: 0.2495\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3350)\nParameters with gradients: 201, Global norm: 2.3510\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4375)\nParameters with gradients: 201, Global norm: 1.1773\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4865)\nParameters with gradients: 201, Global norm: 35.3831\nUpdated grad_norm_threshold: 2.1208 (MA grad_norm: 2.2454)\nParameters with gradients: 201, Global norm: 0.1899\nUpdated grad_norm_threshold: 2.2376 (MA grad_norm: 2.2505)\nParameters with gradients: 201, Global norm: 0.1162\nUpdated grad_norm_threshold: 2.2490 (MA grad_norm: 2.2502)\nParameters with gradients: 201, Global norm: 0.1503\nUpdated grad_norm_threshold: 2.2498 (MA grad_norm: 2.2499)\nParameters with gradients: 201, Global norm: 0.8949\nUpdated grad_norm_threshold: 2.2863 (MA grad_norm: 2.2903)\nParameters with gradients: 201, Global norm: 0.3864\nUpdated grad_norm_threshold: 2.1688 (MA grad_norm: 2.1557)\nParameters with gradients: 201, Global norm: 0.0981\nUpdated grad_norm_threshold: 2.1580 (MA grad_norm: 2.1569)\nParameters with gradients: 201, Global norm: 0.4288\nUpdated grad_norm_threshold: 2.1671 (MA grad_norm: 2.1681)\nParameters with gradients: 201, Global norm: 0.0786\nUpdated grad_norm_threshold: 2.1574 (MA grad_norm: 2.1563)\nParameters with gradients: 201, Global norm: 0.1410\nUpdated grad_norm_threshold: 2.1470 (MA grad_norm: 2.1459)\nParameters with gradients: 201, Global norm: 0.2186\nUpdated grad_norm_threshold: 2.1501 (MA grad_norm: 2.1504)\nParameters with gradients: 201, Global norm: 0.9053\nUpdated grad_norm_threshold: 2.1872 (MA grad_norm: 2.1914)\nParameters with gradients: 201, Global norm: 6.9114\nUpdated grad_norm_threshold: 2.4940 (MA grad_norm: 2.5281)\nParameters with gradients: 201, Global norm: 0.1193\nUpdated grad_norm_threshold: 2.5212 (MA grad_norm: 2.5242)\nParameters with gradients: 201, Global norm: 0.3908\nUpdated grad_norm_threshold: 2.5351 (MA grad_norm: 2.5366)\nParameters with gradients: 201, Global norm: 0.0945\nUpdated grad_norm_threshold: 2.5314 (MA grad_norm: 2.5310)\nParameters with gradients: 201, Global norm: 0.1129\nUpdated grad_norm_threshold: 2.5210 (MA grad_norm: 2.5199)\nParameters with gradients: 201, Global norm: 0.3485\nUpdated grad_norm_threshold: 2.5245 (MA grad_norm: 2.5248)\nParameters with gradients: 201, Global norm: 0.1306\nUpdated grad_norm_threshold: 2.4249 (MA grad_norm: 2.4138)\nParameters with gradients: 201, Global norm: 0.2845\nUpdated grad_norm_threshold: 2.3748 (MA grad_norm: 2.3692)\nParameters with gradients: 201, Global norm: 0.1985\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6100)\nParameters with gradients: 201, Global norm: 0.3260\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6168)\nParameters with gradients: 201, Global norm: 5.0457\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8632)\nParameters with gradients: 201, Global norm: 0.0567\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8586)\nParameters with gradients: 201, Global norm: 0.0412\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8159)\nParameters with gradients: 201, Global norm: 0.5181\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8225)\nParameters with gradients: 201, Global norm: 0.3410\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8346)\nParameters with gradients: 201, Global norm: 0.0803\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8172)\nParameters with gradients: 201, Global norm: 0.0878\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8176)\nParameters with gradients: 201, Global norm: 0.0835\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8148)\nParameters with gradients: 201, Global norm: 19.9825\nUpdated grad_norm_threshold: 1.7227 (MA grad_norm: 1.8029)\nParameters with gradients: 201, Global norm: 0.2652\nUpdated grad_norm_threshold: 1.7661 (MA grad_norm: 1.7709)\nParameters with gradients: 201, Global norm: 0.0735\nUpdated grad_norm_threshold: 1.4627 (MA grad_norm: 1.4290)\nParameters with gradients: 201, Global norm: 0.0493\nUpdated grad_norm_threshold: 1.4293 (MA grad_norm: 1.4255)\nParameters with gradients: 201, Global norm: 0.0796\nUpdated grad_norm_threshold: 1.4119 (MA grad_norm: 1.4100)\nParameters with gradients: 201, Global norm: 0.0383\nUpdated grad_norm_threshold: 1.4076 (MA grad_norm: 1.4072)\nParameters with gradients: 201, Global norm: 0.0690\nUpdated grad_norm_threshold: 1.4052 (MA grad_norm: 1.4050)\nParameters with gradients: 201, Global norm: 5.1338\nUpdated grad_norm_threshold: 1.6203 (MA grad_norm: 1.6442)\nParameters with gradients: 201, Global norm: 0.3784\nUpdated grad_norm_threshold: 1.6530 (MA grad_norm: 1.6566)\nParameters with gradients: 201, Global norm: 0.0643\nUpdated grad_norm_threshold: 1.6464 (MA grad_norm: 1.6456)\nParameters with gradients: 201, Global norm: 0.0847\nUpdated grad_norm_threshold: 1.6406 (MA grad_norm: 1.6399)\nParameters with gradients: 201, Global norm: 0.0602\nUpdated grad_norm_threshold: 1.6280 (MA grad_norm: 1.6266)\nParameters with gradients: 201, Global norm: 0.1215\nUpdated grad_norm_threshold: 1.4052 (MA grad_norm: 1.3804)\nParameters with gradients: 201, Global norm: 0.0968\nUpdated grad_norm_threshold: 1.3847 (MA grad_norm: 1.3824)\nParameters with gradients: 201, Global norm: 0.3529\nUpdated grad_norm_threshold: 1.3967 (MA grad_norm: 1.3980)\nParameters with gradients: 201, Global norm: 0.5729\nUpdated grad_norm_threshold: 1.4004 (MA grad_norm: 1.4008)\nParameters with gradients: 201, Global norm: 0.0608\nUpdated grad_norm_threshold: 1.3881 (MA grad_norm: 1.3868)\nParameters with gradients: 201, Global norm: 0.0842\nUpdated grad_norm_threshold: 1.3871 (MA grad_norm: 1.3870)\nParameters with gradients: 201, Global norm: 0.1242\nUpdated grad_norm_threshold: 1.3886 (MA grad_norm: 1.3888)\nParameters with gradients: 201, Global norm: 0.0744\nUpdated grad_norm_threshold: 1.3883 (MA grad_norm: 1.3883)\nParameters with gradients: 201, Global norm: 0.0457\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3915)\nParameters with gradients: 201, Global norm: 7.3349\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7450)\nParameters with gradients: 201, Global norm: 0.0630\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7444)\nParameters with gradients: 201, Global norm: 0.0595\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7449)\nParameters with gradients: 201, Global norm: 0.0687\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7444)\nParameters with gradients: 201, Global norm: 1.4012\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8125)\nParameters with gradients: 201, Global norm: 0.6851\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8434)\nParameters with gradients: 201, Global norm: 0.0817\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5907)\nParameters with gradients: 201, Global norm: 0.0409\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5739)\nParameters with gradients: 201, Global norm: 0.3099\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5862)\nParameters with gradients: 201, Global norm: 0.0532\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5846)\nParameters with gradients: 201, Global norm: 0.0364\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5834)\nParameters with gradients: 201, Global norm: 0.0724\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5809)\nParameters with gradients: 201, Global norm: 0.0452\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5784)\nParameters with gradients: 201, Global norm: 0.2061\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5710)\nParameters with gradients: 201, Global norm: 0.1248\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5486)\nParameters with gradients: 201, Global norm: 0.0429\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5477)\nParameters with gradients: 201, Global norm: 0.1065\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5488)\nParameters with gradients: 201, Global norm: 0.0388\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5446)\nParameters with gradients: 201, Global norm: 0.0511\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5434)\nParameters with gradients: 201, Global norm: 0.0810\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5452)\nParameters with gradients: 201, Global norm: 0.2137\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1891)\n‚ö° Epoch 4 | Layer 0 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 1 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 2 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 3 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 4 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 5 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 6 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 7 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 8 switched to 4-bit (MA grad_norm=0.1891)\n‚ö° Epoch 4 | Layer 9 switched to 4-bit (MA grad_norm=0.1891)\nParameters with gradients: 201, Global norm: 0.0497\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1884)\nParameters with gradients: 201, Global norm: 0.1908\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1950)\nParameters with gradients: 201, Global norm: 0.0339\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1933)\nParameters with gradients: 201, Global norm: 0.1400\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1302)\nParameters with gradients: 201, Global norm: 0.2074\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1063)\nParameters with gradients: 201, Global norm: 0.1874\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1116)\nParameters with gradients: 201, Global norm: 0.0518\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1121)\nParameters with gradients: 201, Global norm: 0.0968\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1015)\nParameters with gradients: 201, Global norm: 0.0711\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1024)\nParameters with gradients: 201, Global norm: 0.7301\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1371)\nParameters with gradients: 201, Global norm: 0.1582\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1414)\nParameters with gradients: 201, Global norm: 0.0351\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1409)\nParameters with gradients: 201, Global norm: 4.1979\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3404)\nParameters with gradients: 201, Global norm: 0.0771\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3381)\nParameters with gradients: 201, Global norm: 0.6622\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3690)\nParameters with gradients: 201, Global norm: 0.0464\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3660)\nParameters with gradients: 201, Global norm: 0.4937\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3888)\nParameters with gradients: 201, Global norm: 0.0462\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3885)\nParameters with gradients: 201, Global norm: 0.0366\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3863)\nParameters with gradients: 201, Global norm: 0.3624\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3937)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:10:18] Energy consumed for RAM : 0.001083 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:10:18] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:10:18] Energy consumed for All CPU : 0.002301 kWh\n[codecarbon INFO @ 08:10:18] Energy consumed for all GPUs : 0.004087 kWh. Total GPU Power : 75.72787585717391 W\n[codecarbon INFO @ 08:10:18] 0.007470 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 4.4736\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6149)\nParameters with gradients: 201, Global norm: 0.0187\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6063)\nParameters with gradients: 201, Global norm: 0.0423\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6067)\nParameters with gradients: 201, Global norm: 0.3949\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6195)\nParameters with gradients: 201, Global norm: 0.1483\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6165)\nParameters with gradients: 201, Global norm: 1.2742\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6709)\nParameters with gradients: 201, Global norm: 0.9517\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7159)\nParameters with gradients: 201, Global norm: 0.1273\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7174)\nParameters with gradients: 201, Global norm: 0.0230\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7150)\nParameters with gradients: 201, Global norm: 0.0298\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6800)\nParameters with gradients: 201, Global norm: 0.0826\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6762)\nParameters with gradients: 201, Global norm: 0.0290\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6759)\nParameters with gradients: 201, Global norm: 0.0885\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4704)\nParameters with gradients: 201, Global norm: 0.0268\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4679)\nParameters with gradients: 201, Global norm: 0.0912\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4394)\nParameters with gradients: 201, Global norm: 0.0707\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4406)\nParameters with gradients: 201, Global norm: 0.0787\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4198)\nParameters with gradients: 201, Global norm: 1.9602\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5155)\nParameters with gradients: 201, Global norm: 0.1523\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5213)\nParameters with gradients: 201, Global norm: 0.0277\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5046)\nParameters with gradients: 201, Global norm: 0.1090\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2863)\nParameters with gradients: 201, Global norm: 0.0325\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2870)\nParameters with gradients: 201, Global norm: 0.0225\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2860)\nParameters with gradients: 201, Global norm: 0.0247\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2675)\nParameters with gradients: 201, Global norm: 0.0228\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2613)\nParameters with gradients: 201, Global norm: 0.0315\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1991)\nParameters with gradients: 201, Global norm: 1.3689\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2200)\nParameters with gradients: 201, Global norm: 0.0229\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2148)\nParameters with gradients: 201, Global norm: 0.0528\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2162)\nParameters with gradients: 201, Global norm: 0.0361\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2166)\nParameters with gradients: 201, Global norm: 0.1391\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2194)\nParameters with gradients: 201, Global norm: 2.3186\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3339)\nParameters with gradients: 201, Global norm: 0.0978\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3343)\nParameters with gradients: 201, Global norm: 0.0284\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3344)\nParameters with gradients: 201, Global norm: 0.0268\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3312)\nParameters with gradients: 201, Global norm: 0.0257\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3290)\nParameters with gradients: 201, Global norm: 0.4499\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3475)\nParameters with gradients: 201, Global norm: 9.8519\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7421)\nParameters with gradients: 201, Global norm: 0.0396\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7365)\nParameters with gradients: 201, Global norm: 0.0722\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7387)\nParameters with gradients: 201, Global norm: 0.0750\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7370)\nParameters with gradients: 201, Global norm: 0.0798\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7394)\nParameters with gradients: 201, Global norm: 0.1041\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7434)\nParameters with gradients: 201, Global norm: 0.1288\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7486)\nParameters with gradients: 201, Global norm: 0.1391\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7545)\nParameters with gradients: 201, Global norm: 0.0807\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7569)\nParameters with gradients: 201, Global norm: 0.1783\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6974)\nParameters with gradients: 201, Global norm: 0.1450\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7035)\nParameters with gradients: 201, Global norm: 0.0927\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7055)\nParameters with gradients: 201, Global norm: 0.0402\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7057)\nParameters with gradients: 201, Global norm: 0.0316\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7003)\nParameters with gradients: 201, Global norm: 0.1582\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5923)\nParameters with gradients: 201, Global norm: 0.0250\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5887)\nParameters with gradients: 201, Global norm: 0.8802\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6312)\nParameters with gradients: 201, Global norm: 0.4245\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6511)\nParameters with gradients: 201, Global norm: 0.0323\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6515)\nParameters with gradients: 201, Global norm: 0.1975\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6388)\nParameters with gradients: 201, Global norm: 0.3563\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1641)\nParameters with gradients: 201, Global norm: 0.1064\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1674)\nParameters with gradients: 201, Global norm: 0.0602\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1668)\nParameters with gradients: 201, Global norm: 0.0453\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1653)\nParameters with gradients: 201, Global norm: 0.0504\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1638)\nParameters with gradients: 201, Global norm: 0.0405\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1607)\nParameters with gradients: 201, Global norm: 0.0325\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1558)\nParameters with gradients: 201, Global norm: 0.0565\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1517)\nParameters with gradients: 201, Global norm: 0.0261\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1490)\nParameters with gradients: 201, Global norm: 0.0297\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1416)\nParameters with gradients: 201, Global norm: 0.1904\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1438)\nParameters with gradients: 201, Global norm: 0.0828\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1433)\nParameters with gradients: 201, Global norm: 0.1003\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1463)\nParameters with gradients: 201, Global norm: 0.1042\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1500)\nParameters with gradients: 201, Global norm: 0.0931\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1467)\nParameters with gradients: 201, Global norm: 0.1353\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1522)\nParameters with gradients: 201, Global norm: 0.2057\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1185)\nParameters with gradients: 201, Global norm: 0.0960\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1021)\nParameters with gradients: 201, Global norm: 0.1082\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1059)\nParameters with gradients: 201, Global norm: 0.0450\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0983)\nParameters with gradients: 201, Global norm: 0.0263\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0818)\nParameters with gradients: 201, Global norm: 2.8891\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2209)\nParameters with gradients: 201, Global norm: 0.1148\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2236)\nParameters with gradients: 201, Global norm: 1.8918\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3159)\nParameters with gradients: 201, Global norm: 0.3549\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3312)\nParameters with gradients: 201, Global norm: 1.9707\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4277)\nParameters with gradients: 201, Global norm: 0.0471\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4284)\nParameters with gradients: 201, Global norm: 0.1431\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4327)\nParameters with gradients: 201, Global norm: 0.1610\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4395)\nParameters with gradients: 201, Global norm: 0.8980\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4829)\nParameters with gradients: 201, Global norm: 0.0213\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4744)\nParameters with gradients: 201, Global norm: 0.0256\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4716)\nParameters with gradients: 201, Global norm: 0.0178\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4675)\nParameters with gradients: 201, Global norm: 0.0371\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4641)\nParameters with gradients: 201, Global norm: 0.0334\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4611)\nParameters with gradients: 201, Global norm: 0.0495\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4568)\nParameters with gradients: 201, Global norm: 0.0937\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4512)\nParameters with gradients: 201, Global norm: 0.0386\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4483)\nParameters with gradients: 201, Global norm: 0.0372\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4448)\nParameters with gradients: 201, Global norm: 0.0586\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4455)\nParameters with gradients: 201, Global norm: 0.0482\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4466)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:10:33] Energy consumed for RAM : 0.001166 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:10:33] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:10:33] Energy consumed for All CPU : 0.002478 kWh\n[codecarbon INFO @ 08:10:33] Energy consumed for all GPUs : 0.004402 kWh. Total GPU Power : 75.67596455965149 W\n[codecarbon INFO @ 08:10:33] 0.008045 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.0805\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3061)\nParameters with gradients: 201, Global norm: 0.0503\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3029)\nAverage loss: 0.0109\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.9060, F1: 0.9060\n\nEpoch 5/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 5:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.2191\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3693)\nParameters with gradients: 201, Global norm: 0.0507\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3541)\nParameters with gradients: 201, Global norm: 0.0668\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2589)\nParameters with gradients: 201, Global norm: 0.0286\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2579)\nParameters with gradients: 201, Global norm: 0.0302\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2523)\nParameters with gradients: 201, Global norm: 0.1308\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2508)\nParameters with gradients: 201, Global norm: 0.1160\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2117)\nParameters with gradients: 201, Global norm: 0.1492\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2181)\nParameters with gradients: 201, Global norm: 0.0498\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2193)\nParameters with gradients: 201, Global norm: 0.0662\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2217)\nParameters with gradients: 201, Global norm: 0.0492\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2223)\nParameters with gradients: 201, Global norm: 0.0457\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2229)\nParameters with gradients: 201, Global norm: 0.0721\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2241)\nParameters with gradients: 201, Global norm: 0.0279\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2208)\nParameters with gradients: 201, Global norm: 0.0334\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2205)\nParameters with gradients: 201, Global norm: 0.0868\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2230)\nParameters with gradients: 201, Global norm: 0.1166\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2259)\nParameters with gradients: 201, Global norm: 0.0581\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2264)\nParameters with gradients: 201, Global norm: 0.0386\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2243)\nParameters with gradients: 201, Global norm: 0.0350\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2235)\nParameters with gradients: 201, Global norm: 0.0537\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0653)\nParameters with gradients: 201, Global norm: 0.0697\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0662)\nParameters with gradients: 201, Global norm: 0.0362\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0647)\nParameters with gradients: 201, Global norm: 0.0405\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0653)\nParameters with gradients: 201, Global norm: 0.1653\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0720)\nParameters with gradients: 201, Global norm: 0.1587\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0734)\nParameters with gradients: 201, Global norm: 0.1364\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0745)\nParameters with gradients: 201, Global norm: 0.1470\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0743)\nParameters with gradients: 201, Global norm: 0.4556\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0946)\nParameters with gradients: 201, Global norm: 0.0452\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0936)\nParameters with gradients: 201, Global norm: 0.0501\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0936)\nParameters with gradients: 201, Global norm: 0.0263\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0927)\nParameters with gradients: 201, Global norm: 0.0835\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0932)\nParameters with gradients: 201, Global norm: 0.1305\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0984)\nParameters with gradients: 201, Global norm: 0.0464\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0990)\nParameters with gradients: 201, Global norm: 0.0370\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0965)\nParameters with gradients: 201, Global norm: 0.1030\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0959)\nParameters with gradients: 201, Global norm: 0.0205\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0940)\nParameters with gradients: 201, Global norm: 0.0573\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0949)\nParameters with gradients: 201, Global norm: 0.1111\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0987)\nParameters with gradients: 201, Global norm: 0.0509\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0986)\nParameters with gradients: 201, Global norm: 0.0266\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0964)\nParameters with gradients: 201, Global norm: 0.5082\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1200)\nParameters with gradients: 201, Global norm: 0.1220\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1241)\nParameters with gradients: 201, Global norm: 0.0638\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1190)\nParameters with gradients: 201, Global norm: 0.0433\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1132)\nParameters with gradients: 201, Global norm: 0.0577\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1093)\nParameters with gradients: 201, Global norm: 0.0765\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1058)\nParameters with gradients: 201, Global norm: 0.0360\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0848)\nParameters with gradients: 201, Global norm: 0.2138\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0932)\nParameters with gradients: 201, Global norm: 0.0355\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0925)\nParameters with gradients: 201, Global norm: 0.5227\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1173)\nParameters with gradients: 201, Global norm: 0.0527\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1158)\nParameters with gradients: 201, Global norm: 0.1417\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1163)\nParameters with gradients: 201, Global norm: 0.0664\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1173)\nParameters with gradients: 201, Global norm: 0.0605\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1185)\nParameters with gradients: 201, Global norm: 0.1698\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1219)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:10:48] Energy consumed for RAM : 0.001249 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:10:48] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:10:48] Energy consumed for All CPU : 0.002655 kWh\n[codecarbon INFO @ 08:10:48] Energy consumed for all GPUs : 0.004722 kWh. Total GPU Power : 76.97977930549963 W\n[codecarbon INFO @ 08:10:48] 0.008626 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.0428\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1230)\nParameters with gradients: 201, Global norm: 0.0342\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1218)\nParameters with gradients: 201, Global norm: 0.0539\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1190)\nParameters with gradients: 201, Global norm: 0.0333\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1181)\nParameters with gradients: 201, Global norm: 0.0344\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1185)\nParameters with gradients: 201, Global norm: 0.0306\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0946)\nParameters with gradients: 201, Global norm: 0.1435\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0957)\nParameters with gradients: 201, Global norm: 0.0328\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0941)\nParameters with gradients: 201, Global norm: 0.0293\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0934)\nParameters with gradients: 201, Global norm: 0.0567\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0934)\nParameters with gradients: 201, Global norm: 0.0287\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0910)\nParameters with gradients: 201, Global norm: 0.1440\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0964)\nParameters with gradients: 201, Global norm: 0.0783\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0896)\nParameters with gradients: 201, Global norm: 0.0694\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0913)\nParameters with gradients: 201, Global norm: 0.0714\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0687)\nParameters with gradients: 201, Global norm: 0.0360\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0679)\nParameters with gradients: 201, Global norm: 0.0373\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0627)\nParameters with gradients: 201, Global norm: 0.0816\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0634)\nParameters with gradients: 201, Global norm: 0.0904\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0649)\nParameters with gradients: 201, Global norm: 0.0306\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0580)\nParameters with gradients: 201, Global norm: 0.5727\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0845)\nParameters with gradients: 201, Global norm: 0.0457\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0850)\nParameters with gradients: 201, Global norm: 0.0371\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0842)\nParameters with gradients: 201, Global norm: 0.0233\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0837)\nParameters with gradients: 201, Global norm: 0.5793\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1109)\nParameters with gradients: 201, Global norm: 0.0453\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1117)\nParameters with gradients: 201, Global norm: 0.0735\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1082)\nParameters with gradients: 201, Global norm: 0.0232\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1077)\nParameters with gradients: 201, Global norm: 0.0297\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1077)\nParameters with gradients: 201, Global norm: 0.1175\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1108)\nParameters with gradients: 201, Global norm: 0.0351\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1111)\nParameters with gradients: 201, Global norm: 5.2115\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3645)\nParameters with gradients: 201, Global norm: 0.2432\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3727)\nParameters with gradients: 201, Global norm: 0.0390\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3712)\nParameters with gradients: 201, Global norm: 0.0722\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3712)\nParameters with gradients: 201, Global norm: 0.0760\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3732)\nParameters with gradients: 201, Global norm: 0.0276\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3727)\nParameters with gradients: 201, Global norm: 0.0664\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3720)\nParameters with gradients: 201, Global norm: 0.0673\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3708)\nParameters with gradients: 201, Global norm: 0.0281\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3707)\nParameters with gradients: 201, Global norm: 0.1103\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3476)\nParameters with gradients: 201, Global norm: 0.0397\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3473)\nParameters with gradients: 201, Global norm: 0.0463\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3477)\nParameters with gradients: 201, Global norm: 0.1025\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3517)\nParameters with gradients: 201, Global norm: 0.0411\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3248)\nParameters with gradients: 201, Global norm: 0.0391\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3245)\nParameters with gradients: 201, Global norm: 0.0244\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3220)\nParameters with gradients: 201, Global norm: 0.0904\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3254)\nParameters with gradients: 201, Global norm: 0.0553\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3267)\nParameters with gradients: 201, Global norm: 0.0303\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3223)\nParameters with gradients: 201, Global norm: 0.0576\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3234)\nParameters with gradients: 201, Global norm: 0.0817\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0669)\nParameters with gradients: 201, Global norm: 0.0443\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0570)\nParameters with gradients: 201, Global norm: 0.0241\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0562)\nParameters with gradients: 201, Global norm: 0.0232\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0538)\nParameters with gradients: 201, Global norm: 0.0275\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0514)\nParameters with gradients: 201, Global norm: 0.0953\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0547)\nParameters with gradients: 201, Global norm: 0.0269\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0528)\nParameters with gradients: 201, Global norm: 0.0510\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0520)\nParameters with gradients: 201, Global norm: 0.0312\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0521)\nParameters with gradients: 201, Global norm: 0.0824\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0507)\nParameters with gradients: 201, Global norm: 0.0668\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0521)\nParameters with gradients: 201, Global norm: 0.0602\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0528)\nParameters with gradients: 201, Global norm: 0.0536\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0503)\nParameters with gradients: 201, Global norm: 0.0750\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0520)\nParameters with gradients: 201, Global norm: 0.0217\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0511)\nParameters with gradients: 201, Global norm: 0.3202\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0659)\nParameters with gradients: 201, Global norm: 0.0510\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0640)\nParameters with gradients: 201, Global norm: 0.0719\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0648)\nParameters with gradients: 201, Global norm: 0.0246\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0645)\nParameters with gradients: 201, Global norm: 0.0337\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0633)\nParameters with gradients: 201, Global norm: 0.0516\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0618)\nParameters with gradients: 201, Global norm: 0.0736\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0633)\nParameters with gradients: 201, Global norm: 0.4586\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0850)\nParameters with gradients: 201, Global norm: 0.1505\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0914)\nParameters with gradients: 201, Global norm: 0.0860\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0943)\nParameters with gradients: 201, Global norm: 0.0882\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0939)\nParameters with gradients: 201, Global norm: 0.0476\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0950)\nParameters with gradients: 201, Global norm: 0.0562\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0952)\nParameters with gradients: 201, Global norm: 0.0306\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0952)\nParameters with gradients: 201, Global norm: 0.0928\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0957)\nParameters with gradients: 201, Global norm: 0.0329\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0940)\nParameters with gradients: 201, Global norm: 0.0310\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0926)\nParameters with gradients: 201, Global norm: 0.0244\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0911)\nParameters with gradients: 201, Global norm: 0.0358\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0891)\nParameters with gradients: 201, Global norm: 0.0231\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0892)\nParameters with gradients: 201, Global norm: 0.0828\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0773)\nParameters with gradients: 201, Global norm: 0.2035\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0850)\nParameters with gradients: 201, Global norm: 0.1022\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0865)\nParameters with gradients: 201, Global norm: 0.1728\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0939)\nParameters with gradients: 201, Global norm: 0.0528\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0948)\nParameters with gradients: 201, Global norm: 0.2327\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1039)\nParameters with gradients: 201, Global norm: 4.9609\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3483)\nParameters with gradients: 201, Global norm: 0.0326\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3270)\nParameters with gradients: 201, Global norm: 0.2753\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3332)\nParameters with gradients: 201, Global norm: 0.0235\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3301)\nParameters with gradients: 201, Global norm: 0.0596\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3286)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:11:03] Energy consumed for RAM : 0.001332 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:11:03] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:11:03] Energy consumed for All CPU : 0.002832 kWh\n[codecarbon INFO @ 08:11:03] Energy consumed for all GPUs : 0.005037 kWh. Total GPU Power : 75.4906420160068 W\n[codecarbon INFO @ 08:11:03] 0.009201 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:11:03] 0.014200 g.CO2eq/s mean an estimation of 447.822039800175 kg.CO2eq/year\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.0378\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3282)\nParameters with gradients: 201, Global norm: 0.0198\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3263)\nParameters with gradients: 201, Global norm: 0.0716\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3284)\nParameters with gradients: 201, Global norm: 0.0411\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3258)\nParameters with gradients: 201, Global norm: 0.0234\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3253)\nParameters with gradients: 201, Global norm: 0.0248\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3250)\nParameters with gradients: 201, Global norm: 0.0772\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3277)\nParameters with gradients: 201, Global norm: 0.1290\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3323)\nParameters with gradients: 201, Global norm: 0.0580\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3341)\nParameters with gradients: 201, Global norm: 0.1880\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3393)\nParameters with gradients: 201, Global norm: 0.1758\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3379)\nParameters with gradients: 201, Global norm: 0.0382\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3347)\nParameters with gradients: 201, Global norm: 0.0599\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3291)\nParameters with gradients: 201, Global norm: 0.0657\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3297)\nParameters with gradients: 201, Global norm: 0.0450\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3204)\nParameters with gradients: 201, Global norm: 0.0195\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0733)\nParameters with gradients: 201, Global norm: 0.0268\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0730)\nParameters with gradients: 201, Global norm: 0.0288\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0607)\nParameters with gradients: 201, Global norm: 0.0454\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0618)\nParameters with gradients: 201, Global norm: 0.0281\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0602)\nParameters with gradients: 201, Global norm: 0.0458\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0606)\nParameters with gradients: 201, Global norm: 0.0549\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0624)\nParameters with gradients: 201, Global norm: 0.0123\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0594)\nParameters with gradients: 201, Global norm: 0.0367\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0592)\nParameters with gradients: 201, Global norm: 0.0201\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0590)\nParameters with gradients: 201, Global norm: 0.0325\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0594)\nParameters with gradients: 201, Global norm: 0.2109\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0661)\nParameters with gradients: 201, Global norm: 0.0369\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0615)\nParameters with gradients: 201, Global norm: 0.0295\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0600)\nParameters with gradients: 201, Global norm: 0.0355\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0524)\nParameters with gradients: 201, Global norm: 0.0151\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0444)\nParameters with gradients: 201, Global norm: 0.0412\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0445)\nParameters with gradients: 201, Global norm: 0.0206\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0426)\nParameters with gradients: 201, Global norm: 0.0381\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0412)\nParameters with gradients: 201, Global norm: 0.1219\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0450)\nParameters with gradients: 201, Global norm: 0.0382\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0460)\nParameters with gradients: 201, Global norm: 0.0435\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0468)\nParameters with gradients: 201, Global norm: 0.0220\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0465)\nParameters with gradients: 201, Global norm: 0.0330\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0458)\nParameters with gradients: 201, Global norm: 0.1058\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0497)\nParameters with gradients: 201, Global norm: 0.0590\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0504)\nParameters with gradients: 201, Global norm: 0.0222\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0488)\nParameters with gradients: 201, Global norm: 0.0268\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0495)\nParameters with gradients: 201, Global norm: 0.0195\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0486)\nParameters with gradients: 201, Global norm: 0.0566\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0504)\nParameters with gradients: 201, Global norm: 0.0351\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0506)\nParameters with gradients: 201, Global norm: 0.0237\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0412)\nParameters with gradients: 201, Global norm: 0.0621\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0425)\nParameters with gradients: 201, Global norm: 0.0533\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0437)\nParameters with gradients: 201, Global norm: 0.1448\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0491)\nParameters with gradients: 201, Global norm: 0.1523\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0560)\nParameters with gradients: 201, Global norm: 0.0207\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0550)\nParameters with gradients: 201, Global norm: 0.1267\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0603)\nParameters with gradients: 201, Global norm: 0.0311\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0599)\nParameters with gradients: 201, Global norm: 0.0256\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0551)\nParameters with gradients: 201, Global norm: 0.0207\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0542)\nParameters with gradients: 201, Global norm: 0.0271\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0534)\nParameters with gradients: 201, Global norm: 0.1275\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0587)\nParameters with gradients: 201, Global norm: 0.1155\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0628)\nParameters with gradients: 201, Global norm: 53.2791\nUpdated grad_norm_threshold: 2.5493 (MA grad_norm: 2.7215)\nParameters with gradients: 201, Global norm: 0.0313\nUpdated grad_norm_threshold: 2.7030 (MA grad_norm: 2.7201)\nParameters with gradients: 201, Global norm: 0.0517\nUpdated grad_norm_threshold: 2.7197 (MA grad_norm: 2.7216)\nParameters with gradients: 201, Global norm: 0.0242\nUpdated grad_norm_threshold: 2.7212 (MA grad_norm: 2.7214)\nParameters with gradients: 201, Global norm: 0.0385\nUpdated grad_norm_threshold: 2.7223 (MA grad_norm: 2.7224)\nParameters with gradients: 201, Global norm: 0.0361\nUpdated grad_norm_threshold: 2.7214 (MA grad_norm: 2.7213)\nParameters with gradients: 201, Global norm: 0.3639\nUpdated grad_norm_threshold: 2.7362 (MA grad_norm: 2.7378)\nParameters with gradients: 201, Global norm: 0.0428\nUpdated grad_norm_threshold: 2.7385 (MA grad_norm: 2.7387)\nParameters with gradients: 201, Global norm: 0.1190\nUpdated grad_norm_threshold: 2.7413 (MA grad_norm: 2.7416)\nParameters with gradients: 201, Global norm: 0.0524\nUpdated grad_norm_threshold: 2.7415 (MA grad_norm: 2.7415)\nParameters with gradients: 201, Global norm: 0.0141\nUpdated grad_norm_threshold: 2.7357 (MA grad_norm: 2.7350)\nParameters with gradients: 201, Global norm: 0.0384\nUpdated grad_norm_threshold: 2.7299 (MA grad_norm: 2.7293)\nParameters with gradients: 201, Global norm: 0.0545\nUpdated grad_norm_threshold: 2.7309 (MA grad_norm: 2.7310)\nParameters with gradients: 201, Global norm: 0.0422\nUpdated grad_norm_threshold: 2.7272 (MA grad_norm: 2.7268)\nParameters with gradients: 201, Global norm: 0.0256\nUpdated grad_norm_threshold: 2.7266 (MA grad_norm: 2.7265)\nParameters with gradients: 201, Global norm: 0.0522\nUpdated grad_norm_threshold: 2.7277 (MA grad_norm: 2.7278)\nParameters with gradients: 201, Global norm: 3.7911\nUpdated grad_norm_threshold: 2.8975 (MA grad_norm: 2.9164)\nParameters with gradients: 201, Global norm: 0.0174\nUpdated grad_norm_threshold: 2.9140 (MA grad_norm: 2.9159)\nParameters with gradients: 201, Global norm: 0.0445\nUpdated grad_norm_threshold: 2.9120 (MA grad_norm: 2.9117)\nParameters with gradients: 201, Global norm: 0.0594\nUpdated grad_norm_threshold: 2.9092 (MA grad_norm: 2.9089)\nParameters with gradients: 201, Global norm: 0.1295\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2514)\nParameters with gradients: 201, Global norm: 0.0438\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2521)\nParameters with gradients: 201, Global norm: 0.3231\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2656)\nParameters with gradients: 201, Global norm: 0.0783\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2683)\nParameters with gradients: 201, Global norm: 0.0570\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2693)\nParameters with gradients: 201, Global norm: 0.0431\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2696)\nParameters with gradients: 201, Global norm: 0.0978\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2563)\nParameters with gradients: 201, Global norm: 0.0317\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2558)\nParameters with gradients: 201, Global norm: 0.1076\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2552)\nParameters with gradients: 201, Global norm: 0.0390\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2545)\nParameters with gradients: 201, Global norm: 0.0406\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2558)\nParameters with gradients: 201, Global norm: 0.0274\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2553)\nParameters with gradients: 201, Global norm: 0.0666\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2559)\nParameters with gradients: 201, Global norm: 0.0342\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2555)\nParameters with gradients: 201, Global norm: 0.0371\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2561)\nParameters with gradients: 201, Global norm: 1.7643\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3417)\nParameters with gradients: 201, Global norm: 0.0402\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1541)\nParameters with gradients: 201, Global norm: 1.9582\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2512)\nParameters with gradients: 201, Global norm: 0.1755\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2577)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:11:18] Energy consumed for RAM : 0.001416 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:11:18] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:11:18] Energy consumed for All CPU : 0.003009 kWh\n[codecarbon INFO @ 08:11:18] Energy consumed for all GPUs : 0.005353 kWh. Total GPU Power : 75.98425265911527 W\n[codecarbon INFO @ 08:11:18] 0.009778 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.0656\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2580)\nParameters with gradients: 201, Global norm: 0.0396\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2535)\nParameters with gradients: 201, Global norm: 0.0756\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2551)\nParameters with gradients: 201, Global norm: 0.0280\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2404)\nParameters with gradients: 201, Global norm: 1.5288\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3129)\nParameters with gradients: 201, Global norm: 0.0251\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3113)\nParameters with gradients: 201, Global norm: 0.0356\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3109)\nParameters with gradients: 201, Global norm: 0.0889\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3105)\nParameters with gradients: 201, Global norm: 1.1552\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3667)\nParameters with gradients: 201, Global norm: 0.0241\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3625)\nParameters with gradients: 201, Global norm: 0.0460\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3628)\nParameters with gradients: 201, Global norm: 0.0751\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3646)\nParameters with gradients: 201, Global norm: 0.1183\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3691)\nParameters with gradients: 201, Global norm: 0.1858\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3751)\nParameters with gradients: 201, Global norm: 0.0400\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3754)\nParameters with gradients: 201, Global norm: 0.0399\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3755)\nParameters with gradients: 201, Global norm: 0.0577\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2902)\nParameters with gradients: 201, Global norm: 0.0205\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2892)\nParameters with gradients: 201, Global norm: 0.0408\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1933)\nParameters with gradients: 201, Global norm: 0.0318\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1861)\nParameters with gradients: 201, Global norm: 0.1444\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1901)\nParameters with gradients: 201, Global norm: 0.1035\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1933)\nParameters with gradients: 201, Global norm: 0.0690\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1929)\nParameters with gradients: 201, Global norm: 0.0196\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1925)\nParameters with gradients: 201, Global norm: 0.0178\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1170)\nParameters with gradients: 201, Global norm: 0.0382\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1176)\nParameters with gradients: 201, Global norm: 0.0504\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1184)\nParameters with gradients: 201, Global norm: 0.0235\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.1151)\nParameters with gradients: 201, Global norm: 3.5490\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2348)\nParameters with gradients: 201, Global norm: 0.0666\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2369)\nParameters with gradients: 201, Global norm: 0.1270\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2410)\nParameters with gradients: 201, Global norm: 0.0298\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2387)\nParameters with gradients: 201, Global norm: 0.1186\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2387)\nParameters with gradients: 201, Global norm: 2.4588\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3524)\nParameters with gradients: 201, Global norm: 0.0400\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3524)\nParameters with gradients: 201, Global norm: 5.0903\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6049)\nParameters with gradients: 201, Global norm: 0.0317\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6036)\nParameters with gradients: 201, Global norm: 0.0407\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6046)\nParameters with gradients: 201, Global norm: 0.0627\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6057)\nParameters with gradients: 201, Global norm: 0.0272\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6054)\nParameters with gradients: 201, Global norm: 0.0541\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6009)\nParameters with gradients: 201, Global norm: 0.0259\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5970)\nParameters with gradients: 201, Global norm: 0.0706\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5971)\nParameters with gradients: 201, Global norm: 0.1361\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6029)\nParameters with gradients: 201, Global norm: 0.0534\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6047)\nParameters with gradients: 201, Global norm: 0.0977\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6077)\nParameters with gradients: 201, Global norm: 0.0554\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6080)\nParameters with gradients: 201, Global norm: 0.0293\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6082)\nParameters with gradients: 201, Global norm: 0.0681\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4342)\nParameters with gradients: 201, Global norm: 0.0602\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4339)\nParameters with gradients: 201, Global norm: 0.0471\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4299)\nParameters with gradients: 201, Global norm: 0.0409\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4304)\nParameters with gradients: 201, Global norm: 0.2687\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4379)\nParameters with gradients: 201, Global norm: 0.0435\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3172)\nParameters with gradients: 201, Global norm: 0.0535\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3179)\nParameters with gradients: 201, Global norm: 0.0911\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0679)\nParameters with gradients: 201, Global norm: 0.0387\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0682)\nParameters with gradients: 201, Global norm: 0.0452\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0685)\nParameters with gradients: 201, Global norm: 0.3056\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0806)\nParameters with gradients: 201, Global norm: 0.0849\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0835)\nParameters with gradients: 201, Global norm: 0.0535\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.0835)\nAverage loss: 0.0056\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:11:33] Energy consumed for RAM : 0.001499 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:11:33] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:11:33] Energy consumed for All CPU : 0.003186 kWh\n[codecarbon INFO @ 08:11:33] Energy consumed for all GPUs : 0.005674 kWh. Total GPU Power : 77.08425779712641 W\n[codecarbon INFO @ 08:11:33] 0.010359 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Validation - Accuracy: 0.9037, F1: 0.9036\nEarly stopping at epoch 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/55 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:11:40] Energy consumed for RAM : 0.001535 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:11:40] Delta energy consumed for CPU with constant : 0.000076 kWh, power : 42.5 W\n[codecarbon INFO @ 08:11:40] Energy consumed for All CPU : 0.003262 kWh\n[codecarbon INFO @ 08:11:40] Energy consumed for all GPUs : 0.005815 kWh. Total GPU Power : 78.42783993640741 W\n[codecarbon INFO @ 08:11:40] 0.010611 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Results ---\n{\n  \"model_name\": \"Adaptive_Quantized_BERT_SST2\",\n  \"dataset\": \"SST2\",\n  \"accuracy\": 0.9036697247706422,\n  \"f1\": 0.9036453825514973,\n  \"scheduler_metrics\": {\n    \"adaptation_log\": [\n      {\n        \"layer_id\": 0,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 1,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 2,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 3,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 4,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 5,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 6,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 7,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 8,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 9,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.261525478893149,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 0,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 1,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 2,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 3,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 4,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 5,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 6,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 7,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 8,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 9,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.18910602621381295,\n        \"epoch\": 4,\n        \"type\": \"decrease\"\n      }\n    ],\n    \"precision_switch_counts_int4\": 10,\n    \"precision_switch_counts_int8\": 10,\n    \"precision_switch_counts_fp32\": 0,\n    \"avg_precision_level\": 4.0,\n    \"precision_distribution\": {\n      \"INT4\": 1.0,\n      \"INT8\": 0.0,\n      \"FP32\": 0.0\n    }\n  },\n  \"performance_metrics\": {\n    \"total_duration_s\": 277.7048223018646,\n    \"total_emissions_kwh\": 0.0039204309017049485,\n    \"latency_ms_query\": 318.46883291498233,\n    \"throughput_tokens_sec\": 401.9231609837643,\n    \"energy_wh_token\": 3.512427341693797e-05,\n    \"sci_gco2e_query\": 0.0011239767493420151,\n    \"wue_avg_liters_query\": 8.092632595262509e-06\n  }\n}\n\n==================== Running Experiment: Adaptive_Quantized_BERT_MRPC ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b561612b4784248bdc544a91c300c06"}},"metadata":{}},{"name":"stdout","text":"\nüèóÔ∏è Building Adaptive Quantized Bert Model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[codecarbon INFO @ 08:11:49] offline tracker init\n[codecarbon WARNING @ 08:11:49] Multiple instances of codecarbon are allowed to run at the same time.\n[codecarbon INFO @ 08:11:49] [setup] RAM Tracking...\n[codecarbon INFO @ 08:11:49] [setup] CPU Tracking...\n","output_type":"stream"},{"name":"stdout","text":"‚úì Successfully created quantized Bert model.\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon WARNING @ 08:11:50] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon WARNING @ 08:11:50] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n\n[codecarbon INFO @ 08:11:50] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon WARNING @ 08:11:50] No CPU tracking mode found. Falling back on CPU constant mode.\n[codecarbon INFO @ 08:11:50] [setup] GPU Tracking...\n[codecarbon INFO @ 08:11:50] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 08:11:50] The below tracking methods have been set up:\n                RAM Tracking Method: RAM power estimation model\n                CPU Tracking Method: global constant\n                GPU Tracking Method: pynvml\n            \n[codecarbon INFO @ 08:11:50] >>> Tracker's metadata:\n[codecarbon INFO @ 08:11:50]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 08:11:50]   Python version: 3.11.13\n[codecarbon INFO @ 08:11:50]   CodeCarbon version: 3.0.5\n[codecarbon INFO @ 08:11:50]   Available RAM : 31.350 GB\n[codecarbon INFO @ 08:11:50]   CPU count: 4 thread(s) in 1 physical CPU(s)\n[codecarbon INFO @ 08:11:50]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 08:11:50]   GPU count: 2\n[codecarbon INFO @ 08:11:50]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 08:11:50] Emissions data (if any) will be saved to file /kaggle/working/emissions.csv\n","output_type":"stream"},{"name":"stdout","text":"‚úì Robust PrecisionScheduler initialized for 10 layers (Window: 20, Cooldown: 20, Warmup: 500).\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 1:   0%|          | 0/230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 2.6728\nParameters with gradients: 201, Global norm: 5.1829\nParameters with gradients: 201, Global norm: 2.7962\nParameters with gradients: 201, Global norm: 2.9175\nParameters with gradients: 201, Global norm: 4.5300\nParameters with gradients: 201, Global norm: 3.2714\nParameters with gradients: 201, Global norm: 4.1742\nParameters with gradients: 201, Global norm: 2.4486\nParameters with gradients: 201, Global norm: 2.7799\nParameters with gradients: 201, Global norm: 4.1411\nParameters with gradients: 201, Global norm: 2.5639\nParameters with gradients: 201, Global norm: 4.5368\nParameters with gradients: 201, Global norm: 6.8099\nParameters with gradients: 201, Global norm: 2.6728\nParameters with gradients: 201, Global norm: 2.3214\nParameters with gradients: 201, Global norm: 7.0000\nParameters with gradients: 201, Global norm: 3.0885\nParameters with gradients: 201, Global norm: 2.5750\nParameters with gradients: 201, Global norm: 2.2982\nParameters with gradients: 201, Global norm: 2.9016\nParameters with gradients: 201, Global norm: 3.2357\nParameters with gradients: 201, Global norm: 3.3667\nParameters with gradients: 201, Global norm: 4.4679\nParameters with gradients: 201, Global norm: 3.6104\nParameters with gradients: 201, Global norm: 2.7459\nParameters with gradients: 201, Global norm: 6.3111\nParameters with gradients: 201, Global norm: 2.6897\nParameters with gradients: 201, Global norm: 3.3593\nParameters with gradients: 201, Global norm: 3.2230\nParameters with gradients: 201, Global norm: 4.7063\nParameters with gradients: 201, Global norm: 4.1157\nParameters with gradients: 201, Global norm: 3.1111\nParameters with gradients: 201, Global norm: 4.5113\nParameters with gradients: 201, Global norm: 2.4497\nParameters with gradients: 201, Global norm: 2.9645\nParameters with gradients: 201, Global norm: 4.1413\nParameters with gradients: 201, Global norm: 3.0469\nParameters with gradients: 201, Global norm: 7.2101\nParameters with gradients: 201, Global norm: 6.4354\nParameters with gradients: 201, Global norm: 2.4935\nParameters with gradients: 201, Global norm: 2.5168\nParameters with gradients: 201, Global norm: 3.9594\nParameters with gradients: 201, Global norm: 3.4623\nParameters with gradients: 201, Global norm: 3.0130\nParameters with gradients: 201, Global norm: 2.4693\nParameters with gradients: 201, Global norm: 2.1846\nParameters with gradients: 201, Global norm: 3.3894\nParameters with gradients: 201, Global norm: 2.5517\nParameters with gradients: 201, Global norm: 4.7719\nParameters with gradients: 201, Global norm: 3.3026\nParameters with gradients: 201, Global norm: 2.3600\nParameters with gradients: 201, Global norm: 4.7123\nParameters with gradients: 201, Global norm: 3.4104\nParameters with gradients: 201, Global norm: 6.7193\nParameters with gradients: 201, Global norm: 3.2515\nParameters with gradients: 201, Global norm: 3.5908\nParameters with gradients: 201, Global norm: 2.4494\nParameters with gradients: 201, Global norm: 3.8111\nParameters with gradients: 201, Global norm: 2.5411\nParameters with gradients: 201, Global norm: 3.6281\nParameters with gradients: 201, Global norm: 4.1763\nParameters with gradients: 201, Global norm: 3.3384\nParameters with gradients: 201, Global norm: 3.7901\nParameters with gradients: 201, Global norm: 3.5952\nParameters with gradients: 201, Global norm: 2.8910\nParameters with gradients: 201, Global norm: 3.4205\nParameters with gradients: 201, Global norm: 2.6657\nParameters with gradients: 201, Global norm: 4.4756\nParameters with gradients: 201, Global norm: 2.9105\nParameters with gradients: 201, Global norm: 2.6641\nParameters with gradients: 201, Global norm: 4.1711\nParameters with gradients: 201, Global norm: 2.9783\nParameters with gradients: 201, Global norm: 3.3146\nParameters with gradients: 201, Global norm: 2.5104\nParameters with gradients: 201, Global norm: 4.6881\nParameters with gradients: 201, Global norm: 2.6963\nParameters with gradients: 201, Global norm: 2.9583\nParameters with gradients: 201, Global norm: 2.9006\nParameters with gradients: 201, Global norm: 2.7377\nParameters with gradients: 201, Global norm: 4.3990\nParameters with gradients: 201, Global norm: 5.1631\nParameters with gradients: 201, Global norm: 3.9486\nParameters with gradients: 201, Global norm: 2.2210\nParameters with gradients: 201, Global norm: 5.0623\nParameters with gradients: 201, Global norm: 3.5828\nParameters with gradients: 201, Global norm: 2.5380\nParameters with gradients: 201, Global norm: 3.4839\nParameters with gradients: 201, Global norm: 2.2445\nParameters with gradients: 201, Global norm: 6.2656\nParameters with gradients: 201, Global norm: 4.3944\nParameters with gradients: 201, Global norm: 3.3752\nParameters with gradients: 201, Global norm: 3.3529\nParameters with gradients: 201, Global norm: 3.0088\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:12:06] Energy consumed for RAM : 0.000083 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:12:06] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:12:06] Energy consumed for All CPU : 0.000177 kWh\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 2.7544\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:12:06] Energy consumed for all GPUs : 0.000316 kWh. Total GPU Power : 75.6849640639839 W\n[codecarbon INFO @ 08:12:06] 0.000576 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 5.2743\nParameters with gradients: 201, Global norm: 3.0520\nParameters with gradients: 201, Global norm: 2.6140\nParameters with gradients: 201, Global norm: 2.9277\nParameters with gradients: 201, Global norm: 3.0818\nParameters with gradients: 201, Global norm: 3.6101\nParameters with gradients: 201, Global norm: 3.3997\nParameters with gradients: 201, Global norm: 4.0174\nParameters with gradients: 201, Global norm: 5.2483\nWarning: Non-finite gradient norm in model.bert.encoder.layer.5.layer.intermediate.dense.weight: inf\nWarning: Non-finite gradient norm in model.bert.encoder.layer.6.layer.intermediate.dense.weight: inf\nParameters with gradients: 199, Global norm: 4.8003\nParameters with gradients: 201, Global norm: 4.4878\nParameters with gradients: 201, Global norm: 6.3566\nParameters with gradients: 201, Global norm: 4.6622\nParameters with gradients: 201, Global norm: 6.3722\nParameters with gradients: 201, Global norm: 5.1767\nParameters with gradients: 201, Global norm: 4.5290\nParameters with gradients: 201, Global norm: 5.0117\nParameters with gradients: 201, Global norm: 4.6668\nParameters with gradients: 201, Global norm: 4.4157\nParameters with gradients: 201, Global norm: 6.7046\nParameters with gradients: 201, Global norm: 6.9899\nParameters with gradients: 201, Global norm: 4.7358\nParameters with gradients: 201, Global norm: 4.3454\nParameters with gradients: 201, Global norm: 6.5829\nParameters with gradients: 201, Global norm: 4.5445\nParameters with gradients: 201, Global norm: 5.2772\nParameters with gradients: 201, Global norm: 3.9920\nParameters with gradients: 201, Global norm: 4.1319\nParameters with gradients: 201, Global norm: 4.6345\nParameters with gradients: 201, Global norm: 4.1310\nParameters with gradients: 201, Global norm: 4.5087\nParameters with gradients: 201, Global norm: 5.4783\nParameters with gradients: 201, Global norm: 4.4665\nParameters with gradients: 201, Global norm: 4.4420\nParameters with gradients: 201, Global norm: 3.3429\nParameters with gradients: 201, Global norm: 3.2980\nParameters with gradients: 201, Global norm: 3.0304\nParameters with gradients: 201, Global norm: 5.6832\nParameters with gradients: 201, Global norm: 7.7282\nParameters with gradients: 201, Global norm: 3.5187\nParameters with gradients: 201, Global norm: 3.7000\nParameters with gradients: 201, Global norm: 4.9216\nParameters with gradients: 201, Global norm: 3.8229\nParameters with gradients: 201, Global norm: 2.6500\nParameters with gradients: 201, Global norm: 5.1783\nParameters with gradients: 201, Global norm: 3.1959\nParameters with gradients: 201, Global norm: 3.4530\nParameters with gradients: 201, Global norm: 5.2393\nParameters with gradients: 201, Global norm: 3.4889\nParameters with gradients: 201, Global norm: 4.1851\nParameters with gradients: 201, Global norm: 7.8901\nParameters with gradients: 201, Global norm: 7.1421\nParameters with gradients: 201, Global norm: 4.4192\nParameters with gradients: 201, Global norm: 6.0271\nParameters with gradients: 201, Global norm: 4.2855\nParameters with gradients: 201, Global norm: 8.4652\nParameters with gradients: 201, Global norm: 5.2393\nParameters with gradients: 201, Global norm: 3.1978\nParameters with gradients: 201, Global norm: 4.7203\nParameters with gradients: 201, Global norm: 3.7189\nParameters with gradients: 201, Global norm: 3.6884\nParameters with gradients: 201, Global norm: 4.4792\nParameters with gradients: 201, Global norm: 7.5869\nParameters with gradients: 201, Global norm: 3.8807\nParameters with gradients: 201, Global norm: 4.4513\nParameters with gradients: 201, Global norm: 5.0729\nParameters with gradients: 201, Global norm: 3.6569\nParameters with gradients: 201, Global norm: 6.8255\nParameters with gradients: 201, Global norm: 4.8364\nParameters with gradients: 201, Global norm: 5.8931\nParameters with gradients: 201, Global norm: 4.9703\nParameters with gradients: 201, Global norm: 5.2427\nParameters with gradients: 201, Global norm: 7.9085\nParameters with gradients: 201, Global norm: 5.5805\nParameters with gradients: 201, Global norm: 5.3665\nParameters with gradients: 201, Global norm: 8.5001\nParameters with gradients: 201, Global norm: 4.7980\nParameters with gradients: 201, Global norm: 7.2027\nParameters with gradients: 201, Global norm: 4.6291\nParameters with gradients: 201, Global norm: 7.8299\nParameters with gradients: 201, Global norm: 10.3869\nWarning: Non-finite gradient norm in model.bert.encoder.layer.3.layer.intermediate.dense.weight: inf\nParameters with gradients: 200, Global norm: 14.1756\nParameters with gradients: 201, Global norm: 4.1184\nParameters with gradients: 201, Global norm: 4.2003\nParameters with gradients: 201, Global norm: 5.3669\nParameters with gradients: 201, Global norm: 13.5379\nParameters with gradients: 201, Global norm: 7.1891\nParameters with gradients: 201, Global norm: 7.4971\nParameters with gradients: 201, Global norm: 5.3153\nParameters with gradients: 201, Global norm: 5.4937\nParameters with gradients: 201, Global norm: 4.4802\nParameters with gradients: 201, Global norm: 5.2432\nParameters with gradients: 201, Global norm: 3.7444\nParameters with gradients: 201, Global norm: 4.7860\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:12:21] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:12:21] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:12:21] Energy consumed for All CPU : 0.000354 kWh\n[codecarbon INFO @ 08:12:21] Energy consumed for all GPUs : 0.000632 kWh. Total GPU Power : 76.1038243176691 W\n[codecarbon INFO @ 08:12:21] 0.001153 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 6.0687\nParameters with gradients: 201, Global norm: 3.5647\nParameters with gradients: 201, Global norm: 4.4043\nParameters with gradients: 201, Global norm: 5.8524\nParameters with gradients: 201, Global norm: 9.7797\nParameters with gradients: 201, Global norm: 4.9427\nParameters with gradients: 201, Global norm: 5.9177\nParameters with gradients: 201, Global norm: 4.7999\nParameters with gradients: 201, Global norm: 6.2740\nParameters with gradients: 201, Global norm: 4.7166\nParameters with gradients: 201, Global norm: 3.8511\nParameters with gradients: 201, Global norm: 7.2484\nParameters with gradients: 201, Global norm: 3.9331\nParameters with gradients: 201, Global norm: 13.7404\nParameters with gradients: 201, Global norm: 4.9200\nParameters with gradients: 201, Global norm: 5.1419\nParameters with gradients: 201, Global norm: 4.0563\nParameters with gradients: 201, Global norm: 5.1649\nParameters with gradients: 201, Global norm: 11.0242\nParameters with gradients: 201, Global norm: 6.8815\nParameters with gradients: 201, Global norm: 5.0743\nParameters with gradients: 201, Global norm: 8.6681\nParameters with gradients: 201, Global norm: 5.5331\nParameters with gradients: 201, Global norm: 7.0127\nParameters with gradients: 201, Global norm: 4.2456\nParameters with gradients: 201, Global norm: 6.7416\nParameters with gradients: 201, Global norm: 6.4158\nParameters with gradients: 201, Global norm: 4.7950\nParameters with gradients: 201, Global norm: 6.9837\nParameters with gradients: 201, Global norm: 4.7874\nParameters with gradients: 201, Global norm: 6.9095\nParameters with gradients: 201, Global norm: 5.5720\nParameters with gradients: 201, Global norm: 7.6794\nParameters with gradients: 201, Global norm: 4.7657\nParameters with gradients: 201, Global norm: 7.4194\nParameters with gradients: 201, Global norm: 4.0931\nParameters with gradients: 201, Global norm: 4.3378\nParameters with gradients: 201, Global norm: 6.1076\nParameters with gradients: 201, Global norm: 8.4113\nParameters with gradients: 201, Global norm: 5.8566\nParameters with gradients: 201, Global norm: 3.9255\nParameters with gradients: 201, Global norm: 5.3389\nAverage loss: 0.5841\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.7157, F1: 0.6473\n\nEpoch 2/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 2:   0%|          | 0/230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 7.0931\nParameters with gradients: 201, Global norm: 6.2595\nParameters with gradients: 201, Global norm: 4.9059\nParameters with gradients: 201, Global norm: 4.2779\nParameters with gradients: 201, Global norm: 3.4783\nParameters with gradients: 201, Global norm: 5.3225\nParameters with gradients: 201, Global norm: 4.8577\nParameters with gradients: 201, Global norm: 5.4251\nParameters with gradients: 201, Global norm: 6.5982\nParameters with gradients: 201, Global norm: 4.8225\nParameters with gradients: 201, Global norm: 8.4539\nParameters with gradients: 201, Global norm: 8.4528\nParameters with gradients: 201, Global norm: 9.7641\nParameters with gradients: 201, Global norm: 4.3558\nParameters with gradients: 201, Global norm: 7.1351\nParameters with gradients: 201, Global norm: 3.8065\nParameters with gradients: 201, Global norm: 11.1221\nParameters with gradients: 201, Global norm: 5.2370\nParameters with gradients: 201, Global norm: 8.6648\nParameters with gradients: 201, Global norm: 3.4187\nParameters with gradients: 201, Global norm: 6.7789\nParameters with gradients: 201, Global norm: 13.7736\nParameters with gradients: 201, Global norm: 5.6078\nParameters with gradients: 201, Global norm: 7.3726\nParameters with gradients: 201, Global norm: 5.4796\nParameters with gradients: 201, Global norm: 11.1055\nParameters with gradients: 201, Global norm: 7.2005\nParameters with gradients: 201, Global norm: 7.1906\nParameters with gradients: 201, Global norm: 8.2120\nParameters with gradients: 201, Global norm: 4.2266\nParameters with gradients: 201, Global norm: 3.8621\nParameters with gradients: 201, Global norm: 4.9801\nParameters with gradients: 201, Global norm: 5.0832\nParameters with gradients: 201, Global norm: 7.5034\nParameters with gradients: 201, Global norm: 5.1845\nParameters with gradients: 201, Global norm: 12.6312\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:12:36] Energy consumed for RAM : 0.000250 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:12:36] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:12:36] Energy consumed for All CPU : 0.000531 kWh\n[codecarbon INFO @ 08:12:36] Energy consumed for all GPUs : 0.000950 kWh. Total GPU Power : 76.34666749123981 W\n[codecarbon INFO @ 08:12:36] 0.001732 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 6.8477\nParameters with gradients: 201, Global norm: 6.8187\nParameters with gradients: 201, Global norm: 5.6073\nParameters with gradients: 201, Global norm: 6.0921\nParameters with gradients: 201, Global norm: 5.3061\nParameters with gradients: 201, Global norm: 7.8290\nParameters with gradients: 201, Global norm: 2.0337\nParameters with gradients: 201, Global norm: 13.1509\nParameters with gradients: 201, Global norm: 10.3088\nParameters with gradients: 201, Global norm: 5.7961\nParameters with gradients: 201, Global norm: 6.7415\nParameters with gradients: 201, Global norm: 7.8363\nParameters with gradients: 201, Global norm: 9.2577\nParameters with gradients: 201, Global norm: 8.1310\nParameters with gradients: 201, Global norm: 5.2668\nParameters with gradients: 201, Global norm: 7.0356\nParameters with gradients: 201, Global norm: 6.5884\nParameters with gradients: 201, Global norm: 9.9361\nParameters with gradients: 201, Global norm: 4.7916\nParameters with gradients: 201, Global norm: 5.8476\nParameters with gradients: 201, Global norm: 15.5878\nParameters with gradients: 201, Global norm: 8.4058\nParameters with gradients: 201, Global norm: 5.0900\nParameters with gradients: 201, Global norm: 6.3714\nParameters with gradients: 201, Global norm: 10.4550\nParameters with gradients: 201, Global norm: 5.5374\nParameters with gradients: 201, Global norm: 7.9341\nParameters with gradients: 201, Global norm: 4.8315\nParameters with gradients: 201, Global norm: 6.0899\nParameters with gradients: 201, Global norm: 4.6706\nParameters with gradients: 201, Global norm: 9.0817\nParameters with gradients: 201, Global norm: 4.4524\nParameters with gradients: 201, Global norm: 6.7536\nParameters with gradients: 201, Global norm: 3.4632\nParameters with gradients: 201, Global norm: 6.7261\nParameters with gradients: 201, Global norm: 9.1716\nParameters with gradients: 201, Global norm: 7.2696\nParameters with gradients: 201, Global norm: 4.5836\nParameters with gradients: 201, Global norm: 4.7958\nParameters with gradients: 201, Global norm: 5.4415\nParameters with gradients: 201, Global norm: 6.5326\nParameters with gradients: 201, Global norm: 11.3821\nParameters with gradients: 201, Global norm: 7.1587\nParameters with gradients: 201, Global norm: 11.5879\nParameters with gradients: 201, Global norm: 10.5596\nParameters with gradients: 201, Global norm: 13.6398\nParameters with gradients: 201, Global norm: 6.2919\nParameters with gradients: 201, Global norm: 7.0693\nParameters with gradients: 201, Global norm: 4.8667\nParameters with gradients: 201, Global norm: 3.1679\nParameters with gradients: 201, Global norm: 9.5079\nParameters with gradients: 201, Global norm: 7.9462\nParameters with gradients: 201, Global norm: 5.2217\nParameters with gradients: 201, Global norm: 4.4911\nParameters with gradients: 201, Global norm: 9.5294\nParameters with gradients: 201, Global norm: 5.1837\nParameters with gradients: 201, Global norm: 13.3753\nParameters with gradients: 201, Global norm: 10.2411\nParameters with gradients: 201, Global norm: 7.9268\nParameters with gradients: 201, Global norm: 5.6898\nParameters with gradients: 201, Global norm: 8.1652\nParameters with gradients: 201, Global norm: 5.4821\nParameters with gradients: 201, Global norm: 5.1314\nParameters with gradients: 201, Global norm: 7.8172\nParameters with gradients: 201, Global norm: 8.8559\nParameters with gradients: 201, Global norm: 3.9342\nParameters with gradients: 201, Global norm: 5.2766\nParameters with gradients: 201, Global norm: 3.9521\nParameters with gradients: 201, Global norm: 7.8034\nParameters with gradients: 201, Global norm: 6.7442\nParameters with gradients: 201, Global norm: 3.2354\nParameters with gradients: 201, Global norm: 5.5657\nParameters with gradients: 201, Global norm: 4.6054\nParameters with gradients: 201, Global norm: 4.7775\nParameters with gradients: 201, Global norm: 6.9385\nParameters with gradients: 201, Global norm: 3.2924\nParameters with gradients: 201, Global norm: 7.4413\nParameters with gradients: 201, Global norm: 7.7609\nParameters with gradients: 201, Global norm: 5.1032\nParameters with gradients: 201, Global norm: 5.5305\nParameters with gradients: 201, Global norm: 7.2765\nParameters with gradients: 201, Global norm: 5.9833\nParameters with gradients: 201, Global norm: 4.4983\nParameters with gradients: 201, Global norm: 5.7744\nParameters with gradients: 201, Global norm: 8.4108\nParameters with gradients: 201, Global norm: 7.1157\nParameters with gradients: 201, Global norm: 4.5486\nParameters with gradients: 201, Global norm: 6.2130\nParameters with gradients: 201, Global norm: 4.7726\nParameters with gradients: 201, Global norm: 12.2581\nParameters with gradients: 201, Global norm: 15.2482\nParameters with gradients: 201, Global norm: 15.4302\nParameters with gradients: 201, Global norm: 5.9165\nParameters with gradients: 201, Global norm: 6.7489\nParameters with gradients: 201, Global norm: 3.8219\nParameters with gradients: 201, Global norm: 6.1975\nParameters with gradients: 201, Global norm: 4.7186\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:12:51] Energy consumed for RAM : 0.000333 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:12:51] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:12:51] Energy consumed for All CPU : 0.000708 kWh\n[codecarbon INFO @ 08:12:51] Energy consumed for all GPUs : 0.001269 kWh. Total GPU Power : 76.46197662171694 W\n[codecarbon INFO @ 08:12:51] 0.002310 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 7.1725\nParameters with gradients: 201, Global norm: 3.4566\nParameters with gradients: 201, Global norm: 4.6410\nParameters with gradients: 201, Global norm: 5.3948\nParameters with gradients: 201, Global norm: 7.7900\nParameters with gradients: 201, Global norm: 4.0071\nParameters with gradients: 201, Global norm: 6.0281\nParameters with gradients: 201, Global norm: 11.8628\nParameters with gradients: 201, Global norm: 4.9665\nParameters with gradients: 201, Global norm: 8.3538\nParameters with gradients: 201, Global norm: 6.6216\nParameters with gradients: 201, Global norm: 5.8733\nParameters with gradients: 201, Global norm: 5.4174\nParameters with gradients: 201, Global norm: 5.3034\nParameters with gradients: 201, Global norm: 3.9223\nParameters with gradients: 201, Global norm: 6.7829\nParameters with gradients: 201, Global norm: 6.0841\nParameters with gradients: 201, Global norm: 7.3769\nParameters with gradients: 201, Global norm: 6.9408\nParameters with gradients: 201, Global norm: 5.2767\nParameters with gradients: 201, Global norm: 3.8849\nParameters with gradients: 201, Global norm: 2.9248\nParameters with gradients: 201, Global norm: 8.3207\nParameters with gradients: 201, Global norm: 11.6108\nParameters with gradients: 201, Global norm: 5.9955\nParameters with gradients: 201, Global norm: 7.7499\nParameters with gradients: 201, Global norm: 11.9067\nParameters with gradients: 201, Global norm: 12.0617\nParameters with gradients: 201, Global norm: 8.4680\nParameters with gradients: 201, Global norm: 12.8536\nParameters with gradients: 201, Global norm: 3.3554\nParameters with gradients: 201, Global norm: 16.0364\nParameters with gradients: 201, Global norm: 2.8688\nParameters with gradients: 201, Global norm: 10.5230\nParameters with gradients: 201, Global norm: 7.3985\nParameters with gradients: 201, Global norm: 6.4210\nParameters with gradients: 201, Global norm: 5.0906\nParameters with gradients: 201, Global norm: 12.8831\nParameters with gradients: 201, Global norm: 16.2848\nParameters with gradients: 201, Global norm: 7.1371\nParameters with gradients: 201, Global norm: 8.2171\nParameters with gradients: 201, Global norm: 13.0533\nParameters with gradients: 201, Global norm: 11.5264\nParameters with gradients: 201, Global norm: 11.4256\nParameters with gradients: 201, Global norm: 6.6774\nParameters with gradients: 201, Global norm: 10.3588\nParameters with gradients: 201, Global norm: 10.8671\nParameters with gradients: 201, Global norm: 8.7869\nParameters with gradients: 201, Global norm: 8.0848\nParameters with gradients: 201, Global norm: 10.0628\nParameters with gradients: 201, Global norm: 6.1246\nParameters with gradients: 201, Global norm: 3.9207\nParameters with gradients: 201, Global norm: 9.8097\nParameters with gradients: 201, Global norm: 9.2677\nParameters with gradients: 201, Global norm: 15.8797\nParameters with gradients: 201, Global norm: 6.2354\nParameters with gradients: 201, Global norm: 11.8373\nParameters with gradients: 201, Global norm: 16.7593\nParameters with gradients: 201, Global norm: 13.8009\nParameters with gradients: 201, Global norm: 13.5069\nParameters with gradients: 201, Global norm: 3.4960\nParameters with gradients: 201, Global norm: 13.9221\nParameters with gradients: 201, Global norm: 10.6835\nParameters with gradients: 201, Global norm: 7.2361\nParameters with gradients: 201, Global norm: 5.6620\nParameters with gradients: 201, Global norm: 4.8093\nParameters with gradients: 201, Global norm: 4.2908\nParameters with gradients: 201, Global norm: 6.1762\nParameters with gradients: 201, Global norm: 6.0599\nParameters with gradients: 201, Global norm: 7.2467\nParameters with gradients: 201, Global norm: 4.2369\nParameters with gradients: 201, Global norm: 5.3017\nParameters with gradients: 201, Global norm: 6.8459\nParameters with gradients: 201, Global norm: 7.5797\nParameters with gradients: 201, Global norm: 8.0077\nParameters with gradients: 201, Global norm: 5.7735\nParameters with gradients: 201, Global norm: 5.5468\nParameters with gradients: 201, Global norm: 4.3585\nParameters with gradients: 201, Global norm: 12.1951\nParameters with gradients: 201, Global norm: 5.3420\nParameters with gradients: 201, Global norm: 4.9012\nParameters with gradients: 201, Global norm: 17.6779\nParameters with gradients: 201, Global norm: 6.8229\nParameters with gradients: 201, Global norm: 9.3743\nParameters with gradients: 201, Global norm: 5.3859\nParameters with gradients: 201, Global norm: 9.6839\nParameters with gradients: 201, Global norm: 4.4599\nParameters with gradients: 201, Global norm: 8.4643\nParameters with gradients: 201, Global norm: 5.5203\nParameters with gradients: 201, Global norm: 5.7623\nParameters with gradients: 201, Global norm: 4.2201\nParameters with gradients: 201, Global norm: 6.4130\nParameters with gradients: 201, Global norm: 7.7497\nParameters with gradients: 201, Global norm: 4.1188\nParameters with gradients: 201, Global norm: 8.3419\nParameters with gradients: 201, Global norm: 11.9119\nParameters with gradients: 201, Global norm: 1.0643\nAverage loss: 0.3862\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:13:06] Energy consumed for RAM : 0.000417 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:13:06] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:13:06] Energy consumed for All CPU : 0.000885 kWh\n[codecarbon INFO @ 08:13:06] Energy consumed for all GPUs : 0.001586 kWh. Total GPU Power : 76.07878896010753 W\n[codecarbon INFO @ 08:13:06] 0.002887 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Validation - Accuracy: 0.8554, F1: 0.8478\n\nEpoch 3/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 3:   0%|          | 0/230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.0937\nParameters with gradients: 201, Global norm: 3.5380\nParameters with gradients: 201, Global norm: 3.8121\nParameters with gradients: 201, Global norm: 1.2759\nParameters with gradients: 201, Global norm: 6.6920\nParameters with gradients: 201, Global norm: 2.6733\nParameters with gradients: 201, Global norm: 4.0596\nParameters with gradients: 201, Global norm: 5.6572\nParameters with gradients: 201, Global norm: 1.1467\nParameters with gradients: 201, Global norm: 13.6586\nParameters with gradients: 201, Global norm: 6.6094\nParameters with gradients: 201, Global norm: 8.3725\nParameters with gradients: 201, Global norm: 3.9572\nParameters with gradients: 201, Global norm: 1.7082\nParameters with gradients: 201, Global norm: 2.5001\nParameters with gradients: 201, Global norm: 8.3090\nParameters with gradients: 201, Global norm: 15.7531\nParameters with gradients: 201, Global norm: 7.5726\nParameters with gradients: 201, Global norm: 1.1969\nParameters with gradients: 201, Global norm: 8.9865\nParameters with gradients: 201, Global norm: 3.7368\nParameters with gradients: 201, Global norm: 4.7711\nParameters with gradients: 201, Global norm: 1.0612\nParameters with gradients: 201, Global norm: 2.3451\nParameters with gradients: 201, Global norm: 7.4136\nParameters with gradients: 201, Global norm: 3.9328\nParameters with gradients: 201, Global norm: 3.5683\nParameters with gradients: 201, Global norm: 7.6674\nParameters with gradients: 201, Global norm: 0.9985\nParameters with gradients: 201, Global norm: 2.4658\nParameters with gradients: 201, Global norm: 7.9302\nParameters with gradients: 201, Global norm: 4.2986\nParameters with gradients: 201, Global norm: 12.3979\nParameters with gradients: 201, Global norm: 6.5813\nParameters with gradients: 201, Global norm: 4.3805\nParameters with gradients: 201, Global norm: 5.7515\nParameters with gradients: 201, Global norm: 2.3998\nParameters with gradients: 201, Global norm: 12.6018\nParameters with gradients: 201, Global norm: 3.6956\nParameters with gradients: 201, Global norm: 6.7850\nParameters with gradients: 201, Global norm: 1.4438\nUpdated grad_norm_threshold: 2.2994 (MA grad_norm: 1.4438)\nParameters with gradients: 201, Global norm: 8.4196\nUpdated grad_norm_threshold: 4.6685 (MA grad_norm: 4.9317)\nParameters with gradients: 201, Global norm: 14.4017\nUpdated grad_norm_threshold: 7.7464 (MA grad_norm: 8.0884)\nParameters with gradients: 201, Global norm: 5.5097\nUpdated grad_norm_threshold: 7.4740 (MA grad_norm: 7.4437)\nParameters with gradients: 201, Global norm: 6.7055\nUpdated grad_norm_threshold: 7.3139 (MA grad_norm: 7.2961)\nParameters with gradients: 201, Global norm: 10.9630\nUpdated grad_norm_threshold: 7.8479 (MA grad_norm: 7.9072)\nParameters with gradients: 201, Global norm: 6.5798\nUpdated grad_norm_threshold: 7.7306 (MA grad_norm: 7.7176)\nParameters with gradients: 201, Global norm: 5.1439\nUpdated grad_norm_threshold: 7.4294 (MA grad_norm: 7.3959)\nParameters with gradients: 201, Global norm: 1.4520\nUpdated grad_norm_threshold: 6.8048 (MA grad_norm: 6.7354)\nParameters with gradients: 201, Global norm: 1.5691\nUpdated grad_norm_threshold: 6.2774 (MA grad_norm: 6.2188)\nParameters with gradients: 201, Global norm: 4.7505\nUpdated grad_norm_threshold: 6.1045 (MA grad_norm: 6.0853)\nParameters with gradients: 201, Global norm: 6.9142\nUpdated grad_norm_threshold: 6.1494 (MA grad_norm: 6.1544)\nParameters with gradients: 201, Global norm: 6.8223\nUpdated grad_norm_threshold: 6.2001 (MA grad_norm: 6.2058)\nParameters with gradients: 201, Global norm: 4.6927\nUpdated grad_norm_threshold: 6.1079 (MA grad_norm: 6.0977)\nParameters with gradients: 201, Global norm: 4.0137\nUpdated grad_norm_threshold: 5.9737 (MA grad_norm: 5.9588)\nParameters with gradients: 201, Global norm: 1.0819\nUpdated grad_norm_threshold: 5.6859 (MA grad_norm: 5.6540)\nParameters with gradients: 201, Global norm: 13.0507\nUpdated grad_norm_threshold: 6.0488 (MA grad_norm: 6.0891)\nParameters with gradients: 201, Global norm: 0.9651\nUpdated grad_norm_threshold: 5.8288 (MA grad_norm: 5.8044)\nParameters with gradients: 201, Global norm: 4.9144\nUpdated grad_norm_threshold: 5.7647 (MA grad_norm: 5.7576)\nParameters with gradients: 201, Global norm: 6.6862\nUpdated grad_norm_threshold: 5.8001 (MA grad_norm: 5.8040)\nParameters with gradients: 201, Global norm: 5.4587\nUpdated grad_norm_threshold: 5.9843 (MA grad_norm: 6.0047)\nParameters with gradients: 201, Global norm: 4.4438\nUpdated grad_norm_threshold: 5.8238 (MA grad_norm: 5.8059)\nParameters with gradients: 201, Global norm: 7.4260\nUpdated grad_norm_threshold: 5.4938 (MA grad_norm: 5.4572)\nParameters with gradients: 201, Global norm: 0.8528\nUpdated grad_norm_threshold: 5.2513 (MA grad_norm: 5.2243)\nParameters with gradients: 201, Global norm: 11.1447\nUpdated grad_norm_threshold: 5.4268 (MA grad_norm: 5.4463)\nParameters with gradients: 201, Global norm: 3.9955\nUpdated grad_norm_threshold: 5.1308 (MA grad_norm: 5.0979)\nParameters with gradients: 201, Global norm: 15.4323\nUpdated grad_norm_threshold: 5.4996 (MA grad_norm: 5.5405)\nParameters with gradients: 201, Global norm: 5.8178\nUpdated grad_norm_threshold: 5.5668 (MA grad_norm: 5.5742)\nParameters with gradients: 201, Global norm: 4.3557\nUpdated grad_norm_threshold: 5.7041 (MA grad_norm: 5.7194)\nParameters with gradients: 201, Global norm: 5.1601\nUpdated grad_norm_threshold: 5.8795 (MA grad_norm: 5.8990)\nParameters with gradients: 201, Global norm: 2.2382\nUpdated grad_norm_threshold: 5.7840 (MA grad_norm: 5.7733)\nParameters with gradients: 201, Global norm: 2.8934\nUpdated grad_norm_threshold: 5.5935 (MA grad_norm: 5.5723)\nParameters with gradients: 201, Global norm: 3.0384\nUpdated grad_norm_threshold: 5.4041 (MA grad_norm: 5.3831)\nParameters with gradients: 201, Global norm: 5.3652\nUpdated grad_norm_threshold: 5.4155 (MA grad_norm: 5.4167)\nParameters with gradients: 201, Global norm: 5.6187\nUpdated grad_norm_threshold: 5.4888 (MA grad_norm: 5.4970)\nParameters with gradients: 201, Global norm: 9.9549\nUpdated grad_norm_threshold: 5.8955 (MA grad_norm: 5.9406)\nParameters with gradients: 201, Global norm: 3.8921\nUpdated grad_norm_threshold: 5.5240 (MA grad_norm: 5.4827)\nParameters with gradients: 201, Global norm: 4.9560\nUpdated grad_norm_threshold: 5.6664 (MA grad_norm: 5.6822)\nParameters with gradients: 201, Global norm: 1.4916\nUpdated grad_norm_threshold: 5.5266 (MA grad_norm: 5.5111)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:13:21] Energy consumed for RAM : 0.000500 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:13:21] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:13:21] Energy consumed for All CPU : 0.001062 kWh\n[codecarbon INFO @ 08:13:21] Energy consumed for all GPUs : 0.001904 kWh. Total GPU Power : 76.32211966153358 W\n[codecarbon INFO @ 08:13:21] 0.003465 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.2166\nUpdated grad_norm_threshold: 5.3565 (MA grad_norm: 5.3376)\nParameters with gradients: 201, Global norm: 13.1794\nUpdated grad_norm_threshold: 5.6869 (MA grad_norm: 5.7237)\nParameters with gradients: 201, Global norm: 11.4772\nUpdated grad_norm_threshold: 6.0365 (MA grad_norm: 6.0753)\nParameters with gradients: 201, Global norm: 15.5330\nUpdated grad_norm_threshold: 6.4363 (MA grad_norm: 6.4807)\nParameters with gradients: 201, Global norm: 13.5439\nUpdated grad_norm_threshold: 7.0473 (MA grad_norm: 7.1152)\nParameters with gradients: 201, Global norm: 0.6940\nUpdated grad_norm_threshold: 6.6382 (MA grad_norm: 6.5927)\nParameters with gradients: 201, Global norm: 0.5473\nUpdated grad_norm_threshold: 6.4421 (MA grad_norm: 6.4203)\nParameters with gradients: 201, Global norm: 8.9083\nUpdated grad_norm_threshold: 6.1289 (MA grad_norm: 6.0941)\nParameters with gradients: 201, Global norm: 1.1086\nUpdated grad_norm_threshold: 5.8857 (MA grad_norm: 5.8586)\nParameters with gradients: 201, Global norm: 12.0851\nUpdated grad_norm_threshold: 6.2092 (MA grad_norm: 6.2451)\nParameters with gradients: 201, Global norm: 8.3985\nUpdated grad_norm_threshold: 6.3872 (MA grad_norm: 6.4070)\nParameters with gradients: 201, Global norm: 3.8521\nUpdated grad_norm_threshold: 6.4777 (MA grad_norm: 6.4877)\nParameters with gradients: 201, Global norm: 7.2473\nUpdated grad_norm_threshold: 6.6826 (MA grad_norm: 6.7054)\nParameters with gradients: 201, Global norm: 6.9501\nUpdated grad_norm_threshold: 6.8792 (MA grad_norm: 6.9010)\nParameters with gradients: 201, Global norm: 9.0106\nUpdated grad_norm_threshold: 7.0628 (MA grad_norm: 7.0833)\nParameters with gradients: 201, Global norm: 8.7296\nUpdated grad_norm_threshold: 7.2212 (MA grad_norm: 7.2388)\nParameters with gradients: 201, Global norm: 7.9493\nUpdated grad_norm_threshold: 7.1468 (MA grad_norm: 7.1385)\nParameters with gradients: 201, Global norm: 5.6571\nUpdated grad_norm_threshold: 7.2188 (MA grad_norm: 7.2268)\nParameters with gradients: 201, Global norm: 15.7132\nUpdated grad_norm_threshold: 7.7101 (MA grad_norm: 7.7646)\nParameters with gradients: 201, Global norm: 5.4556\nUpdated grad_norm_threshold: 7.9376 (MA grad_norm: 7.9628)\nParameters with gradients: 201, Global norm: 4.2087\nUpdated grad_norm_threshold: 8.0050 (MA grad_norm: 8.0125)\nParameters with gradients: 201, Global norm: 20.1031\nUpdated grad_norm_threshold: 8.3233 (MA grad_norm: 8.3586)\nParameters with gradients: 201, Global norm: 10.6103\nUpdated grad_norm_threshold: 8.3161 (MA grad_norm: 8.3153)\nParameters with gradients: 201, Global norm: 3.4097\nUpdated grad_norm_threshold: 7.7698 (MA grad_norm: 7.7091)\nParameters with gradients: 201, Global norm: 3.6733\nUpdated grad_norm_threshold: 7.2710 (MA grad_norm: 7.2156)\nParameters with gradients: 201, Global norm: 2.0213\nUpdated grad_norm_threshold: 7.2809 (MA grad_norm: 7.2820)\nParameters with gradients: 201, Global norm: 1.6515\nUpdated grad_norm_threshold: 7.3315 (MA grad_norm: 7.3372)\nParameters with gradients: 201, Global norm: 9.0871\nUpdated grad_norm_threshold: 7.3446 (MA grad_norm: 7.3461)\nParameters with gradients: 201, Global norm: 0.5352\nUpdated grad_norm_threshold: 7.3202 (MA grad_norm: 7.3174)\nParameters with gradients: 201, Global norm: 0.9249\nUpdated grad_norm_threshold: 6.8155 (MA grad_norm: 6.7594)\nParameters with gradients: 201, Global norm: 0.9247\nUpdated grad_norm_threshold: 6.4287 (MA grad_norm: 6.3857)\nParameters with gradients: 201, Global norm: 4.6009\nUpdated grad_norm_threshold: 6.4237 (MA grad_norm: 6.4232)\nParameters with gradients: 201, Global norm: 5.1045\nUpdated grad_norm_threshold: 6.3268 (MA grad_norm: 6.3160)\nParameters with gradients: 201, Global norm: 6.5188\nUpdated grad_norm_threshold: 6.2977 (MA grad_norm: 6.2945)\nParameters with gradients: 201, Global norm: 0.4499\nUpdated grad_norm_threshold: 5.9096 (MA grad_norm: 5.8664)\nParameters with gradients: 201, Global norm: 3.8186\nUpdated grad_norm_threshold: 5.6498 (MA grad_norm: 5.6209)\nParameters with gradients: 201, Global norm: 5.9030\nUpdated grad_norm_threshold: 5.5317 (MA grad_norm: 5.5186)\nParameters with gradients: 201, Global norm: 2.3790\nUpdated grad_norm_threshold: 5.3724 (MA grad_norm: 5.3547)\nParameters with gradients: 201, Global norm: 0.7464\nUpdated grad_norm_threshold: 4.6829 (MA grad_norm: 4.6063)\nParameters with gradients: 201, Global norm: 7.4280\nUpdated grad_norm_threshold: 4.7027 (MA grad_norm: 4.7049)\nParameters with gradients: 201, Global norm: 4.4276\nUpdated grad_norm_threshold: 4.7146 (MA grad_norm: 4.7159)\nParameters with gradients: 201, Global norm: 3.1151\nUpdated grad_norm_threshold: 3.9513 (MA grad_norm: 3.8665)\nParameters with gradients: 201, Global norm: 9.8005\nUpdated grad_norm_threshold: 3.8385 (MA grad_norm: 3.8260)\nParameters with gradients: 201, Global norm: 0.9589\nUpdated grad_norm_threshold: 3.7170 (MA grad_norm: 3.7035)\nParameters with gradients: 201, Global norm: 12.2441\nUpdated grad_norm_threshold: 4.0905 (MA grad_norm: 4.1320)\nParameters with gradients: 201, Global norm: 7.9364\nUpdated grad_norm_threshold: 4.3940 (MA grad_norm: 4.4278)\nParameters with gradients: 201, Global norm: 25.9942\nUpdated grad_norm_threshold: 5.5198 (MA grad_norm: 5.6449)\nParameters with gradients: 201, Global norm: 15.8638\nUpdated grad_norm_threshold: 5.9373 (MA grad_norm: 5.9837)\nParameters with gradients: 201, Global norm: 12.2590\nUpdated grad_norm_threshold: 6.5067 (MA grad_norm: 6.5699)\nParameters with gradients: 201, Global norm: 8.1980\nUpdated grad_norm_threshold: 6.8909 (MA grad_norm: 6.9336)\nParameters with gradients: 201, Global norm: 14.2388\nUpdated grad_norm_threshold: 7.5284 (MA grad_norm: 7.5993)\nParameters with gradients: 201, Global norm: 12.9775\nUpdated grad_norm_threshold: 7.9691 (MA grad_norm: 8.0181)\nParameters with gradients: 201, Global norm: 15.1472\nUpdated grad_norm_threshold: 8.4651 (MA grad_norm: 8.5202)\nParameters with gradients: 201, Global norm: 8.1662\nUpdated grad_norm_threshold: 8.5889 (MA grad_norm: 8.6026)\nParameters with gradients: 201, Global norm: 5.8187\nUpdated grad_norm_threshold: 8.8428 (MA grad_norm: 8.8711)\nParameters with gradients: 201, Global norm: 13.7103\nUpdated grad_norm_threshold: 9.3134 (MA grad_norm: 9.3656)\nParameters with gradients: 201, Global norm: 5.2417\nUpdated grad_norm_threshold: 9.3307 (MA grad_norm: 9.3326)\nParameters with gradients: 201, Global norm: 12.9642\nUpdated grad_norm_threshold: 9.8087 (MA grad_norm: 9.8618)\nParameters with gradients: 201, Global norm: 22.3936\nUpdated grad_norm_threshold: 10.8306 (MA grad_norm: 10.9442)\nParameters with gradients: 201, Global norm: 15.0562\nUpdated grad_norm_threshold: 11.2761 (MA grad_norm: 11.3256)\nParameters with gradients: 201, Global norm: 4.1996\nUpdated grad_norm_threshold: 11.3104 (MA grad_norm: 11.3142)\nParameters with gradients: 201, Global norm: 7.3852\nUpdated grad_norm_threshold: 11.5060 (MA grad_norm: 11.5277)\nParameters with gradients: 201, Global norm: 12.7984\nUpdated grad_norm_threshold: 11.6604 (MA grad_norm: 11.6776)\nParameters with gradients: 201, Global norm: 3.0463\nUpdated grad_norm_threshold: 11.7698 (MA grad_norm: 11.7820)\nParameters with gradients: 201, Global norm: 1.3182\nUpdated grad_norm_threshold: 11.2891 (MA grad_norm: 11.2357)\nParameters with gradients: 201, Global norm: 8.8083\nUpdated grad_norm_threshold: 11.2802 (MA grad_norm: 11.2793)\nParameters with gradients: 201, Global norm: 8.1244\nUpdated grad_norm_threshold: 10.4752 (MA grad_norm: 10.3858)\nParameters with gradients: 201, Global norm: 4.4941\nUpdated grad_norm_threshold: 9.8831 (MA grad_norm: 9.8173)\nParameters with gradients: 201, Global norm: 4.4318\nUpdated grad_norm_threshold: 9.4716 (MA grad_norm: 9.4259)\nParameters with gradients: 201, Global norm: 3.9019\nUpdated grad_norm_threshold: 9.2372 (MA grad_norm: 9.2111)\nParameters with gradients: 201, Global norm: 6.5049\nUpdated grad_norm_threshold: 8.8657 (MA grad_norm: 8.8244)\nParameters with gradients: 201, Global norm: 1.2723\nUpdated grad_norm_threshold: 8.3018 (MA grad_norm: 8.2392)\nParameters with gradients: 201, Global norm: 23.7749\nUpdated grad_norm_threshold: 8.6337 (MA grad_norm: 8.6705)\nParameters with gradients: 201, Global norm: 2.4049\nUpdated grad_norm_threshold: 8.4076 (MA grad_norm: 8.3825)\nParameters with gradients: 201, Global norm: 11.0700\nUpdated grad_norm_threshold: 8.6213 (MA grad_norm: 8.6450)\nParameters with gradients: 201, Global norm: 8.6138\nUpdated grad_norm_threshold: 8.4133 (MA grad_norm: 8.3902)\nParameters with gradients: 201, Global norm: 6.2795\nUpdated grad_norm_threshold: 8.4392 (MA grad_norm: 8.4421)\nParameters with gradients: 201, Global norm: 2.6054\nUpdated grad_norm_threshold: 7.9757 (MA grad_norm: 7.9242)\nParameters with gradients: 201, Global norm: 9.3098\nUpdated grad_norm_threshold: 7.3406 (MA grad_norm: 7.2700)\nParameters with gradients: 201, Global norm: 1.0399\nUpdated grad_norm_threshold: 6.6463 (MA grad_norm: 6.5692)\nParameters with gradients: 201, Global norm: 0.9336\nUpdated grad_norm_threshold: 6.4299 (MA grad_norm: 6.4059)\nParameters with gradients: 201, Global norm: 0.7572\nUpdated grad_norm_threshold: 6.1100 (MA grad_norm: 6.0745)\nParameters with gradients: 201, Global norm: 0.6406\nUpdated grad_norm_threshold: 5.5309 (MA grad_norm: 5.4666)\nParameters with gradients: 201, Global norm: 2.3312\nUpdated grad_norm_threshold: 5.4408 (MA grad_norm: 5.4308)\nParameters with gradients: 201, Global norm: 9.3721\nUpdated grad_norm_threshold: 5.7943 (MA grad_norm: 5.8335)\nParameters with gradients: 201, Global norm: 9.0895\nUpdated grad_norm_threshold: 5.8422 (MA grad_norm: 5.8476)\nParameters with gradients: 201, Global norm: 5.6717\nUpdated grad_norm_threshold: 5.7367 (MA grad_norm: 5.7249)\nParameters with gradients: 201, Global norm: 1.7247\nUpdated grad_norm_threshold: 5.6015 (MA grad_norm: 5.5865)\nParameters with gradients: 201, Global norm: 1.7210\nUpdated grad_norm_threshold: 5.4660 (MA grad_norm: 5.4509)\nParameters with gradients: 201, Global norm: 9.3078\nUpdated grad_norm_threshold: 5.6957 (MA grad_norm: 5.7212)\nParameters with gradients: 201, Global norm: 9.3847\nUpdated grad_norm_threshold: 5.8483 (MA grad_norm: 5.8652)\nParameters with gradients: 201, Global norm: 5.1902\nUpdated grad_norm_threshold: 6.0398 (MA grad_norm: 6.0611)\nParameters with gradients: 201, Global norm: 7.2708\nUpdated grad_norm_threshold: 5.3163 (MA grad_norm: 5.2359)\nParameters with gradients: 201, Global norm: 0.7284\nUpdated grad_norm_threshold: 5.1685 (MA grad_norm: 5.1521)\nParameters with gradients: 201, Global norm: 4.6383\nUpdated grad_norm_threshold: 4.8643 (MA grad_norm: 4.8305)\nParameters with gradients: 201, Global norm: 6.8427\nUpdated grad_norm_threshold: 4.7542 (MA grad_norm: 4.7419)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:13:36] Energy consumed for RAM : 0.000583 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:13:36] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:13:36] Energy consumed for All CPU : 0.001239 kWh\n[codecarbon INFO @ 08:13:36] Energy consumed for all GPUs : 0.002219 kWh. Total GPU Power : 75.85730023309998 W\n[codecarbon INFO @ 08:13:36] 0.004042 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.1589\nUpdated grad_norm_threshold: 4.6027 (MA grad_norm: 4.5859)\nParameters with gradients: 201, Global norm: 3.5023\nUpdated grad_norm_threshold: 4.6280 (MA grad_norm: 4.6308)\nParameters with gradients: 201, Global norm: 10.3098\nUpdated grad_norm_threshold: 4.6755 (MA grad_norm: 4.6808)\nParameters with gradients: 201, Global norm: 1.6731\nUpdated grad_norm_threshold: 4.7087 (MA grad_norm: 4.7124)\nParameters with gradients: 201, Global norm: 2.3841\nUpdated grad_norm_threshold: 4.7773 (MA grad_norm: 4.7849)\nParameters with gradients: 201, Global norm: 1.7565\nUpdated grad_norm_threshold: 4.8292 (MA grad_norm: 4.8349)\nParameters with gradients: 201, Global norm: 7.7065\nUpdated grad_norm_threshold: 5.1523 (MA grad_norm: 5.1882)\nParameters with gradients: 201, Global norm: 1.4403\nUpdated grad_norm_threshold: 5.1445 (MA grad_norm: 5.1437)\nParameters with gradients: 201, Global norm: 9.8674\nUpdated grad_norm_threshold: 5.1660 (MA grad_norm: 5.1684)\nParameters with gradients: 201, Global norm: 12.3757\nUpdated grad_norm_threshold: 5.3161 (MA grad_norm: 5.3327)\nParameters with gradients: 201, Global norm: 16.3151\nUpdated grad_norm_threshold: 5.8100 (MA grad_norm: 5.8649)\nParameters with gradients: 201, Global norm: 6.8943\nUpdated grad_norm_threshold: 6.0921 (MA grad_norm: 6.1234)\nParameters with gradients: 201, Global norm: 7.6120\nUpdated grad_norm_threshold: 6.3854 (MA grad_norm: 6.4179)\nParameters with gradients: 201, Global norm: 4.5156\nUpdated grad_norm_threshold: 6.1990 (MA grad_norm: 6.1783)\nParameters with gradients: 201, Global norm: 10.6616\nUpdated grad_norm_threshold: 6.2379 (MA grad_norm: 6.2422)\nParameters with gradients: 201, Global norm: 5.7887\nUpdated grad_norm_threshold: 6.2687 (MA grad_norm: 6.2721)\nParameters with gradients: 201, Global norm: 2.5251\nUpdated grad_norm_threshold: 6.0582 (MA grad_norm: 6.0348)\nParameters with gradients: 201, Global norm: 2.3120\nUpdated grad_norm_threshold: 6.1084 (MA grad_norm: 6.1140)\nParameters with gradients: 201, Global norm: 5.2922\nUpdated grad_norm_threshold: 6.1429 (MA grad_norm: 6.1467)\nParameters with gradients: 201, Global norm: 1.0006\nUpdated grad_norm_threshold: 5.8834 (MA grad_norm: 5.8546)\nParameters with gradients: 201, Global norm: 7.0126\nUpdated grad_norm_threshold: 6.0309 (MA grad_norm: 6.0473)\nParameters with gradients: 201, Global norm: 2.3283\nUpdated grad_norm_threshold: 5.9928 (MA grad_norm: 5.9886)\nParameters with gradients: 201, Global norm: 29.3177\nUpdated grad_norm_threshold: 6.8443 (MA grad_norm: 6.9390)\nParameters with gradients: 201, Global norm: 0.7481\nUpdated grad_norm_threshold: 6.8879 (MA grad_norm: 6.8927)\nParameters with gradients: 201, Global norm: 8.1507\nUpdated grad_norm_threshold: 7.1517 (MA grad_norm: 7.1810)\nParameters with gradients: 201, Global norm: 7.8834\nUpdated grad_norm_threshold: 7.4538 (MA grad_norm: 7.4874)\nParameters with gradients: 201, Global norm: 9.4273\nUpdated grad_norm_threshold: 7.5615 (MA grad_norm: 7.5734)\nParameters with gradients: 201, Global norm: 2.6965\nUpdated grad_norm_threshold: 7.6288 (MA grad_norm: 7.6362)\nParameters with gradients: 201, Global norm: 10.6454\nUpdated grad_norm_threshold: 7.6705 (MA grad_norm: 7.6751)\nParameters with gradients: 201, Global norm: 7.8039\nUpdated grad_norm_threshold: 7.4689 (MA grad_norm: 7.4465)\nParameters with gradients: 201, Global norm: 2.5829\nUpdated grad_norm_threshold: 6.8308 (MA grad_norm: 6.7599)\nParameters with gradients: 201, Global norm: 6.9731\nUpdated grad_norm_threshold: 6.7706 (MA grad_norm: 6.7639)\nParameters with gradients: 201, Global norm: 2.4528\nUpdated grad_norm_threshold: 6.5324 (MA grad_norm: 6.5059)\nParameters with gradients: 201, Global norm: 0.8163\nUpdated grad_norm_threshold: 6.3421 (MA grad_norm: 6.3210)\nParameters with gradients: 201, Global norm: 1.4760\nUpdated grad_norm_threshold: 5.9097 (MA grad_norm: 5.8617)\nParameters with gradients: 201, Global norm: 2.7484\nUpdated grad_norm_threshold: 5.7297 (MA grad_norm: 5.7097)\nParameters with gradients: 201, Global norm: 7.1771\nUpdated grad_norm_threshold: 5.9210 (MA grad_norm: 5.9423)\nParameters with gradients: 201, Global norm: 0.7300\nUpdated grad_norm_threshold: 5.8689 (MA grad_norm: 5.8632)\nParameters with gradients: 201, Global norm: 2.7981\nUpdated grad_norm_threshold: 5.7515 (MA grad_norm: 5.7385)\nParameters with gradients: 201, Global norm: 1.6993\nUpdated grad_norm_threshold: 5.7712 (MA grad_norm: 5.7734)\nParameters with gradients: 201, Global norm: 6.8512\nUpdated grad_norm_threshold: 5.7659 (MA grad_norm: 5.7653)\nParameters with gradients: 201, Global norm: 1.4480\nUpdated grad_norm_threshold: 5.7258 (MA grad_norm: 5.7213)\nParameters with gradients: 201, Global norm: 3.2403\nUpdated grad_norm_threshold: 4.5483 (MA grad_norm: 4.4174)\nParameters with gradients: 201, Global norm: 6.2317\nUpdated grad_norm_threshold: 4.6773 (MA grad_norm: 4.6916)\nParameters with gradients: 201, Global norm: 1.9725\nUpdated grad_norm_threshold: 4.4122 (MA grad_norm: 4.3827)\nParameters with gradients: 201, Global norm: 0.6466\nUpdated grad_norm_threshold: 4.0600 (MA grad_norm: 4.0209)\nParameters with gradients: 201, Global norm: 9.0938\nUpdated grad_norm_threshold: 4.0098 (MA grad_norm: 4.0042)\nParameters with gradients: 201, Global norm: 1.2109\nUpdated grad_norm_threshold: 3.9379 (MA grad_norm: 3.9299)\nParameters with gradients: 201, Global norm: 8.6298\nUpdated grad_norm_threshold: 3.8400 (MA grad_norm: 3.8291)\nParameters with gradients: 201, Global norm: 6.7461\nUpdated grad_norm_threshold: 3.7826 (MA grad_norm: 3.7762)\nParameters with gradients: 201, Global norm: 6.6925\nUpdated grad_norm_threshold: 3.9618 (MA grad_norm: 3.9817)\nParameters with gradients: 201, Global norm: 6.8338\nUpdated grad_norm_threshold: 3.9735 (MA grad_norm: 3.9748)\nParameters with gradients: 201, Global norm: 6.5287\nUpdated grad_norm_threshold: 4.1580 (MA grad_norm: 4.1786)\nParameters with gradients: 201, Global norm: 4.3465\nUpdated grad_norm_threshold: 4.3354 (MA grad_norm: 4.3551)\nParameters with gradients: 201, Global norm: 3.9697\nUpdated grad_norm_threshold: 4.4653 (MA grad_norm: 4.4798)\nAverage loss: 0.1331\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.8529, F1: 0.8535\n\nEpoch 4/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 4:   0%|          | 0/230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.4539\nUpdated grad_norm_threshold: 4.3751 (MA grad_norm: 4.3650)\nParameters with gradients: 201, Global norm: 2.7987\nUpdated grad_norm_threshold: 4.1690 (MA grad_norm: 4.1461)\nParameters with gradients: 201, Global norm: 1.3002\nUpdated grad_norm_threshold: 4.1741 (MA grad_norm: 4.1746)\nParameters with gradients: 201, Global norm: 0.4817\nUpdated grad_norm_threshold: 4.0703 (MA grad_norm: 4.0588)\nParameters with gradients: 201, Global norm: 5.4398\nUpdated grad_norm_threshold: 4.2283 (MA grad_norm: 4.2458)\nParameters with gradients: 201, Global norm: 0.7709\nUpdated grad_norm_threshold: 3.9705 (MA grad_norm: 3.9418)\nParameters with gradients: 201, Global norm: 0.7369\nUpdated grad_norm_threshold: 3.9127 (MA grad_norm: 3.9063)\nParameters with gradients: 201, Global norm: 0.5201\nUpdated grad_norm_threshold: 3.7845 (MA grad_norm: 3.7702)\nParameters with gradients: 201, Global norm: 0.6332\nUpdated grad_norm_threshold: 3.5197 (MA grad_norm: 3.4903)\nParameters with gradients: 201, Global norm: 2.5503\nUpdated grad_norm_threshold: 3.5193 (MA grad_norm: 3.5192)\nParameters with gradients: 201, Global norm: 3.2637\nUpdated grad_norm_threshold: 3.6370 (MA grad_norm: 3.6501)\nParameters with gradients: 201, Global norm: 0.4814\nUpdated grad_norm_threshold: 3.2612 (MA grad_norm: 3.2194)\nParameters with gradients: 201, Global norm: 0.5766\nUpdated grad_norm_threshold: 3.1951 (MA grad_norm: 3.1877)\nParameters with gradients: 201, Global norm: 14.7757\nUpdated grad_norm_threshold: 3.4650 (MA grad_norm: 3.4950)\nParameters with gradients: 201, Global norm: 6.8072\nUpdated grad_norm_threshold: 3.4948 (MA grad_norm: 3.4981)\nParameters with gradients: 201, Global norm: 2.5475\nUpdated grad_norm_threshold: 3.3112 (MA grad_norm: 3.2908)\nParameters with gradients: 201, Global norm: 1.0742\nUpdated grad_norm_threshold: 3.0337 (MA grad_norm: 3.0028)\nParameters with gradients: 201, Global norm: 1.7765\nUpdated grad_norm_threshold: 2.7921 (MA grad_norm: 2.7652)\nParameters with gradients: 201, Global norm: 0.9735\nUpdated grad_norm_threshold: 2.6161 (MA grad_norm: 2.5966)\nParameters with gradients: 201, Global norm: 1.9496\nUpdated grad_norm_threshold: 2.5076 (MA grad_norm: 2.4956)\nParameters with gradients: 201, Global norm: 0.4056\nUpdated grad_norm_threshold: 2.4946 (MA grad_norm: 2.4932)\nParameters with gradients: 201, Global norm: 1.4234\nUpdated grad_norm_threshold: 2.4314 (MA grad_norm: 2.4244)\nParameters with gradients: 201, Global norm: 3.1663\nUpdated grad_norm_threshold: 2.5091 (MA grad_norm: 2.5177)\nParameters with gradients: 201, Global norm: 0.6470\nUpdated grad_norm_threshold: 2.5243 (MA grad_norm: 2.5260)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:13:51] Energy consumed for RAM : 0.000666 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:13:51] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:13:51] Energy consumed for All CPU : 0.001416 kWh\n[codecarbon INFO @ 08:13:51] Energy consumed for all GPUs : 0.002537 kWh. Total GPU Power : 76.15328063180846 W\n[codecarbon INFO @ 08:13:51] 0.004619 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:13:51] 0.014219 g.CO2eq/s mean an estimation of 448.4258184987255 kg.CO2eq/year\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.3779\nUpdated grad_norm_threshold: 2.2980 (MA grad_norm: 2.2729)\nParameters with gradients: 201, Global norm: 0.3973\nUpdated grad_norm_threshold: 2.2586 (MA grad_norm: 2.2542)\nParameters with gradients: 201, Global norm: 0.4843\nUpdated grad_norm_threshold: 2.2433 (MA grad_norm: 2.2416)\nParameters with gradients: 201, Global norm: 0.7729\nUpdated grad_norm_threshold: 2.2531 (MA grad_norm: 2.2542)\nParameters with gradients: 201, Global norm: 0.4718\nUpdated grad_norm_threshold: 2.2468 (MA grad_norm: 2.2461)\nParameters with gradients: 201, Global norm: 2.5406\nUpdated grad_norm_threshold: 2.2458 (MA grad_norm: 2.2457)\nParameters with gradients: 201, Global norm: 2.6228\nUpdated grad_norm_threshold: 2.2168 (MA grad_norm: 2.2136)\nParameters with gradients: 201, Global norm: 0.5348\nUpdated grad_norm_threshold: 2.2163 (MA grad_norm: 2.2163)\nParameters with gradients: 201, Global norm: 0.8105\nUpdated grad_norm_threshold: 2.2268 (MA grad_norm: 2.2280)\nParameters with gradients: 201, Global norm: 0.2930\nUpdated grad_norm_threshold: 1.5761 (MA grad_norm: 1.5038)\nParameters with gradients: 201, Global norm: 0.4000\nUpdated grad_norm_threshold: 1.2227 (MA grad_norm: 1.1835)\nParameters with gradients: 201, Global norm: 2.4577\nUpdated grad_norm_threshold: 1.1834 (MA grad_norm: 1.1790)\nParameters with gradients: 201, Global norm: 14.3756\nUpdated grad_norm_threshold: 1.7780 (MA grad_norm: 1.8441)\nParameters with gradients: 201, Global norm: 0.6054\nUpdated grad_norm_threshold: 1.7848 (MA grad_norm: 1.7855)\nParameters with gradients: 201, Global norm: 6.3991\nUpdated grad_norm_threshold: 2.0296 (MA grad_norm: 2.0568)\nParameters with gradients: 201, Global norm: 0.5924\nUpdated grad_norm_threshold: 1.9930 (MA grad_norm: 1.9889)\nParameters with gradients: 201, Global norm: 0.4550\nUpdated grad_norm_threshold: 1.9916 (MA grad_norm: 1.9914)\nParameters with gradients: 201, Global norm: 2.6541\nUpdated grad_norm_threshold: 2.0468 (MA grad_norm: 2.0529)\nParameters with gradients: 201, Global norm: 0.5049\nUpdated grad_norm_threshold: 1.9326 (MA grad_norm: 1.9199)\nParameters with gradients: 201, Global norm: 0.4661\nUpdated grad_norm_threshold: 1.9130 (MA grad_norm: 1.9108)\nParameters with gradients: 201, Global norm: 13.8513\nUpdated grad_norm_threshold: 2.5173 (MA grad_norm: 2.5845)\nParameters with gradients: 201, Global norm: 0.8003\nUpdated grad_norm_threshold: 2.5959 (MA grad_norm: 2.6046)\nParameters with gradients: 201, Global norm: 0.5363\nUpdated grad_norm_threshold: 2.6061 (MA grad_norm: 2.6072)\nParameters with gradients: 201, Global norm: 0.3735\nUpdated grad_norm_threshold: 2.5891 (MA grad_norm: 2.5873)\nParameters with gradients: 201, Global norm: 15.1266\nUpdated grad_norm_threshold: 3.2469 (MA grad_norm: 3.3200)\nParameters with gradients: 201, Global norm: 0.6096\nUpdated grad_norm_threshold: 3.2258 (MA grad_norm: 3.2235)\nParameters with gradients: 201, Global norm: 0.2806\nUpdated grad_norm_threshold: 3.1183 (MA grad_norm: 3.1063)\nParameters with gradients: 201, Global norm: 0.8956\nUpdated grad_norm_threshold: 3.1238 (MA grad_norm: 3.1244)\nParameters with gradients: 201, Global norm: 3.7062\nUpdated grad_norm_threshold: 3.2546 (MA grad_norm: 3.2692)\nParameters with gradients: 201, Global norm: 6.3232\nUpdated grad_norm_threshold: 3.5391 (MA grad_norm: 3.5707)\nParameters with gradients: 201, Global norm: 0.6882\nUpdated grad_norm_threshold: 3.5805 (MA grad_norm: 3.5851)\nParameters with gradients: 201, Global norm: 2.0879\nUpdated grad_norm_threshold: 3.5680 (MA grad_norm: 3.5666)\nParameters with gradients: 201, Global norm: 0.2666\nUpdated grad_norm_threshold: 2.9318 (MA grad_norm: 2.8611)\nParameters with gradients: 201, Global norm: 0.3747\nUpdated grad_norm_threshold: 2.8578 (MA grad_norm: 2.8496)\nParameters with gradients: 201, Global norm: 16.1446\nUpdated grad_norm_threshold: 3.2890 (MA grad_norm: 3.3369)\nParameters with gradients: 201, Global norm: 0.7212\nUpdated grad_norm_threshold: 3.3379 (MA grad_norm: 3.3433)\nParameters with gradients: 201, Global norm: 0.7264\nUpdated grad_norm_threshold: 3.3550 (MA grad_norm: 3.3569)\nParameters with gradients: 201, Global norm: 0.3198\nUpdated grad_norm_threshold: 3.2517 (MA grad_norm: 3.2402)\nParameters with gradients: 201, Global norm: 0.4376\nUpdated grad_norm_threshold: 3.2383 (MA grad_norm: 3.2368)\nParameters with gradients: 201, Global norm: 1.3206\nUpdated grad_norm_threshold: 3.2754 (MA grad_norm: 3.2795)\nParameters with gradients: 201, Global norm: 0.8340\nUpdated grad_norm_threshold: 2.6933 (MA grad_norm: 2.6287)\nParameters with gradients: 201, Global norm: 10.6605\nUpdated grad_norm_threshold: 3.0788 (MA grad_norm: 3.1217)\nParameters with gradients: 201, Global norm: 0.4065\nUpdated grad_norm_threshold: 3.1116 (MA grad_norm: 3.1152)\nParameters with gradients: 201, Global norm: 0.4157\nUpdated grad_norm_threshold: 3.1167 (MA grad_norm: 3.1173)\nParameters with gradients: 201, Global norm: 2.0959\nUpdated grad_norm_threshold: 2.5309 (MA grad_norm: 2.4658)\nParameters with gradients: 201, Global norm: 2.7993\nUpdated grad_norm_threshold: 2.5708 (MA grad_norm: 2.5753)\nParameters with gradients: 201, Global norm: 0.3221\nUpdated grad_norm_threshold: 2.5767 (MA grad_norm: 2.5773)\nParameters with gradients: 201, Global norm: 0.3000\nUpdated grad_norm_threshold: 2.5505 (MA grad_norm: 2.5476)\nParameters with gradients: 201, Global norm: 0.9247\nUpdated grad_norm_threshold: 2.4227 (MA grad_norm: 2.4085)\nParameters with gradients: 201, Global norm: 0.4604\nUpdated grad_norm_threshold: 2.1461 (MA grad_norm: 2.1153)\nParameters with gradients: 201, Global norm: 0.6541\nUpdated grad_norm_threshold: 2.1169 (MA grad_norm: 2.1136)\nParameters with gradients: 201, Global norm: 0.3974\nUpdated grad_norm_threshold: 2.0379 (MA grad_norm: 2.0291)\nParameters with gradients: 201, Global norm: 0.3794\nUpdated grad_norm_threshold: 2.0351 (MA grad_norm: 2.0347)\nParameters with gradients: 201, Global norm: 0.3896\nUpdated grad_norm_threshold: 2.0354 (MA grad_norm: 2.0355)\nParameters with gradients: 201, Global norm: 0.2390\nUpdated grad_norm_threshold: 1.3197 (MA grad_norm: 1.2402)\nParameters with gradients: 201, Global norm: 17.7470\nUpdated grad_norm_threshold: 2.0143 (MA grad_norm: 2.0915)\nParameters with gradients: 201, Global norm: 0.9402\nUpdated grad_norm_threshold: 2.0934 (MA grad_norm: 2.1022)\nParameters with gradients: 201, Global norm: 0.2573\nUpdated grad_norm_threshold: 2.0985 (MA grad_norm: 2.0991)\nParameters with gradients: 201, Global norm: 0.4444\nUpdated grad_norm_threshold: 2.0993 (MA grad_norm: 2.0994)\nParameters with gradients: 201, Global norm: 0.2568\nUpdated grad_norm_threshold: 2.0515 (MA grad_norm: 2.0462)\nParameters with gradients: 201, Global norm: 0.4811\nUpdated grad_norm_threshold: 2.0309 (MA grad_norm: 2.0286)\nParameters with gradients: 201, Global norm: 0.8711\nUpdated grad_norm_threshold: 1.5883 (MA grad_norm: 1.5391)\nParameters with gradients: 201, Global norm: 0.3381\nUpdated grad_norm_threshold: 1.5409 (MA grad_norm: 1.5357)\nParameters with gradients: 201, Global norm: 0.4683\nUpdated grad_norm_threshold: 1.5386 (MA grad_norm: 1.5383)\nParameters with gradients: 201, Global norm: 2.9623\nUpdated grad_norm_threshold: 1.5773 (MA grad_norm: 1.5816)\nParameters with gradients: 201, Global norm: 0.2510\nUpdated grad_norm_threshold: 1.4665 (MA grad_norm: 1.4542)\nParameters with gradients: 201, Global norm: 0.5147\nUpdated grad_norm_threshold: 1.4641 (MA grad_norm: 1.4638)\nParameters with gradients: 201, Global norm: 0.1804\nUpdated grad_norm_threshold: 1.4585 (MA grad_norm: 1.4579)\nParameters with gradients: 201, Global norm: 0.2706\nUpdated grad_norm_threshold: 1.4285 (MA grad_norm: 1.4252)\nParameters with gradients: 201, Global norm: 0.3651\nUpdated grad_norm_threshold: 1.4212 (MA grad_norm: 1.4204)\nParameters with gradients: 201, Global norm: 0.1971\nUpdated grad_norm_threshold: 1.3999 (MA grad_norm: 1.3975)\nParameters with gradients: 201, Global norm: 0.3466\nUpdated grad_norm_threshold: 1.3955 (MA grad_norm: 1.3950)\nParameters with gradients: 201, Global norm: 0.4962\nUpdated grad_norm_threshold: 1.4003 (MA grad_norm: 1.4009)\nParameters with gradients: 201, Global norm: 0.3484\nUpdated grad_norm_threshold: 1.3989 (MA grad_norm: 1.3988)\nParameters with gradients: 201, Global norm: 0.4342\nUpdated grad_norm_threshold: 1.4076 (MA grad_norm: 1.4085)\nParameters with gradients: 201, Global norm: 1.3156\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5870)\nParameters with gradients: 201, Global norm: 0.5421\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5671)\nParameters with gradients: 201, Global norm: 0.6046\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5844)\nParameters with gradients: 201, Global norm: 7.5025\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9373)\nParameters with gradients: 201, Global norm: 0.3837\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9437)\nParameters with gradients: 201, Global norm: 1.1081\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9750)\nParameters with gradients: 201, Global norm: 3.2647\nUpdated grad_norm_threshold: 1.0852 (MA grad_norm: 1.0947)\nParameters with gradients: 201, Global norm: 0.4932\nUpdated grad_norm_threshold: 1.1008 (MA grad_norm: 1.1025)\nParameters with gradients: 201, Global norm: 0.5866\nUpdated grad_norm_threshold: 1.1076 (MA grad_norm: 1.1084)\nParameters with gradients: 201, Global norm: 0.2864\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9746)\nParameters with gradients: 201, Global norm: 0.2686\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9755)\nParameters with gradients: 201, Global norm: 1.1261\nUpdated grad_norm_threshold: 1.0054 (MA grad_norm: 1.0060)\nParameters with gradients: 201, Global norm: 0.4023\nUpdated grad_norm_threshold: 1.0160 (MA grad_norm: 1.0171)\nParameters with gradients: 201, Global norm: 0.4924\nUpdated grad_norm_threshold: 1.0270 (MA grad_norm: 1.0282)\nParameters with gradients: 201, Global norm: 16.1542\nUpdated grad_norm_threshold: 1.7386 (MA grad_norm: 1.8177)\nParameters with gradients: 201, Global norm: 0.6206\nUpdated grad_norm_threshold: 1.8288 (MA grad_norm: 1.8389)\nParameters with gradients: 201, Global norm: 0.2790\nUpdated grad_norm_threshold: 1.8348 (MA grad_norm: 1.8355)\nParameters with gradients: 201, Global norm: 9.9497\nUpdated grad_norm_threshold: 2.2608 (MA grad_norm: 2.3082)\nParameters with gradients: 201, Global norm: 0.2602\nUpdated grad_norm_threshold: 2.2994 (MA grad_norm: 2.3037)\nParameters with gradients: 201, Global norm: 0.2399\nUpdated grad_norm_threshold: 2.2946 (MA grad_norm: 2.2940)\nParameters with gradients: 201, Global norm: 0.3714\nUpdated grad_norm_threshold: 2.2516 (MA grad_norm: 2.2468)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:14:06] Energy consumed for RAM : 0.000750 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:14:06] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:14:06] Energy consumed for All CPU : 0.001593 kWh\n[codecarbon INFO @ 08:14:06] Energy consumed for all GPUs : 0.002853 kWh. Total GPU Power : 75.93430286048499 W\n[codecarbon INFO @ 08:14:06] 0.005195 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.4694\nUpdated grad_norm_threshold: 2.2440 (MA grad_norm: 2.2432)\nParameters with gradients: 201, Global norm: 1.0789\nUpdated grad_norm_threshold: 2.2646 (MA grad_norm: 2.2669)\nParameters with gradients: 201, Global norm: 0.3075\nUpdated grad_norm_threshold: 1.9429 (MA grad_norm: 1.9071)\nParameters with gradients: 201, Global norm: 0.2955\nUpdated grad_norm_threshold: 1.9068 (MA grad_norm: 1.9027)\nParameters with gradients: 201, Global norm: 0.8615\nUpdated grad_norm_threshold: 1.8920 (MA grad_norm: 1.8904)\nParameters with gradients: 201, Global norm: 10.2500\nUpdated grad_norm_threshold: 2.2049 (MA grad_norm: 2.2397)\nParameters with gradients: 201, Global norm: 0.4117\nUpdated grad_norm_threshold: 2.2325 (MA grad_norm: 2.2356)\nParameters with gradients: 201, Global norm: 1.0684\nUpdated grad_norm_threshold: 2.2570 (MA grad_norm: 2.2597)\nParameters with gradients: 201, Global norm: 0.9719\nUpdated grad_norm_threshold: 2.2903 (MA grad_norm: 2.2940)\nParameters with gradients: 201, Global norm: 0.5750\nUpdated grad_norm_threshold: 2.3074 (MA grad_norm: 2.3093)\nParameters with gradients: 201, Global norm: 0.2606\nUpdated grad_norm_threshold: 2.2701 (MA grad_norm: 2.2660)\nParameters with gradients: 201, Global norm: 0.4873\nUpdated grad_norm_threshold: 2.2702 (MA grad_norm: 2.2703)\nParameters with gradients: 201, Global norm: 0.2720\nUpdated grad_norm_threshold: 2.2603 (MA grad_norm: 2.2592)\nParameters with gradients: 201, Global norm: 2.8791\nUpdated grad_norm_threshold: 1.6620 (MA grad_norm: 1.5955)\nParameters with gradients: 201, Global norm: 0.6205\nUpdated grad_norm_threshold: 1.6021 (MA grad_norm: 1.5955)\nParameters with gradients: 201, Global norm: 0.5216\nUpdated grad_norm_threshold: 1.6071 (MA grad_norm: 1.6076)\nParameters with gradients: 201, Global norm: 2.4441\nUpdated grad_norm_threshold: 1.2698 (MA grad_norm: 1.2323)\nParameters with gradients: 201, Global norm: 0.2775\nUpdated grad_norm_threshold: 1.2368 (MA grad_norm: 1.2332)\nParameters with gradients: 201, Global norm: 3.6032\nUpdated grad_norm_threshold: 1.3849 (MA grad_norm: 1.4013)\nParameters with gradients: 201, Global norm: 0.8301\nUpdated grad_norm_threshold: 1.4203 (MA grad_norm: 1.4243)\nParameters with gradients: 201, Global norm: 9.7721\nUpdated grad_norm_threshold: 1.8425 (MA grad_norm: 1.8894)\nParameters with gradients: 201, Global norm: 0.5093\nUpdated grad_norm_threshold: 1.8591 (MA grad_norm: 1.8609)\nParameters with gradients: 201, Global norm: 0.2556\nUpdated grad_norm_threshold: 1.8584 (MA grad_norm: 1.8583)\nParameters with gradients: 201, Global norm: 0.2632\nUpdated grad_norm_threshold: 1.8569 (MA grad_norm: 1.8567)\nParameters with gradients: 201, Global norm: 2.7101\nUpdated grad_norm_threshold: 1.9399 (MA grad_norm: 1.9492)\nParameters with gradients: 201, Global norm: 0.4488\nUpdated grad_norm_threshold: 1.5072 (MA grad_norm: 1.4591)\nParameters with gradients: 201, Global norm: 0.6434\nUpdated grad_norm_threshold: 1.4743 (MA grad_norm: 1.4707)\nParameters with gradients: 201, Global norm: 0.2544\nUpdated grad_norm_threshold: 1.4344 (MA grad_norm: 1.4300)\nParameters with gradients: 201, Global norm: 0.3569\nUpdated grad_norm_threshold: 1.4027 (MA grad_norm: 1.3992)\nParameters with gradients: 201, Global norm: 0.6141\nUpdated grad_norm_threshold: 1.4013 (MA grad_norm: 1.4012)\nParameters with gradients: 201, Global norm: 0.3985\nUpdated grad_norm_threshold: 1.4074 (MA grad_norm: 1.4081)\nParameters with gradients: 201, Global norm: 0.2424\nUpdated grad_norm_threshold: 1.3970 (MA grad_norm: 1.3958)\nParameters with gradients: 201, Global norm: 0.4041\nUpdated grad_norm_threshold: 1.4019 (MA grad_norm: 1.4024)\nParameters with gradients: 201, Global norm: 0.3208\nUpdated grad_norm_threshold: 1.2873 (MA grad_norm: 1.2745)\nParameters with gradients: 201, Global norm: 0.5169\nUpdated grad_norm_threshold: 1.2711 (MA grad_norm: 1.2693)\nParameters with gradients: 201, Global norm: 0.2546\nUpdated grad_norm_threshold: 1.2575 (MA grad_norm: 1.2560)\nParameters with gradients: 201, Global norm: 0.8564\nUpdated grad_norm_threshold: 1.1847 (MA grad_norm: 1.1766)\nParameters with gradients: 201, Global norm: 0.2265\nUpdated grad_norm_threshold: 1.1751 (MA grad_norm: 1.1741)\nParameters with gradients: 201, Global norm: 0.3385\nUpdated grad_norm_threshold: 1.0273 (MA grad_norm: 1.0108)\nParameters with gradients: 201, Global norm: 3.0342\nUpdated grad_norm_threshold: 1.1117 (MA grad_norm: 1.1210)\nParameters with gradients: 201, Global norm: 0.6641\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6656)\nParameters with gradients: 201, Global norm: 0.5759\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6690)\nParameters with gradients: 201, Global norm: 0.4145\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6769)\nParameters with gradients: 201, Global norm: 2.7356\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8005)\nParameters with gradients: 201, Global norm: 4.6380\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8969)\nParameters with gradients: 201, Global norm: 0.3629\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8926)\nParameters with gradients: 201, Global norm: 0.3372\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8773)\nParameters with gradients: 201, Global norm: 27.9710\nUpdated grad_norm_threshold: 2.1368 (MA grad_norm: 2.2631)\nParameters with gradients: 201, Global norm: 0.9030\nUpdated grad_norm_threshold: 2.2751 (MA grad_norm: 2.2905)\nParameters with gradients: 201, Global norm: 0.6154\nUpdated grad_norm_threshold: 2.2890 (MA grad_norm: 2.2905)\nParameters with gradients: 201, Global norm: 4.9729\nUpdated grad_norm_threshold: 2.4962 (MA grad_norm: 2.5192)\nParameters with gradients: 201, Global norm: 0.2967\nUpdated grad_norm_threshold: 2.5194 (MA grad_norm: 2.5220)\nParameters with gradients: 201, Global norm: 0.3043\nUpdated grad_norm_threshold: 2.5172 (MA grad_norm: 2.5170)\nParameters with gradients: 201, Global norm: 0.4342\nUpdated grad_norm_threshold: 2.5221 (MA grad_norm: 2.5226)\nParameters with gradients: 201, Global norm: 0.3099\nUpdated grad_norm_threshold: 2.5133 (MA grad_norm: 2.5123)\nParameters with gradients: 201, Global norm: 0.5746\nUpdated grad_norm_threshold: 2.5268 (MA grad_norm: 2.5283)\nParameters with gradients: 201, Global norm: 1.1876\nUpdated grad_norm_threshold: 2.5430 (MA grad_norm: 2.5448)\nParameters with gradients: 201, Global norm: 0.2036\nUpdated grad_norm_threshold: 2.5436 (MA grad_norm: 2.5437)\nParameters with gradients: 201, Global norm: 0.2892\nUpdated grad_norm_threshold: 2.5415 (MA grad_norm: 2.5412)\nParameters with gradients: 201, Global norm: 1.5917\nUpdated grad_norm_threshold: 2.4763 (MA grad_norm: 2.4691)\nParameters with gradients: 201, Global norm: 10.1949\nUpdated grad_norm_threshold: 2.8987 (MA grad_norm: 2.9456)\nParameters with gradients: 201, Global norm: 0.5358\nUpdated grad_norm_threshold: 2.9391 (MA grad_norm: 2.9436)\nParameters with gradients: 201, Global norm: 9.5270\nUpdated grad_norm_threshold: 3.3533 (MA grad_norm: 3.3993)\nParameters with gradients: 201, Global norm: 1.0227\nUpdated grad_norm_threshold: 3.3176 (MA grad_norm: 3.3136)\nParameters with gradients: 201, Global norm: 1.4396\nUpdated grad_norm_threshold: 3.1701 (MA grad_norm: 3.1537)\nParameters with gradients: 201, Global norm: 4.4261\nUpdated grad_norm_threshold: 3.3382 (MA grad_norm: 3.3569)\nParameters with gradients: 201, Global norm: 0.2884\nUpdated grad_norm_threshold: 3.3528 (MA grad_norm: 3.3544)\nParameters with gradients: 201, Global norm: 0.2337\nUpdated grad_norm_threshold: 2.1061 (MA grad_norm: 1.9676)\nParameters with gradients: 201, Global norm: 3.5901\nUpdated grad_norm_threshold: 2.1023 (MA grad_norm: 2.1019)\nParameters with gradients: 201, Global norm: 7.6429\nUpdated grad_norm_threshold: 2.4182 (MA grad_norm: 2.4533)\nParameters with gradients: 201, Global norm: 13.7253\nUpdated grad_norm_threshold: 2.8436 (MA grad_norm: 2.8909)\nParameters with gradients: 201, Global norm: 3.9965\nUpdated grad_norm_threshold: 3.0527 (MA grad_norm: 3.0759)\nParameters with gradients: 201, Global norm: 6.7177\nUpdated grad_norm_threshold: 3.3622 (MA grad_norm: 3.3966)\nParameters with gradients: 201, Global norm: 0.2987\nUpdated grad_norm_threshold: 3.3870 (MA grad_norm: 3.3898)\nParameters with gradients: 201, Global norm: 0.3115\nUpdated grad_norm_threshold: 3.3896 (MA grad_norm: 3.3899)\nParameters with gradients: 201, Global norm: 0.6404\nUpdated grad_norm_threshold: 3.3928 (MA grad_norm: 3.3932)\nParameters with gradients: 201, Global norm: 0.8854\nUpdated grad_norm_threshold: 3.3795 (MA grad_norm: 3.3781)\nParameters with gradients: 201, Global norm: 0.1776\nUpdated grad_norm_threshold: 3.3770 (MA grad_norm: 3.3768)\nParameters with gradients: 201, Global norm: 0.4028\nUpdated grad_norm_threshold: 3.3819 (MA grad_norm: 3.3824)\nParameters with gradients: 201, Global norm: 8.9955\nUpdated grad_norm_threshold: 3.7155 (MA grad_norm: 3.7526)\nParameters with gradients: 201, Global norm: 0.2805\nUpdated grad_norm_threshold: 3.3028 (MA grad_norm: 3.2569)\nParameters with gradients: 201, Global norm: 0.2178\nUpdated grad_norm_threshold: 3.2472 (MA grad_norm: 3.2410)\nParameters with gradients: 201, Global norm: 2.0170\nUpdated grad_norm_threshold: 2.9037 (MA grad_norm: 2.8655)\nParameters with gradients: 201, Global norm: 0.8301\nUpdated grad_norm_threshold: 2.8607 (MA grad_norm: 2.8559)\nParameters with gradients: 201, Global norm: 0.2159\nUpdated grad_norm_threshold: 2.8013 (MA grad_norm: 2.7947)\nParameters with gradients: 201, Global norm: 10.3523\nUpdated grad_norm_threshold: 3.0620 (MA grad_norm: 3.0910)\nParameters with gradients: 201, Global norm: 0.7614\nUpdated grad_norm_threshold: 3.1094 (MA grad_norm: 3.1147)\nParameters with gradients: 201, Global norm: 3.8242\nUpdated grad_norm_threshold: 3.2757 (MA grad_norm: 3.2942)\nParameters with gradients: 201, Global norm: 0.6204\nUpdated grad_norm_threshold: 3.1587 (MA grad_norm: 3.1457)\nParameters with gradients: 201, Global norm: 0.5302\nUpdated grad_norm_threshold: 2.8269 (MA grad_norm: 2.7901)\nParameters with gradients: 201, Global norm: 0.2773\nUpdated grad_norm_threshold: 2.1886 (MA grad_norm: 2.1177)\nParameters with gradients: 201, Global norm: 0.9713\nUpdated grad_norm_threshold: 1.9886 (MA grad_norm: 1.9664)\nParameters with gradients: 201, Global norm: 0.2316\nUpdated grad_norm_threshold: 1.6767 (MA grad_norm: 1.6421)\nParameters with gradients: 201, Global norm: 0.4758\nUpdated grad_norm_threshold: 1.6535 (MA grad_norm: 1.6509)\nParameters with gradients: 201, Global norm: 0.1972\nUpdated grad_norm_threshold: 1.6461 (MA grad_norm: 1.6452)\nParameters with gradients: 201, Global norm: 0.6769\nUpdated grad_norm_threshold: 1.6470 (MA grad_norm: 1.6471)\nParameters with gradients: 201, Global norm: 1.9345\nUpdated grad_norm_threshold: 1.6943 (MA grad_norm: 1.6995)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:14:21] Energy consumed for RAM : 0.000833 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:14:21] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:14:21] Energy consumed for All CPU : 0.001770 kWh\n[codecarbon INFO @ 08:14:21] Energy consumed for all GPUs : 0.003169 kWh. Total GPU Power : 75.90531241030327 W\n[codecarbon INFO @ 08:14:21] 0.005772 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.9063\nUpdated grad_norm_threshold: 1.7318 (MA grad_norm: 1.7359)\nParameters with gradients: 201, Global norm: 0.3887\nUpdated grad_norm_threshold: 1.7349 (MA grad_norm: 1.7352)\nParameters with gradients: 201, Global norm: 1.4414\nUpdated grad_norm_threshold: 1.3953 (MA grad_norm: 1.3575)\nParameters with gradients: 201, Global norm: 0.2973\nUpdated grad_norm_threshold: 1.3621 (MA grad_norm: 1.3584)\nParameters with gradients: 201, Global norm: 0.3943\nUpdated grad_norm_threshold: 1.3667 (MA grad_norm: 1.3672)\nParameters with gradients: 201, Global norm: 0.2409\nUpdated grad_norm_threshold: 1.2872 (MA grad_norm: 1.2784)\nParameters with gradients: 201, Global norm: 0.4861\nUpdated grad_norm_threshold: 1.2638 (MA grad_norm: 1.2612)\nParameters with gradients: 201, Global norm: 0.2245\nUpdated grad_norm_threshold: 1.2618 (MA grad_norm: 1.2616)\nParameters with gradients: 201, Global norm: 0.4042\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7642)\nParameters with gradients: 201, Global norm: 0.1951\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7359)\nParameters with gradients: 201, Global norm: 0.2523\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5573)\nParameters with gradients: 201, Global norm: 19.2677\nUpdated grad_norm_threshold: 1.4407 (MA grad_norm: 1.4897)\nParameters with gradients: 201, Global norm: 0.2295\nUpdated grad_norm_threshold: 1.4712 (MA grad_norm: 1.4746)\nAverage loss: 0.0351\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.8578, F1: 0.8566\n\nEpoch 5/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 5:   0%|          | 0/230 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.4715\nUpdated grad_norm_threshold: 1.4830 (MA grad_norm: 1.4844)\nParameters with gradients: 201, Global norm: 0.2793\nUpdated grad_norm_threshold: 1.4531 (MA grad_norm: 1.4497)\nParameters with gradients: 201, Global norm: 0.5188\nUpdated grad_norm_threshold: 1.4630 (MA grad_norm: 1.4641)\nParameters with gradients: 201, Global norm: 0.2908\nUpdated grad_norm_threshold: 1.4557 (MA grad_norm: 1.4549)\nParameters with gradients: 201, Global norm: 0.2230\nUpdated grad_norm_threshold: 1.4561 (MA grad_norm: 1.4561)\nParameters with gradients: 201, Global norm: 0.2411\nUpdated grad_norm_threshold: 1.4365 (MA grad_norm: 1.4344)\nParameters with gradients: 201, Global norm: 0.3010\nUpdated grad_norm_threshold: 1.3611 (MA grad_norm: 1.3527)\nParameters with gradients: 201, Global norm: 1.0471\nUpdated grad_norm_threshold: 1.3599 (MA grad_norm: 1.3597)\nParameters with gradients: 201, Global norm: 0.4139\nUpdated grad_norm_threshold: 1.3609 (MA grad_norm: 1.3610)\nParameters with gradients: 201, Global norm: 0.5114\nUpdated grad_norm_threshold: 1.3191 (MA grad_norm: 1.3145)\nParameters with gradients: 201, Global norm: 1.2526\nUpdated grad_norm_threshold: 1.3579 (MA grad_norm: 1.3622)\nParameters with gradients: 201, Global norm: 0.2863\nUpdated grad_norm_threshold: 1.3570 (MA grad_norm: 1.3569)\nParameters with gradients: 201, Global norm: 2.6383\nUpdated grad_norm_threshold: 1.4647 (MA grad_norm: 1.4767)\nParameters with gradients: 201, Global norm: 0.3152\nUpdated grad_norm_threshold: 1.4678 (MA grad_norm: 1.4682)\nParameters with gradients: 201, Global norm: 0.1841\nUpdated grad_norm_threshold: 1.4663 (MA grad_norm: 1.4662)\nParameters with gradients: 201, Global norm: 0.1523\nUpdated grad_norm_threshold: 1.4548 (MA grad_norm: 1.4536)\nParameters with gradients: 201, Global norm: 0.2190\nUpdated grad_norm_threshold: 1.4548 (MA grad_norm: 1.4548)\nParameters with gradients: 201, Global norm: 0.2953\nUpdated grad_norm_threshold: 1.4567 (MA grad_norm: 1.4569)\nParameters with gradients: 201, Global norm: 0.3364\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5103)\nParameters with gradients: 201, Global norm: 0.2063\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5092)\nParameters with gradients: 201, Global norm: 0.2332\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4973)\nParameters with gradients: 201, Global norm: 0.2396\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4953)\nParameters with gradients: 201, Global norm: 0.2441\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4815)\nParameters with gradients: 201, Global norm: 0.1938\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4767)\nParameters with gradients: 201, Global norm: 5.3450\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7328)\nParameters with gradients: 201, Global norm: 0.2213\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7318)\nParameters with gradients: 201, Global norm: 0.2111\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7273)\nParameters with gradients: 201, Global norm: 0.3467\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6923)\nParameters with gradients: 201, Global norm: 0.1485\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6790)\nParameters with gradients: 201, Global norm: 0.7974\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6933)\nParameters with gradients: 201, Global norm: 0.2443\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6429)\nParameters with gradients: 201, Global norm: 0.4699\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6521)\nParameters with gradients: 201, Global norm: 0.2594\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5331)\nParameters with gradients: 201, Global norm: 0.2525\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5300)\nParameters with gradients: 201, Global norm: 3.0259\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6721)\nParameters with gradients: 201, Global norm: 0.2253\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6757)\nParameters with gradients: 201, Global norm: 0.3004\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6798)\nParameters with gradients: 201, Global norm: 0.2308\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6766)\nParameters with gradients: 201, Global norm: 0.3233\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6759)\nParameters with gradients: 201, Global norm: 0.3654\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6839)\nParameters with gradients: 201, Global norm: 0.4401\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6942)\nParameters with gradients: 201, Global norm: 3.6501\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8648)\nParameters with gradients: 201, Global norm: 0.2291\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8640)\nParameters with gradients: 201, Global norm: 0.3372\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8712)\nParameters with gradients: 201, Global norm: 0.1993\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6139)\nParameters with gradients: 201, Global norm: 0.1686\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6113)\nParameters with gradients: 201, Global norm: 0.1543\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6084)\nParameters with gradients: 201, Global norm: 0.2484\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6035)\nParameters with gradients: 201, Global norm: 0.4088\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6165)\nParameters with gradients: 201, Global norm: 0.1987\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5866)\nParameters with gradients: 201, Global norm: 0.1928\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5840)\nParameters with gradients: 201, Global norm: 0.2016\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5706)\nParameters with gradients: 201, Global norm: 0.2088\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5681)\nParameters with gradients: 201, Global norm: 0.2607\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5685)\nParameters with gradients: 201, Global norm: 9.0121\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8678)\nParameters with gradients: 201, Global norm: 1.0716\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9101)\nParameters with gradients: 201, Global norm: 0.1657\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9034)\nParameters with gradients: 201, Global norm: 0.1758\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9006)\nParameters with gradients: 201, Global norm: 1.1322\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9411)\nParameters with gradients: 201, Global norm: 0.2537\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9355)\nParameters with gradients: 201, Global norm: 0.2312\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9250)\nParameters with gradients: 201, Global norm: 0.2773\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7564)\nParameters with gradients: 201, Global norm: 0.1700\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7534)\nParameters with gradients: 201, Global norm: 0.2785\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7505)\nParameters with gradients: 201, Global norm: 0.1460\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7478)\nParameters with gradients: 201, Global norm: 0.4792\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7634)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:14:36] Energy consumed for RAM : 0.000916 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:14:36] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:14:36] Energy consumed for All CPU : 0.001947 kWh\n[codecarbon INFO @ 08:14:36] Energy consumed for all GPUs : 0.003487 kWh. Total GPU Power : 76.40977996087764 W\n[codecarbon INFO @ 08:14:36] 0.006350 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.1727\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7643)\nParameters with gradients: 201, Global norm: 0.2060\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7622)\nParameters with gradients: 201, Global norm: 0.4376\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7636)\nParameters with gradients: 201, Global norm: 0.3137\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7694)\nParameters with gradients: 201, Global norm: 0.2037\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7699)\nParameters with gradients: 201, Global norm: 0.1493\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7673)\nParameters with gradients: 201, Global norm: 0.2554\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.7696)\nParameters with gradients: 201, Global norm: 2.6415\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8887)\nParameters with gradients: 201, Global norm: 0.2393\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4500)\nParameters with gradients: 201, Global norm: 0.1640\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4046)\nParameters with gradients: 201, Global norm: 1.5400\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4734)\nParameters with gradients: 201, Global norm: 0.3067\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4799)\nParameters with gradients: 201, Global norm: 0.5402\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4503)\nParameters with gradients: 201, Global norm: 0.1876\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4470)\n‚ö° Epoch 5 | Layer 0 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 1 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 2 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 3 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 4 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 5 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 6 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 7 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 8 switched to 8-bit (MA grad_norm=0.4470)\n‚ö° Epoch 5 | Layer 9 switched to 8-bit (MA grad_norm=0.4470)\nParameters with gradients: 201, Global norm: 0.2757\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4492)\nParameters with gradients: 201, Global norm: 0.9417\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4824)\nParameters with gradients: 201, Global norm: 0.2438\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4861)\nParameters with gradients: 201, Global norm: 0.2395\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4842)\nParameters with gradients: 201, Global norm: 0.2667\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4902)\nParameters with gradients: 201, Global norm: 0.1430\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4734)\nParameters with gradients: 201, Global norm: 0.1817\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4739)\nParameters with gradients: 201, Global norm: 0.9263\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5099)\nParameters with gradients: 201, Global norm: 0.1916\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4976)\nParameters with gradients: 201, Global norm: 0.1984\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4918)\nParameters with gradients: 201, Global norm: 0.1601\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4896)\nParameters with gradients: 201, Global norm: 0.3190\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4981)\nParameters with gradients: 201, Global norm: 0.1568\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4932)\nParameters with gradients: 201, Global norm: 3.0886\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5156)\nParameters with gradients: 201, Global norm: 0.2370\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5154)\nParameters with gradients: 201, Global norm: 0.1167\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5131)\nParameters with gradients: 201, Global norm: 0.3705\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4546)\nParameters with gradients: 201, Global norm: 0.1419\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4464)\nParameters with gradients: 201, Global norm: 0.2187\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4303)\nParameters with gradients: 201, Global norm: 0.1330\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4275)\nParameters with gradients: 201, Global norm: 0.2038\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4239)\nParameters with gradients: 201, Global norm: 0.2549\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3896)\nParameters with gradients: 201, Global norm: 0.8464\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4197)\nParameters with gradients: 201, Global norm: 0.3008\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4228)\nParameters with gradients: 201, Global norm: 0.5964\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4393)\nParameters with gradients: 201, Global norm: 0.2143\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4428)\nParameters with gradients: 201, Global norm: 0.1812\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4428)\nParameters with gradients: 201, Global norm: 0.1593\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4045)\nParameters with gradients: 201, Global norm: 0.4732\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4186)\nParameters with gradients: 201, Global norm: 0.2822\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4227)\nParameters with gradients: 201, Global norm: 0.4792\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4387)\nParameters with gradients: 201, Global norm: 0.1856\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4320)\nParameters with gradients: 201, Global norm: 0.1603\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4322)\nParameters with gradients: 201, Global norm: 0.5598\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3058)\nParameters with gradients: 201, Global norm: 0.2031\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3041)\nParameters with gradients: 201, Global norm: 0.4044\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3185)\nParameters with gradients: 201, Global norm: 11.6142\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8806)\nParameters with gradients: 201, Global norm: 0.7641\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9118)\nParameters with gradients: 201, Global norm: 0.1451\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9081)\nParameters with gradients: 201, Global norm: 0.2482\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9138)\nParameters with gradients: 201, Global norm: 0.2561\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9165)\nParameters with gradients: 201, Global norm: 0.1470\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9111)\nParameters with gradients: 201, Global norm: 0.5409\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8958)\nParameters with gradients: 201, Global norm: 0.1359\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.8875)\nParameters with gradients: 201, Global norm: 5.6290\nUpdated grad_norm_threshold: 1.1253 (MA grad_norm: 1.1392)\nParameters with gradients: 201, Global norm: 0.1810\nUpdated grad_norm_threshold: 1.1363 (MA grad_norm: 1.1375)\nParameters with gradients: 201, Global norm: 0.2229\nUpdated grad_norm_threshold: 1.1393 (MA grad_norm: 1.1396)\nParameters with gradients: 201, Global norm: 0.4301\nUpdated grad_norm_threshold: 1.1517 (MA grad_norm: 1.1531)\nParameters with gradients: 201, Global norm: 1.7268\nUpdated grad_norm_threshold: 1.2094 (MA grad_norm: 1.2158)\nParameters with gradients: 201, Global norm: 0.2152\nUpdated grad_norm_threshold: 1.2122 (MA grad_norm: 1.2125)\nParameters with gradients: 201, Global norm: 0.1998\nUpdated grad_norm_threshold: 1.1999 (MA grad_norm: 1.1985)\nParameters with gradients: 201, Global norm: 0.2775\nUpdated grad_norm_threshold: 1.2028 (MA grad_norm: 1.2031)\nParameters with gradients: 201, Global norm: 0.2156\nUpdated grad_norm_threshold: 1.2055 (MA grad_norm: 1.2058)\nParameters with gradients: 201, Global norm: 0.3396\nUpdated grad_norm_threshold: 1.1959 (MA grad_norm: 1.1948)\nParameters with gradients: 201, Global norm: 0.2511\nUpdated grad_norm_threshold: 1.1971 (MA grad_norm: 1.1972)\nParameters with gradients: 201, Global norm: 0.1733\nUpdated grad_norm_threshold: 1.1868 (MA grad_norm: 1.1857)\nParameters with gradients: 201, Global norm: 0.1873\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6143)\nParameters with gradients: 201, Global norm: 0.1294\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5826)\nParameters with gradients: 201, Global norm: 0.2779\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5892)\nParameters with gradients: 201, Global norm: 0.4788\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6008)\nParameters with gradients: 201, Global norm: 0.6102\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6185)\nParameters with gradients: 201, Global norm: 0.1315\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6177)\nParameters with gradients: 201, Global norm: 0.4399\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6126)\nParameters with gradients: 201, Global norm: 0.1267\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6122)\nParameters with gradients: 201, Global norm: 0.2022\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3408)\nParameters with gradients: 201, Global norm: 0.2140\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3425)\nParameters with gradients: 201, Global norm: 0.1127\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3370)\nParameters with gradients: 201, Global norm: 3.9204\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5115)\nParameters with gradients: 201, Global norm: 0.1502\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4327)\nParameters with gradients: 201, Global norm: 0.1508\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4294)\nParameters with gradients: 201, Global norm: 0.2621\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4326)\nParameters with gradients: 201, Global norm: 12.4982\nUpdated grad_norm_threshold: 1.0392 (MA grad_norm: 1.0436)\nParameters with gradients: 201, Global norm: 0.1426\nUpdated grad_norm_threshold: 1.0399 (MA grad_norm: 1.0399)\nParameters with gradients: 201, Global norm: 0.4428\nUpdated grad_norm_threshold: 1.0446 (MA grad_norm: 1.0451)\nParameters with gradients: 201, Global norm: 0.1997\nUpdated grad_norm_threshold: 1.0427 (MA grad_norm: 1.0425)\nParameters with gradients: 201, Global norm: 0.2220\nUpdated grad_norm_threshold: 1.0447 (MA grad_norm: 1.0450)\nParameters with gradients: 201, Global norm: 0.2510\nUpdated grad_norm_threshold: 1.0478 (MA grad_norm: 1.0482)\nParameters with gradients: 201, Global norm: 0.3111\nUpdated grad_norm_threshold: 1.0563 (MA grad_norm: 1.0572)\nParameters with gradients: 201, Global norm: 0.1785\nUpdated grad_norm_threshold: 1.0527 (MA grad_norm: 1.0523)\nParameters with gradients: 201, Global norm: 0.1583\nUpdated grad_norm_threshold: 1.0379 (MA grad_norm: 1.0362)\nParameters with gradients: 201, Global norm: 0.1615\nUpdated grad_norm_threshold: 1.0162 (MA grad_norm: 1.0138)\nParameters with gradients: 201, Global norm: 0.1512\nUpdated grad_norm_threshold: 1.0149 (MA grad_norm: 1.0148)\nParameters with gradients: 201, Global norm: 0.1429\nUpdated grad_norm_threshold: 1.0014 (MA grad_norm: 0.9999)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:14:51] Energy consumed for RAM : 0.000999 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:14:51] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:14:51] Energy consumed for All CPU : 0.002124 kWh\n[codecarbon INFO @ 08:14:51] Energy consumed for all GPUs : 0.003803 kWh. Total GPU Power : 75.86591482229008 W\n[codecarbon INFO @ 08:14:51] 0.006927 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 0.1843\nUpdated grad_norm_threshold: 1.0027 (MA grad_norm: 1.0028)\nParameters with gradients: 201, Global norm: 0.5886\nUpdated grad_norm_threshold: 1.0202 (MA grad_norm: 1.0221)\nParameters with gradients: 201, Global norm: 0.2219\nUpdated grad_norm_threshold: 1.0223 (MA grad_norm: 1.0225)\nParameters with gradients: 201, Global norm: 0.2293\nUpdated grad_norm_threshold: 1.0278 (MA grad_norm: 1.0284)\nParameters with gradients: 201, Global norm: 1.5059\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9077)\nParameters with gradients: 201, Global norm: 0.3956\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9199)\nParameters with gradients: 201, Global norm: 0.2298\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9239)\nParameters with gradients: 201, Global norm: 0.2982\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9257)\nParameters with gradients: 201, Global norm: 0.1921\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3104)\nParameters with gradients: 201, Global norm: 0.1839\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.3124)\nParameters with gradients: 201, Global norm: 0.1797\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2993)\n‚ö° Epoch 5 | Layer 0 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 1 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 2 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 3 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 4 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 5 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 6 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 7 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 8 switched to 4-bit (MA grad_norm=0.2993)\n‚ö° Epoch 5 | Layer 9 switched to 4-bit (MA grad_norm=0.2993)\nParameters with gradients: 201, Global norm: 0.1846\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2985)\nParameters with gradients: 201, Global norm: 4.3994\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5074)\nParameters with gradients: 201, Global norm: 0.1467\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5022)\nParameters with gradients: 201, Global norm: 0.1185\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4926)\nParameters with gradients: 201, Global norm: 0.1442\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4908)\nParameters with gradients: 201, Global norm: 0.1326\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4895)\nParameters with gradients: 201, Global norm: 0.2121\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4921)\nParameters with gradients: 201, Global norm: 0.3978\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5044)\nParameters with gradients: 201, Global norm: 0.1724\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5059)\nParameters with gradients: 201, Global norm: 0.2597\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5097)\nParameters with gradients: 201, Global norm: 0.2792\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4942)\nParameters with gradients: 201, Global norm: 0.2517\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4957)\nParameters with gradients: 201, Global norm: 0.2733\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4979)\nParameters with gradients: 201, Global norm: 0.1567\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4304)\nParameters with gradients: 201, Global norm: 0.3284\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4270)\nParameters with gradients: 201, Global norm: 0.2592\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4285)\nParameters with gradients: 201, Global norm: 0.1249\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4199)\nParameters with gradients: 201, Global norm: 0.1110\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4158)\nParameters with gradients: 201, Global norm: 0.1331\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4133)\nParameters with gradients: 201, Global norm: 0.2103\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4148)\nParameters with gradients: 201, Global norm: 0.4248\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.4268)\nParameters with gradients: 201, Global norm: 0.2490\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2193)\nParameters with gradients: 201, Global norm: 0.2514\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2245)\nParameters with gradients: 201, Global norm: 0.3544\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2363)\nParameters with gradients: 201, Global norm: 0.2675\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2425)\nParameters with gradients: 201, Global norm: 0.3278\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2522)\nParameters with gradients: 201, Global norm: 0.2904\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2561)\nParameters with gradients: 201, Global norm: 0.3223\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2524)\nParameters with gradients: 201, Global norm: 0.2661\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2571)\nParameters with gradients: 201, Global norm: 0.1350\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2508)\nParameters with gradients: 201, Global norm: 0.3014\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2519)\nParameters with gradients: 201, Global norm: 0.2472\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2517)\nParameters with gradients: 201, Global norm: 0.2769\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2519)\nParameters with gradients: 201, Global norm: 0.1300\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2506)\nParameters with gradients: 201, Global norm: 0.2679\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2475)\nParameters with gradients: 201, Global norm: 0.1967\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2444)\nParameters with gradients: 201, Global norm: 0.1079\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2436)\nParameters with gradients: 201, Global norm: 0.1311\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2446)\nParameters with gradients: 201, Global norm: 0.2325\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.2495)\nParameters with gradients: 201, Global norm: 7.3921\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.6086)\nParameters with gradients: 201, Global norm: 0.2234\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5986)\nParameters with gradients: 201, Global norm: 0.2622\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5992)\nParameters with gradients: 201, Global norm: 0.1519\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5942)\nParameters with gradients: 201, Global norm: 0.2080\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5869)\nParameters with gradients: 201, Global norm: 0.1903\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5831)\nParameters with gradients: 201, Global norm: 0.1158\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5725)\nParameters with gradients: 201, Global norm: 0.7791\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5969)\nParameters with gradients: 201, Global norm: 0.3250\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5970)\nParameters with gradients: 201, Global norm: 0.1249\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.5900)\nParameters with gradients: 201, Global norm: 8.5533\nUpdated grad_norm_threshold: 1.0098 (MA grad_norm: 1.0109)\nParameters with gradients: 201, Global norm: 0.2401\nUpdated grad_norm_threshold: 1.0080 (MA grad_norm: 1.0078)\nParameters with gradients: 201, Global norm: 0.1030\nUpdated grad_norm_threshold: 1.0013 (MA grad_norm: 1.0006)\nParameters with gradients: 201, Global norm: 0.1516\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9943)\nParameters with gradients: 201, Global norm: 0.1281\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9942)\nParameters with gradients: 201, Global norm: 0.1381\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9878)\nParameters with gradients: 201, Global norm: 0.1684\nUpdated grad_norm_threshold: 1.0000 (MA grad_norm: 0.9863)\nAverage loss: 0.0174\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.8407, F1: 0.8379\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:15:06] Energy consumed for RAM : 0.001083 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:15:06] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:15:06] Energy consumed for All CPU : 0.002301 kWh\n[codecarbon INFO @ 08:15:06] Energy consumed for all GPUs : 0.004122 kWh. Total GPU Power : 76.55221450095075 W\n[codecarbon INFO @ 08:15:06] 0.007506 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:15:07] Energy consumed for RAM : 0.001088 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:15:07] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 08:15:07] Energy consumed for All CPU : 0.002313 kWh\n[codecarbon INFO @ 08:15:07] Energy consumed for all GPUs : 0.004146 kWh. Total GPU Power : 83.51249083345611 W\n[codecarbon INFO @ 08:15:07] 0.007547 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Results ---\n{\n  \"model_name\": \"Adaptive_Quantized_BERT_MRPC\",\n  \"dataset\": \"MRPC\",\n  \"accuracy\": 0.8406862745098039,\n  \"f1\": 0.8378544424039412,\n  \"scheduler_metrics\": {\n    \"adaptation_log\": [\n      {\n        \"layer_id\": 0,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 1,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 2,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 3,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 4,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 5,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 6,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 7,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 8,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 9,\n        \"new_precision\": 8,\n        \"grad_norm\": 0.44700065158013247,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 0,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 1,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 2,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 3,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 4,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 5,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 6,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 7,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 8,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      },\n      {\n        \"layer_id\": 9,\n        \"new_precision\": 4,\n        \"grad_norm\": 0.2992834978588771,\n        \"epoch\": 5,\n        \"type\": \"decrease\"\n      }\n    ],\n    \"precision_switch_counts_int4\": 10,\n    \"precision_switch_counts_int8\": 10,\n    \"precision_switch_counts_fp32\": 0,\n    \"avg_precision_level\": 4.0,\n    \"precision_distribution\": {\n      \"INT4\": 1.0,\n      \"INT8\": 0.0,\n      \"FP32\": 0.0\n    }\n  },\n  \"performance_metrics\": {\n    \"total_duration_s\": 197.28146743774414,\n    \"total_emissions_kwh\": 0.0027885038330605367,\n    \"latency_ms_query\": 483.53300842584343,\n    \"throughput_tokens_sec\": 264.71822558031334,\n    \"energy_wh_token\": 5.339506420535647e-05,\n    \"sci_gco2e_query\": 0.001708642054571407,\n    \"wue_avg_liters_query\": 1.2302222792914133e-05\n  }\n}\n\n==================== Running Experiment: Adaptive_Quantized_BERT_RTE ====================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00bb3f62ac284a29b343515063a93641"}},"metadata":{}},{"name":"stdout","text":"\nüèóÔ∏è Building Adaptive Quantized Bert Model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[codecarbon INFO @ 08:15:16] offline tracker init\n[codecarbon WARNING @ 08:15:16] Multiple instances of codecarbon are allowed to run at the same time.\n[codecarbon INFO @ 08:15:16] [setup] RAM Tracking...\n[codecarbon INFO @ 08:15:16] [setup] CPU Tracking...\n","output_type":"stream"},{"name":"stdout","text":"‚úì Successfully created quantized Bert model.\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon WARNING @ 08:15:17] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon WARNING @ 08:15:17] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n\n[codecarbon INFO @ 08:15:17] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon WARNING @ 08:15:17] No CPU tracking mode found. Falling back on CPU constant mode.\n[codecarbon INFO @ 08:15:17] [setup] GPU Tracking...\n[codecarbon INFO @ 08:15:17] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 08:15:17] The below tracking methods have been set up:\n                RAM Tracking Method: RAM power estimation model\n                CPU Tracking Method: global constant\n                GPU Tracking Method: pynvml\n            \n[codecarbon INFO @ 08:15:17] >>> Tracker's metadata:\n[codecarbon INFO @ 08:15:17]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 08:15:17]   Python version: 3.11.13\n[codecarbon INFO @ 08:15:17]   CodeCarbon version: 3.0.5\n[codecarbon INFO @ 08:15:17]   Available RAM : 31.350 GB\n[codecarbon INFO @ 08:15:17]   CPU count: 4 thread(s) in 1 physical CPU(s)\n[codecarbon INFO @ 08:15:17]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 08:15:17]   GPU count: 2\n[codecarbon INFO @ 08:15:17]   GPU model: 2 x Tesla T4\n[codecarbon INFO @ 08:15:17] Emissions data (if any) will be saved to file /kaggle/working/emissions.csv\n","output_type":"stream"},{"name":"stdout","text":"‚úì Robust PrecisionScheduler initialized for 10 layers (Window: 20, Cooldown: 20, Warmup: 500).\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 1:   0%|          | 0/156 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 8.4974\nParameters with gradients: 201, Global norm: 5.3883\nParameters with gradients: 201, Global norm: 9.0721\nParameters with gradients: 201, Global norm: 5.3176\nParameters with gradients: 201, Global norm: 2.1266\nParameters with gradients: 201, Global norm: 10.8745\nParameters with gradients: 201, Global norm: 5.7182\nParameters with gradients: 201, Global norm: 9.2663\nParameters with gradients: 201, Global norm: 10.9128\nParameters with gradients: 201, Global norm: 4.2657\nParameters with gradients: 201, Global norm: 2.2763\nParameters with gradients: 201, Global norm: 4.5268\nParameters with gradients: 201, Global norm: 6.4471\nParameters with gradients: 201, Global norm: 7.0684\nParameters with gradients: 201, Global norm: 5.5372\nParameters with gradients: 201, Global norm: 4.9472\nParameters with gradients: 201, Global norm: 5.9909\nParameters with gradients: 201, Global norm: 6.7651\nParameters with gradients: 201, Global norm: 5.1708\nParameters with gradients: 201, Global norm: 5.7861\nWarning: Non-finite gradient norm in model.bert.encoder.layer.8.layer.intermediate.dense.weight: inf\nWarning: Non-finite gradient norm in model.bert.encoder.layer.10.layer.intermediate.dense.weight: inf\nParameters with gradients: 199, Global norm: 13.0944\nParameters with gradients: 201, Global norm: 10.2634\nParameters with gradients: 201, Global norm: 6.5866\nParameters with gradients: 201, Global norm: 5.2425\nParameters with gradients: 201, Global norm: 3.9658\nParameters with gradients: 201, Global norm: 15.6470\nParameters with gradients: 201, Global norm: 9.3307\nParameters with gradients: 201, Global norm: 6.2170\nParameters with gradients: 201, Global norm: 5.5527\nParameters with gradients: 201, Global norm: 5.7192\nParameters with gradients: 201, Global norm: 5.6313\nParameters with gradients: 201, Global norm: 6.2164\nParameters with gradients: 201, Global norm: 4.2693\nParameters with gradients: 201, Global norm: 4.7492\nParameters with gradients: 201, Global norm: 4.7071\nParameters with gradients: 201, Global norm: 7.9401\nParameters with gradients: 201, Global norm: 4.9890\nParameters with gradients: 201, Global norm: 6.8164\nParameters with gradients: 201, Global norm: 7.7784\nParameters with gradients: 201, Global norm: 6.2109\nParameters with gradients: 201, Global norm: 11.2638\nWarning: Non-finite gradient norm in model.bert.encoder.layer.3.layer.intermediate.dense.weight: inf\nWarning: Non-finite gradient norm in model.bert.encoder.layer.10.layer.intermediate.dense.weight: inf\nParameters with gradients: 199, Global norm: 17.1012\nParameters with gradients: 201, Global norm: 8.8649\nParameters with gradients: 201, Global norm: 10.4291\nParameters with gradients: 201, Global norm: 5.0545\nParameters with gradients: 201, Global norm: 6.3871\nParameters with gradients: 201, Global norm: 5.4464\nParameters with gradients: 201, Global norm: 5.2430\nParameters with gradients: 201, Global norm: 8.2907\nParameters with gradients: 201, Global norm: 5.0223\nParameters with gradients: 201, Global norm: 9.2677\nParameters with gradients: 201, Global norm: 9.4651\nParameters with gradients: 201, Global norm: 8.5260\nParameters with gradients: 201, Global norm: 10.0078\nParameters with gradients: 201, Global norm: 8.0473\nParameters with gradients: 201, Global norm: 12.0612\nParameters with gradients: 201, Global norm: 8.5243\nParameters with gradients: 201, Global norm: 9.4228\nParameters with gradients: 201, Global norm: 5.0423\nParameters with gradients: 201, Global norm: 6.4055\nParameters with gradients: 201, Global norm: 9.5143\nParameters with gradients: 201, Global norm: 2.8600\nParameters with gradients: 201, Global norm: 2.5603\nParameters with gradients: 201, Global norm: 5.6858\nParameters with gradients: 201, Global norm: 8.5954\nParameters with gradients: 201, Global norm: 3.5716\nParameters with gradients: 201, Global norm: 6.5866\nParameters with gradients: 201, Global norm: 5.4771\nParameters with gradients: 201, Global norm: 8.3521\nParameters with gradients: 201, Global norm: 5.0724\nParameters with gradients: 201, Global norm: 4.6395\nParameters with gradients: 201, Global norm: 6.7104\nParameters with gradients: 201, Global norm: 2.2987\nParameters with gradients: 201, Global norm: 4.8097\nParameters with gradients: 201, Global norm: 2.2409\nParameters with gradients: 201, Global norm: 5.3386\nParameters with gradients: 201, Global norm: 3.7415\nParameters with gradients: 201, Global norm: 7.9269\nParameters with gradients: 201, Global norm: 4.8395\nParameters with gradients: 201, Global norm: 6.4914\nParameters with gradients: 201, Global norm: 4.6385\nParameters with gradients: 201, Global norm: 7.0636\nParameters with gradients: 201, Global norm: 5.5108\nParameters with gradients: 201, Global norm: 5.6537\nParameters with gradients: 201, Global norm: 5.5223\nParameters with gradients: 201, Global norm: 5.6578\nParameters with gradients: 201, Global norm: 5.0760\nParameters with gradients: 201, Global norm: 3.1051\nParameters with gradients: 201, Global norm: 4.6402\nParameters with gradients: 201, Global norm: 3.3377\nParameters with gradients: 201, Global norm: 3.6785\nParameters with gradients: 201, Global norm: 7.2768\nParameters with gradients: 201, Global norm: 3.9834\nParameters with gradients: 201, Global norm: 3.6473\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:15:32] Energy consumed for RAM : 0.000083 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:15:32] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:15:32] Energy consumed for All CPU : 0.000177 kWh\n[codecarbon INFO @ 08:15:32] Energy consumed for all GPUs : 0.000316 kWh. Total GPU Power : 75.70217197101358 W\n[codecarbon INFO @ 08:15:32] 0.000576 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.3896\nParameters with gradients: 201, Global norm: 5.7066\nParameters with gradients: 201, Global norm: 2.6768\nParameters with gradients: 201, Global norm: 2.8679\nParameters with gradients: 201, Global norm: 6.5261\nParameters with gradients: 201, Global norm: 4.0453\nParameters with gradients: 201, Global norm: 2.4181\nParameters with gradients: 201, Global norm: 6.7836\nParameters with gradients: 201, Global norm: 7.9055\nParameters with gradients: 201, Global norm: 3.7751\nParameters with gradients: 201, Global norm: 2.5084\nParameters with gradients: 201, Global norm: 3.6613\nParameters with gradients: 201, Global norm: 5.0582\nParameters with gradients: 201, Global norm: 2.6516\nParameters with gradients: 201, Global norm: 3.5557\nParameters with gradients: 201, Global norm: 4.4714\nParameters with gradients: 201, Global norm: 5.6504\nParameters with gradients: 201, Global norm: 4.2989\nParameters with gradients: 201, Global norm: 9.5188\nParameters with gradients: 201, Global norm: 4.5792\nParameters with gradients: 201, Global norm: 3.2123\nParameters with gradients: 201, Global norm: 2.5941\nParameters with gradients: 201, Global norm: 2.4483\nParameters with gradients: 201, Global norm: 7.2474\nParameters with gradients: 201, Global norm: 4.4535\nParameters with gradients: 201, Global norm: 2.3833\nParameters with gradients: 201, Global norm: 4.9108\nParameters with gradients: 201, Global norm: 3.5448\nParameters with gradients: 201, Global norm: 10.1977\nParameters with gradients: 201, Global norm: 8.3107\nParameters with gradients: 201, Global norm: 2.8745\nParameters with gradients: 201, Global norm: 2.3447\nParameters with gradients: 201, Global norm: 11.0388\nParameters with gradients: 201, Global norm: 5.9157\nParameters with gradients: 201, Global norm: 6.5145\nParameters with gradients: 201, Global norm: 4.7159\nParameters with gradients: 201, Global norm: 7.4570\nParameters with gradients: 201, Global norm: 7.6581\nParameters with gradients: 201, Global norm: 6.3580\nParameters with gradients: 201, Global norm: 3.0691\nParameters with gradients: 201, Global norm: 7.1711\nParameters with gradients: 201, Global norm: 5.6417\nParameters with gradients: 201, Global norm: 7.9070\nParameters with gradients: 201, Global norm: 15.2617\nParameters with gradients: 201, Global norm: 6.5738\nParameters with gradients: 201, Global norm: 2.4509\nParameters with gradients: 201, Global norm: 2.4925\nParameters with gradients: 201, Global norm: 4.6587\nParameters with gradients: 201, Global norm: 2.4257\nParameters with gradients: 201, Global norm: 2.7217\nParameters with gradients: 201, Global norm: 7.9325\nParameters with gradients: 201, Global norm: 4.9942\nParameters with gradients: 201, Global norm: 5.2007\nParameters with gradients: 201, Global norm: 3.8426\nParameters with gradients: 201, Global norm: 4.5904\nParameters with gradients: 201, Global norm: 7.7273\nParameters with gradients: 201, Global norm: 6.1142\nParameters with gradients: 201, Global norm: 3.0606\nParameters with gradients: 201, Global norm: 2.2984\nParameters with gradients: 201, Global norm: 2.7113\nParameters with gradients: 201, Global norm: 3.9461\nParameters with gradients: 201, Global norm: 3.1951\nAverage loss: 0.7104\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.4693, F1: 0.3654\n\nEpoch 2/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 2:   0%|          | 0/156 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 6.2220\nParameters with gradients: 201, Global norm: 1.8371\nParameters with gradients: 201, Global norm: 3.9886\nParameters with gradients: 201, Global norm: 2.0315\nParameters with gradients: 201, Global norm: 3.3427\nParameters with gradients: 201, Global norm: 9.8779\nParameters with gradients: 201, Global norm: 2.2403\nParameters with gradients: 201, Global norm: 1.8485\nParameters with gradients: 201, Global norm: 9.4092\nParameters with gradients: 201, Global norm: 9.6885\nParameters with gradients: 201, Global norm: 6.7957\nParameters with gradients: 201, Global norm: 2.4349\nParameters with gradients: 201, Global norm: 4.3041\nParameters with gradients: 201, Global norm: 3.9073\nParameters with gradients: 201, Global norm: 1.7390\nParameters with gradients: 201, Global norm: 1.9153\nParameters with gradients: 201, Global norm: 7.2239\nParameters with gradients: 201, Global norm: 4.3699\nParameters with gradients: 201, Global norm: 1.7100\nParameters with gradients: 201, Global norm: 2.3079\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:15:47] Energy consumed for RAM : 0.000167 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:15:47] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:15:47] Energy consumed for All CPU : 0.000354 kWh\n[codecarbon INFO @ 08:15:47] Energy consumed for all GPUs : 0.000633 kWh. Total GPU Power : 76.21176769199907 W\n[codecarbon INFO @ 08:15:47] 0.001154 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 6.1128\nParameters with gradients: 201, Global norm: 2.0085\nParameters with gradients: 201, Global norm: 6.0155\nParameters with gradients: 201, Global norm: 2.4200\nParameters with gradients: 201, Global norm: 2.5260\nParameters with gradients: 201, Global norm: 5.9780\nParameters with gradients: 201, Global norm: 2.1982\nParameters with gradients: 201, Global norm: 3.6389\nParameters with gradients: 201, Global norm: 1.8660\nParameters with gradients: 201, Global norm: 2.5361\nParameters with gradients: 201, Global norm: 3.3452\nParameters with gradients: 201, Global norm: 2.8388\nParameters with gradients: 201, Global norm: 3.1332\nParameters with gradients: 201, Global norm: 2.0318\nParameters with gradients: 201, Global norm: 5.8028\nParameters with gradients: 201, Global norm: 5.2630\nParameters with gradients: 201, Global norm: 5.5426\nParameters with gradients: 201, Global norm: 5.3102\nParameters with gradients: 201, Global norm: 2.5952\nParameters with gradients: 201, Global norm: 3.8751\nParameters with gradients: 201, Global norm: 3.9473\nParameters with gradients: 201, Global norm: 3.6415\nParameters with gradients: 201, Global norm: 8.3634\nParameters with gradients: 201, Global norm: 5.8230\nParameters with gradients: 201, Global norm: 3.3414\nParameters with gradients: 201, Global norm: 6.4539\nParameters with gradients: 201, Global norm: 18.5569\nParameters with gradients: 201, Global norm: 3.1777\nParameters with gradients: 201, Global norm: 4.9959\nParameters with gradients: 201, Global norm: 9.2006\nParameters with gradients: 201, Global norm: 4.5080\nParameters with gradients: 201, Global norm: 11.3583\nParameters with gradients: 201, Global norm: 4.0451\nParameters with gradients: 201, Global norm: 6.2255\nParameters with gradients: 201, Global norm: 3.2949\nParameters with gradients: 201, Global norm: 3.7838\nParameters with gradients: 201, Global norm: 2.7896\nParameters with gradients: 201, Global norm: 4.0969\nParameters with gradients: 201, Global norm: 8.0693\nParameters with gradients: 201, Global norm: 4.3378\nParameters with gradients: 201, Global norm: 3.9670\nParameters with gradients: 201, Global norm: 8.6542\nParameters with gradients: 201, Global norm: 3.1376\nParameters with gradients: 201, Global norm: 3.1881\nParameters with gradients: 201, Global norm: 7.9805\nParameters with gradients: 201, Global norm: 4.3354\nParameters with gradients: 201, Global norm: 5.4597\nParameters with gradients: 201, Global norm: 2.8028\nParameters with gradients: 201, Global norm: 2.2877\nParameters with gradients: 201, Global norm: 3.6817\nParameters with gradients: 201, Global norm: 3.4231\nParameters with gradients: 201, Global norm: 3.3140\nParameters with gradients: 201, Global norm: 12.4730\nParameters with gradients: 201, Global norm: 2.6459\nParameters with gradients: 201, Global norm: 2.0433\nParameters with gradients: 201, Global norm: 2.6308\nParameters with gradients: 201, Global norm: 2.4217\nParameters with gradients: 201, Global norm: 5.6195\nParameters with gradients: 201, Global norm: 6.0642\nParameters with gradients: 201, Global norm: 2.7199\nParameters with gradients: 201, Global norm: 6.0543\nParameters with gradients: 201, Global norm: 5.7802\nParameters with gradients: 201, Global norm: 2.1792\nParameters with gradients: 201, Global norm: 2.3750\nParameters with gradients: 201, Global norm: 5.7964\nParameters with gradients: 201, Global norm: 5.3292\nParameters with gradients: 201, Global norm: 2.4920\nParameters with gradients: 201, Global norm: 3.1525\nParameters with gradients: 201, Global norm: 4.5212\nParameters with gradients: 201, Global norm: 3.3672\nParameters with gradients: 201, Global norm: 7.1356\nParameters with gradients: 201, Global norm: 3.3173\nParameters with gradients: 201, Global norm: 3.3059\nParameters with gradients: 201, Global norm: 4.6701\nParameters with gradients: 201, Global norm: 6.7754\nParameters with gradients: 201, Global norm: 6.1618\nParameters with gradients: 201, Global norm: 3.0303\nParameters with gradients: 201, Global norm: 3.0878\nParameters with gradients: 201, Global norm: 5.4281\nParameters with gradients: 201, Global norm: 6.2080\nParameters with gradients: 201, Global norm: 5.2894\nParameters with gradients: 201, Global norm: 5.5792\nParameters with gradients: 201, Global norm: 7.2695\nParameters with gradients: 201, Global norm: 4.0030\nParameters with gradients: 201, Global norm: 4.2483\nParameters with gradients: 201, Global norm: 3.9472\nParameters with gradients: 201, Global norm: 6.7005\nParameters with gradients: 201, Global norm: 6.3700\nParameters with gradients: 201, Global norm: 9.4031\nParameters with gradients: 201, Global norm: 4.4996\nParameters with gradients: 201, Global norm: 3.6623\nParameters with gradients: 201, Global norm: 3.6193\nParameters with gradients: 201, Global norm: 6.8885\nParameters with gradients: 201, Global norm: 11.0457\nParameters with gradients: 201, Global norm: 9.2257\nParameters with gradients: 201, Global norm: 8.1303\nParameters with gradients: 201, Global norm: 10.3718\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:16:02] Energy consumed for RAM : 0.000250 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:16:02] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:16:02] Energy consumed for All CPU : 0.000531 kWh\n[codecarbon INFO @ 08:16:02] Energy consumed for all GPUs : 0.000949 kWh. Total GPU Power : 75.92316747785993 W\n[codecarbon INFO @ 08:16:02] 0.001730 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 3.8541\nParameters with gradients: 201, Global norm: 7.2470\nParameters with gradients: 201, Global norm: 5.8121\nParameters with gradients: 201, Global norm: 9.2664\nParameters with gradients: 201, Global norm: 7.1993\nParameters with gradients: 201, Global norm: 3.9833\nParameters with gradients: 201, Global norm: 4.8237\nParameters with gradients: 201, Global norm: 4.7818\nParameters with gradients: 201, Global norm: 9.4651\nParameters with gradients: 201, Global norm: 7.2158\nParameters with gradients: 201, Global norm: 5.9046\nParameters with gradients: 201, Global norm: 4.4629\nParameters with gradients: 201, Global norm: 5.6076\nParameters with gradients: 201, Global norm: 3.0667\nParameters with gradients: 201, Global norm: 2.7868\nParameters with gradients: 201, Global norm: 10.8453\nParameters with gradients: 201, Global norm: 3.0095\nParameters with gradients: 201, Global norm: 6.5291\nParameters with gradients: 201, Global norm: 6.8431\nParameters with gradients: 201, Global norm: 6.7912\nParameters with gradients: 201, Global norm: 9.3958\nParameters with gradients: 201, Global norm: 6.7401\nParameters with gradients: 201, Global norm: 4.7497\nParameters with gradients: 201, Global norm: 2.6886\nParameters with gradients: 201, Global norm: 4.7095\nParameters with gradients: 201, Global norm: 4.7774\nParameters with gradients: 201, Global norm: 6.8219\nParameters with gradients: 201, Global norm: 10.5380\nParameters with gradients: 201, Global norm: 6.3744\nParameters with gradients: 201, Global norm: 4.6016\nParameters with gradients: 201, Global norm: 5.2401\nParameters with gradients: 201, Global norm: 3.3810\nParameters with gradients: 201, Global norm: 6.1769\nParameters with gradients: 201, Global norm: 3.2031\nParameters with gradients: 201, Global norm: 7.3911\nParameters with gradients: 201, Global norm: 7.7235\nParameters with gradients: 201, Global norm: 3.4098\nParameters with gradients: 201, Global norm: 9.9396\nParameters with gradients: 201, Global norm: 3.9337\nAverage loss: 0.6677\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.5884, F1: 0.5867\n\nEpoch 3/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 3:   0%|          | 0/156 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 4.9426\nParameters with gradients: 201, Global norm: 6.9505\nParameters with gradients: 201, Global norm: 8.2993\nParameters with gradients: 201, Global norm: 5.0407\nParameters with gradients: 201, Global norm: 3.5975\nParameters with gradients: 201, Global norm: 5.4591\nParameters with gradients: 201, Global norm: 3.6040\nParameters with gradients: 201, Global norm: 6.7779\nParameters with gradients: 201, Global norm: 6.9670\nParameters with gradients: 201, Global norm: 18.8493\nParameters with gradients: 201, Global norm: 2.8636\nParameters with gradients: 201, Global norm: 4.4989\nParameters with gradients: 201, Global norm: 7.0713\nParameters with gradients: 201, Global norm: 7.2342\nParameters with gradients: 201, Global norm: 7.8527\nParameters with gradients: 201, Global norm: 7.4547\nParameters with gradients: 201, Global norm: 9.3527\nParameters with gradients: 201, Global norm: 5.8618\nParameters with gradients: 201, Global norm: 6.5964\nParameters with gradients: 201, Global norm: 5.4356\nParameters with gradients: 201, Global norm: 4.9778\nParameters with gradients: 201, Global norm: 6.7098\nParameters with gradients: 201, Global norm: 7.4601\nParameters with gradients: 201, Global norm: 6.3919\nParameters with gradients: 201, Global norm: 11.5100\nParameters with gradients: 201, Global norm: 6.4332\nParameters with gradients: 201, Global norm: 6.1008\nParameters with gradients: 201, Global norm: 8.5454\nParameters with gradients: 201, Global norm: 10.0370\nParameters with gradients: 201, Global norm: 7.1339\nParameters with gradients: 201, Global norm: 10.1481\nParameters with gradients: 201, Global norm: 7.0310\nParameters with gradients: 201, Global norm: 5.6459\nParameters with gradients: 201, Global norm: 6.3830\nParameters with gradients: 201, Global norm: 4.7481\nParameters with gradients: 201, Global norm: 11.4077\nParameters with gradients: 201, Global norm: 18.6767\nParameters with gradients: 201, Global norm: 10.8910\nParameters with gradients: 201, Global norm: 6.4082\nParameters with gradients: 201, Global norm: 12.1098\nParameters with gradients: 201, Global norm: 8.2555\nParameters with gradients: 201, Global norm: 13.6186\nParameters with gradients: 201, Global norm: 5.5085\nParameters with gradients: 201, Global norm: 4.9080\nParameters with gradients: 201, Global norm: 7.8613\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:16:17] Energy consumed for RAM : 0.000333 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:16:17] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:16:17] Energy consumed for All CPU : 0.000708 kWh\n[codecarbon INFO @ 08:16:17] Energy consumed for all GPUs : 0.001266 kWh. Total GPU Power : 76.08022239810356 W\n[codecarbon INFO @ 08:16:17] 0.002307 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 15.0339\nParameters with gradients: 201, Global norm: 6.2422\nParameters with gradients: 201, Global norm: 6.5149\nParameters with gradients: 201, Global norm: 7.7617\nParameters with gradients: 201, Global norm: 10.3612\nParameters with gradients: 201, Global norm: 10.9039\nParameters with gradients: 201, Global norm: 16.1668\nParameters with gradients: 201, Global norm: 7.7739\nParameters with gradients: 201, Global norm: 7.9546\nParameters with gradients: 201, Global norm: 8.4885\nParameters with gradients: 201, Global norm: 6.1559\nParameters with gradients: 201, Global norm: 5.4745\nParameters with gradients: 201, Global norm: 7.7674\nParameters with gradients: 201, Global norm: 9.1063\nParameters with gradients: 201, Global norm: 5.2937\nParameters with gradients: 201, Global norm: 8.3194\nParameters with gradients: 201, Global norm: 6.5853\nParameters with gradients: 201, Global norm: 4.8173\nParameters with gradients: 201, Global norm: 5.5798\nParameters with gradients: 201, Global norm: 6.4663\nParameters with gradients: 201, Global norm: 5.5514\nParameters with gradients: 201, Global norm: 5.0411\nParameters with gradients: 201, Global norm: 5.1845\nParameters with gradients: 201, Global norm: 11.6348\nParameters with gradients: 201, Global norm: 12.5577\nParameters with gradients: 201, Global norm: 5.3849\nParameters with gradients: 201, Global norm: 11.2479\nParameters with gradients: 201, Global norm: 4.8231\nParameters with gradients: 201, Global norm: 11.3574\nParameters with gradients: 201, Global norm: 7.4353\nParameters with gradients: 201, Global norm: 12.7539\nParameters with gradients: 201, Global norm: 5.7741\nParameters with gradients: 201, Global norm: 9.7418\nParameters with gradients: 201, Global norm: 7.4619\nParameters with gradients: 201, Global norm: 5.0398\nParameters with gradients: 201, Global norm: 7.5426\nParameters with gradients: 201, Global norm: 21.8100\nParameters with gradients: 201, Global norm: 9.7615\nParameters with gradients: 201, Global norm: 15.2922\nParameters with gradients: 201, Global norm: 6.1534\nParameters with gradients: 201, Global norm: 8.8187\nParameters with gradients: 201, Global norm: 8.5739\nParameters with gradients: 201, Global norm: 8.7216\nParameters with gradients: 201, Global norm: 7.2585\nParameters with gradients: 201, Global norm: 11.8852\nParameters with gradients: 201, Global norm: 8.6371\nParameters with gradients: 201, Global norm: 6.9186\nParameters with gradients: 201, Global norm: 4.9118\nParameters with gradients: 201, Global norm: 9.9252\nParameters with gradients: 201, Global norm: 4.9350\nParameters with gradients: 201, Global norm: 7.2493\nParameters with gradients: 201, Global norm: 4.6385\nParameters with gradients: 201, Global norm: 5.8362\nParameters with gradients: 201, Global norm: 9.6163\nParameters with gradients: 201, Global norm: 6.2841\nParameters with gradients: 201, Global norm: 18.6936\nParameters with gradients: 201, Global norm: 8.8086\nParameters with gradients: 201, Global norm: 11.2976\nParameters with gradients: 201, Global norm: 8.9572\nParameters with gradients: 201, Global norm: 12.0074\nParameters with gradients: 201, Global norm: 8.8276\nParameters with gradients: 201, Global norm: 6.6063\nParameters with gradients: 201, Global norm: 12.1291\nParameters with gradients: 201, Global norm: 9.5555\nParameters with gradients: 201, Global norm: 10.0939\nParameters with gradients: 201, Global norm: 15.4655\nParameters with gradients: 201, Global norm: 8.9482\nParameters with gradients: 201, Global norm: 11.6842\nParameters with gradients: 201, Global norm: 17.8993\nParameters with gradients: 201, Global norm: 6.9684\nParameters with gradients: 201, Global norm: 12.9418\nParameters with gradients: 201, Global norm: 11.3533\nParameters with gradients: 201, Global norm: 10.6351\nParameters with gradients: 201, Global norm: 14.2460\nParameters with gradients: 201, Global norm: 7.5026\nParameters with gradients: 201, Global norm: 7.6508\nParameters with gradients: 201, Global norm: 9.6342\nParameters with gradients: 201, Global norm: 9.6892\nParameters with gradients: 201, Global norm: 7.6717\nParameters with gradients: 201, Global norm: 14.9245\nParameters with gradients: 201, Global norm: 8.5470\nParameters with gradients: 201, Global norm: 7.7987\nParameters with gradients: 201, Global norm: 7.5674\nParameters with gradients: 201, Global norm: 7.7952\nParameters with gradients: 201, Global norm: 8.4542\nParameters with gradients: 201, Global norm: 7.5051\nParameters with gradients: 201, Global norm: 9.7047\nParameters with gradients: 201, Global norm: 11.1116\nParameters with gradients: 201, Global norm: 7.3742\nParameters with gradients: 201, Global norm: 11.4259\nParameters with gradients: 201, Global norm: 11.3162\nParameters with gradients: 201, Global norm: 6.4041\nParameters with gradients: 201, Global norm: 5.7125\nParameters with gradients: 201, Global norm: 7.5947\nParameters with gradients: 201, Global norm: 11.7257\nParameters with gradients: 201, Global norm: 23.6319\nParameters with gradients: 201, Global norm: 7.2350\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:16:32] Energy consumed for RAM : 0.000416 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:16:32] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:16:32] Energy consumed for All CPU : 0.000885 kWh\n[codecarbon INFO @ 08:16:32] Energy consumed for all GPUs : 0.001582 kWh. Total GPU Power : 75.958967791987 W\n[codecarbon INFO @ 08:16:32] 0.002884 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 8.7176\nParameters with gradients: 201, Global norm: 6.3285\nParameters with gradients: 201, Global norm: 15.7346\nParameters with gradients: 201, Global norm: 8.1915\nParameters with gradients: 201, Global norm: 24.1216\nParameters with gradients: 201, Global norm: 8.5166\nParameters with gradients: 201, Global norm: 11.3824\nParameters with gradients: 201, Global norm: 10.0560\nParameters with gradients: 201, Global norm: 21.2678\nParameters with gradients: 201, Global norm: 22.6362\nParameters with gradients: 201, Global norm: 10.9480\nParameters with gradients: 201, Global norm: 11.4548\nParameters with gradients: 201, Global norm: 20.2956\nParameters with gradients: 201, Global norm: 10.4414\nAverage loss: 0.5101\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.5884, F1: 0.5886\n\nEpoch 4/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 4:   0%|          | 0/156 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 8.2767\nParameters with gradients: 201, Global norm: 7.7181\nParameters with gradients: 201, Global norm: 4.0379\nParameters with gradients: 201, Global norm: 5.3531\nParameters with gradients: 201, Global norm: 4.7020\nParameters with gradients: 201, Global norm: 27.1139\nParameters with gradients: 201, Global norm: 10.9732\nParameters with gradients: 201, Global norm: 4.3033\nParameters with gradients: 201, Global norm: 5.1200\nParameters with gradients: 201, Global norm: 7.0244\nParameters with gradients: 201, Global norm: 6.9743\nParameters with gradients: 201, Global norm: 6.7602\nParameters with gradients: 201, Global norm: 5.9226\nParameters with gradients: 201, Global norm: 4.6015\nParameters with gradients: 201, Global norm: 6.4485\nParameters with gradients: 201, Global norm: 4.9173\nParameters with gradients: 201, Global norm: 6.2030\nParameters with gradients: 201, Global norm: 5.4323\nParameters with gradients: 201, Global norm: 5.0560\nParameters with gradients: 201, Global norm: 5.2941\nParameters with gradients: 201, Global norm: 8.6185\nParameters with gradients: 201, Global norm: 4.7378\nParameters with gradients: 201, Global norm: 7.6935\nParameters with gradients: 201, Global norm: 5.5233\nParameters with gradients: 201, Global norm: 7.4724\nParameters with gradients: 201, Global norm: 4.3082\nParameters with gradients: 201, Global norm: 7.6839\nParameters with gradients: 201, Global norm: 3.2746\nParameters with gradients: 201, Global norm: 5.8522\nParameters with gradients: 201, Global norm: 8.2360\nParameters with gradients: 201, Global norm: 4.1156\nParameters with gradients: 201, Global norm: 4.6286\nParameters with gradients: 201, Global norm: 9.2581\nUpdated grad_norm_threshold: 9.3323 (MA grad_norm: 9.2581)\nParameters with gradients: 201, Global norm: 2.2063\nUpdated grad_norm_threshold: 6.0922 (MA grad_norm: 5.7322)\nParameters with gradients: 201, Global norm: 6.0745\nUpdated grad_norm_threshold: 5.8709 (MA grad_norm: 5.8463)\nParameters with gradients: 201, Global norm: 4.3834\nUpdated grad_norm_threshold: 5.5196 (MA grad_norm: 5.4806)\nParameters with gradients: 201, Global norm: 4.1056\nUpdated grad_norm_threshold: 5.2370 (MA grad_norm: 5.2056)\nParameters with gradients: 201, Global norm: 18.1559\nUpdated grad_norm_threshold: 7.1513 (MA grad_norm: 7.3640)\nParameters with gradients: 201, Global norm: 7.1315\nUpdated grad_norm_threshold: 7.3128 (MA grad_norm: 7.3307)\nParameters with gradients: 201, Global norm: 7.6273\nUpdated grad_norm_threshold: 7.3623 (MA grad_norm: 7.3678)\nParameters with gradients: 201, Global norm: 7.1315\nUpdated grad_norm_threshold: 7.3436 (MA grad_norm: 7.3416)\nParameters with gradients: 201, Global norm: 10.3416\nUpdated grad_norm_threshold: 7.6118 (MA grad_norm: 7.6416)\nParameters with gradients: 201, Global norm: 5.3047\nUpdated grad_norm_threshold: 7.4474 (MA grad_norm: 7.4291)\nParameters with gradients: 201, Global norm: 4.3337\nUpdated grad_norm_threshold: 7.1988 (MA grad_norm: 7.1712)\nParameters with gradients: 201, Global norm: 2.2785\nUpdated grad_norm_threshold: 6.8352 (MA grad_norm: 6.7948)\nParameters with gradients: 201, Global norm: 15.2576\nUpdated grad_norm_threshold: 7.3429 (MA grad_norm: 7.3993)\nParameters with gradients: 201, Global norm: 5.9211\nUpdated grad_norm_threshold: 7.3050 (MA grad_norm: 7.3008)\nParameters with gradients: 201, Global norm: 17.7912\nUpdated grad_norm_threshold: 7.8913 (MA grad_norm: 7.9564)\nParameters with gradients: 201, Global norm: 21.8455\nUpdated grad_norm_threshold: 8.6852 (MA grad_norm: 8.7734)\nParameters with gradients: 201, Global norm: 9.5567\nUpdated grad_norm_threshold: 8.8037 (MA grad_norm: 8.8169)\nParameters with gradients: 201, Global norm: 6.7491\nUpdated grad_norm_threshold: 8.7177 (MA grad_norm: 8.7081)\nParameters with gradients: 201, Global norm: 3.6315\nUpdated grad_norm_threshold: 8.4806 (MA grad_norm: 8.4543)\nParameters with gradients: 201, Global norm: 5.8753\nUpdated grad_norm_threshold: 8.3047 (MA grad_norm: 8.2851)\nParameters with gradients: 201, Global norm: 10.5669\nUpdated grad_norm_threshold: 8.6633 (MA grad_norm: 8.7032)\nParameters with gradients: 201, Global norm: 13.2562\nUpdated grad_norm_threshold: 9.0223 (MA grad_norm: 9.0622)\nParameters with gradients: 201, Global norm: 11.5730\nUpdated grad_norm_threshold: 9.3818 (MA grad_norm: 9.4217)\nParameters with gradients: 201, Global norm: 10.3663\nUpdated grad_norm_threshold: 9.6995 (MA grad_norm: 9.7348)\nParameters with gradients: 201, Global norm: 12.1397\nUpdated grad_norm_threshold: 9.4605 (MA grad_norm: 9.4339)\nParameters with gradients: 201, Global norm: 6.5570\nUpdated grad_norm_threshold: 9.4107 (MA grad_norm: 9.4052)\nParameters with gradients: 201, Global norm: 30.8118\nUpdated grad_norm_threshold: 10.4491 (MA grad_norm: 10.5644)\nParameters with gradients: 201, Global norm: 5.5578\nUpdated grad_norm_threshold: 10.4821 (MA grad_norm: 10.4858)\nParameters with gradients: 201, Global norm: 2.9890\nUpdated grad_norm_threshold: 10.1545 (MA grad_norm: 10.1181)\nParameters with gradients: 201, Global norm: 7.6171\nUpdated grad_norm_threshold: 10.2258 (MA grad_norm: 10.2337)\nParameters with gradients: 201, Global norm: 7.1468\nUpdated grad_norm_threshold: 10.3595 (MA grad_norm: 10.3744)\nParameters with gradients: 201, Global norm: 9.2501\nUpdated grad_norm_threshold: 10.6866 (MA grad_norm: 10.7230)\nParameters with gradients: 201, Global norm: 10.0788\nUpdated grad_norm_threshold: 10.4863 (MA grad_norm: 10.4640)\nParameters with gradients: 201, Global norm: 7.4797\nUpdated grad_norm_threshold: 10.5364 (MA grad_norm: 10.5420)\nParameters with gradients: 201, Global norm: 14.3721\nUpdated grad_norm_threshold: 10.3876 (MA grad_norm: 10.3710)\nParameters with gradients: 201, Global norm: 7.4556\nUpdated grad_norm_threshold: 9.7251 (MA grad_norm: 9.6515)\nParameters with gradients: 201, Global norm: 7.6281\nUpdated grad_norm_threshold: 9.5721 (MA grad_norm: 9.5551)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:16:47] Energy consumed for RAM : 0.000500 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:16:47] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:16:47] Energy consumed for All CPU : 0.001062 kWh\n[codecarbon INFO @ 08:16:47] Energy consumed for all GPUs : 0.001900 kWh. Total GPU Power : 76.20737864328996 W\n[codecarbon INFO @ 08:16:47] 0.003461 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 7.8943\nUpdated grad_norm_threshold: 9.6083 (MA grad_norm: 9.6124)\nParameters with gradients: 201, Global norm: 9.2487\nUpdated grad_norm_threshold: 9.8647 (MA grad_norm: 9.8932)\nParameters with gradients: 201, Global norm: 4.5096\nUpdated grad_norm_threshold: 9.8289 (MA grad_norm: 9.8249)\nParameters with gradients: 201, Global norm: 7.9280\nUpdated grad_norm_threshold: 9.7066 (MA grad_norm: 9.6930)\nParameters with gradients: 201, Global norm: 5.4731\nUpdated grad_norm_threshold: 9.3441 (MA grad_norm: 9.3038)\nParameters with gradients: 201, Global norm: 8.9461\nUpdated grad_norm_threshold: 9.1896 (MA grad_norm: 9.1725)\nParameters with gradients: 201, Global norm: 8.9708\nUpdated grad_norm_threshold: 9.1114 (MA grad_norm: 9.1027)\nParameters with gradients: 201, Global norm: 15.6357\nUpdated grad_norm_threshold: 9.2609 (MA grad_norm: 9.2775)\nParameters with gradients: 201, Global norm: 2.8658\nUpdated grad_norm_threshold: 9.1097 (MA grad_norm: 9.0929)\nParameters with gradients: 201, Global norm: 7.1826\nUpdated grad_norm_threshold: 8.0313 (MA grad_norm: 7.9115)\nParameters with gradients: 201, Global norm: 12.9624\nUpdated grad_norm_threshold: 8.2567 (MA grad_norm: 8.2817)\nParameters with gradients: 201, Global norm: 6.8216\nUpdated grad_norm_threshold: 8.4517 (MA grad_norm: 8.4734)\nParameters with gradients: 201, Global norm: 2.8537\nUpdated grad_norm_threshold: 8.2568 (MA grad_norm: 8.2352)\nParameters with gradients: 201, Global norm: 3.1112\nUpdated grad_norm_threshold: 8.0557 (MA grad_norm: 8.0334)\nParameters with gradients: 201, Global norm: 9.8723\nUpdated grad_norm_threshold: 8.0636 (MA grad_norm: 8.0645)\nParameters with gradients: 201, Global norm: 20.7840\nUpdated grad_norm_threshold: 8.5462 (MA grad_norm: 8.5998)\nParameters with gradients: 201, Global norm: 12.2742\nUpdated grad_norm_threshold: 8.8102 (MA grad_norm: 8.8395)\nParameters with gradients: 201, Global norm: 3.5962\nUpdated grad_norm_threshold: 8.3517 (MA grad_norm: 8.3007)\nParameters with gradients: 201, Global norm: 8.4952\nUpdated grad_norm_threshold: 8.3526 (MA grad_norm: 8.3527)\nParameters with gradients: 201, Global norm: 5.5737\nUpdated grad_norm_threshold: 8.2602 (MA grad_norm: 8.2500)\nParameters with gradients: 201, Global norm: 9.3790\nUpdated grad_norm_threshold: 8.3178 (MA grad_norm: 8.3242)\nParameters with gradients: 201, Global norm: 12.1180\nUpdated grad_norm_threshold: 8.4527 (MA grad_norm: 8.4677)\nParameters with gradients: 201, Global norm: 9.1395\nUpdated grad_norm_threshold: 8.6745 (MA grad_norm: 8.6992)\nParameters with gradients: 201, Global norm: 24.4787\nUpdated grad_norm_threshold: 9.4415 (MA grad_norm: 9.5267)\nParameters with gradients: 201, Global norm: 7.0519\nUpdated grad_norm_threshold: 9.5892 (MA grad_norm: 9.6056)\nParameters with gradients: 201, Global norm: 8.1782\nUpdated grad_norm_threshold: 9.5694 (MA grad_norm: 9.5672)\nParameters with gradients: 201, Global norm: 5.4005\nUpdated grad_norm_threshold: 9.4068 (MA grad_norm: 9.3887)\nParameters with gradients: 201, Global norm: 8.8989\nUpdated grad_norm_threshold: 9.0874 (MA grad_norm: 9.0519)\nParameters with gradients: 201, Global norm: 9.9193\nUpdated grad_norm_threshold: 9.3728 (MA grad_norm: 9.4046)\nParameters with gradients: 201, Global norm: 12.9169\nUpdated grad_norm_threshold: 9.6594 (MA grad_norm: 9.6913)\nParameters with gradients: 201, Global norm: 5.6662\nUpdated grad_norm_threshold: 9.3598 (MA grad_norm: 9.3265)\nParameters with gradients: 201, Global norm: 3.8609\nUpdated grad_norm_threshold: 9.1966 (MA grad_norm: 9.1784)\nParameters with gradients: 201, Global norm: 4.9925\nUpdated grad_norm_threshold: 9.2765 (MA grad_norm: 9.2854)\nParameters with gradients: 201, Global norm: 18.3840\nUpdated grad_norm_threshold: 9.9718 (MA grad_norm: 10.0490)\nParameters with gradients: 201, Global norm: 4.9993\nUpdated grad_norm_threshold: 9.8220 (MA grad_norm: 9.8054)\nParameters with gradients: 201, Global norm: 5.6958\nUpdated grad_norm_threshold: 9.1281 (MA grad_norm: 9.0509)\nParameters with gradients: 201, Global norm: 11.4155\nUpdated grad_norm_threshold: 9.0200 (MA grad_norm: 9.0080)\nParameters with gradients: 201, Global norm: 2.1635\nUpdated grad_norm_threshold: 8.9447 (MA grad_norm: 8.9364)\nParameters with gradients: 201, Global norm: 6.7506\nUpdated grad_norm_threshold: 8.8587 (MA grad_norm: 8.8491)\nParameters with gradients: 201, Global norm: 8.4269\nUpdated grad_norm_threshold: 8.9785 (MA grad_norm: 8.9918)\nParameters with gradients: 201, Global norm: 6.7917\nUpdated grad_norm_threshold: 8.8740 (MA grad_norm: 8.8624)\nParameters with gradients: 201, Global norm: 5.6736\nUpdated grad_norm_threshold: 8.5736 (MA grad_norm: 8.5402)\nParameters with gradients: 201, Global norm: 6.8729\nUpdated grad_norm_threshold: 8.4416 (MA grad_norm: 8.4269)\nParameters with gradients: 201, Global norm: 7.8484\nUpdated grad_norm_threshold: 7.6800 (MA grad_norm: 7.5954)\nParameters with gradients: 201, Global norm: 13.7964\nUpdated grad_norm_threshold: 7.9073 (MA grad_norm: 7.9326)\nParameters with gradients: 201, Global norm: 2.5580\nUpdated grad_norm_threshold: 7.6772 (MA grad_norm: 7.6516)\nParameters with gradients: 201, Global norm: 7.9025\nUpdated grad_norm_threshold: 7.7667 (MA grad_norm: 7.7767)\nParameters with gradients: 201, Global norm: 5.5019\nUpdated grad_norm_threshold: 7.6228 (MA grad_norm: 7.6068)\nParameters with gradients: 201, Global norm: 15.1677\nUpdated grad_norm_threshold: 7.8446 (MA grad_norm: 7.8693)\nParameters with gradients: 201, Global norm: 16.9370\nUpdated grad_norm_threshold: 8.0477 (MA grad_norm: 8.0703)\nParameters with gradients: 201, Global norm: 8.6839\nUpdated grad_norm_threshold: 8.2038 (MA grad_norm: 8.2212)\nParameters with gradients: 201, Global norm: 8.6954\nUpdated grad_norm_threshold: 8.4370 (MA grad_norm: 8.4629)\nParameters with gradients: 201, Global norm: 6.5969\nUpdated grad_norm_threshold: 8.5325 (MA grad_norm: 8.5431)\nParameters with gradients: 201, Global norm: 9.4651\nUpdated grad_norm_threshold: 8.1407 (MA grad_norm: 8.0972)\nParameters with gradients: 201, Global norm: 8.9112\nUpdated grad_norm_threshold: 8.2775 (MA grad_norm: 8.2927)\nParameters with gradients: 201, Global norm: 14.1234\nUpdated grad_norm_threshold: 8.6705 (MA grad_norm: 8.7141)\nParameters with gradients: 201, Global norm: 25.5881\nUpdated grad_norm_threshold: 9.3475 (MA grad_norm: 9.4228)\nParameters with gradients: 201, Global norm: 5.1889\nUpdated grad_norm_threshold: 9.5514 (MA grad_norm: 9.5740)\nParameters with gradients: 201, Global norm: 11.4140\nUpdated grad_norm_threshold: 9.7816 (MA grad_norm: 9.8072)\nParameters with gradients: 201, Global norm: 6.2713\nUpdated grad_norm_threshold: 9.7076 (MA grad_norm: 9.6994)\nParameters with gradients: 201, Global norm: 7.5123\nUpdated grad_norm_threshold: 9.7327 (MA grad_norm: 9.7354)\nParameters with gradients: 201, Global norm: 4.0347\nUpdated grad_norm_threshold: 9.6614 (MA grad_norm: 9.6535)\nParameters with gradients: 201, Global norm: 17.5814\nUpdated grad_norm_threshold: 10.1362 (MA grad_norm: 10.1889)\nParameters with gradients: 201, Global norm: 23.4219\nUpdated grad_norm_threshold: 10.8845 (MA grad_norm: 10.9676)\nParameters with gradients: 201, Global norm: 16.1498\nUpdated grad_norm_threshold: 11.0652 (MA grad_norm: 11.0853)\nParameters with gradients: 201, Global norm: 20.9694\nUpdated grad_norm_threshold: 11.9118 (MA grad_norm: 12.0058)\nParameters with gradients: 201, Global norm: 9.6441\nUpdated grad_norm_threshold: 12.0748 (MA grad_norm: 12.0929)\nParameters with gradients: 201, Global norm: 3.8512\nUpdated grad_norm_threshold: 12.0168 (MA grad_norm: 12.0104)\nParameters with gradients: 201, Global norm: 8.3499\nUpdated grad_norm_threshold: 11.7042 (MA grad_norm: 11.6695)\nParameters with gradients: 201, Global norm: 2.5317\nUpdated grad_norm_threshold: 11.0247 (MA grad_norm: 10.9492)\nParameters with gradients: 201, Global norm: 4.2848\nUpdated grad_norm_threshold: 10.7588 (MA grad_norm: 10.7293)\nParameters with gradients: 201, Global norm: 4.0378\nUpdated grad_norm_threshold: 10.5226 (MA grad_norm: 10.4964)\nParameters with gradients: 201, Global norm: 1.9286\nUpdated grad_norm_threshold: 10.2889 (MA grad_norm: 10.2630)\nParameters with gradients: 201, Global norm: 3.9952\nUpdated grad_norm_threshold: 10.0194 (MA grad_norm: 9.9895)\nParameters with gradients: 201, Global norm: 14.1795\nUpdated grad_norm_threshold: 10.2296 (MA grad_norm: 10.2529)\nParameters with gradients: 201, Global norm: 8.6642\nUpdated grad_norm_threshold: 10.0049 (MA grad_norm: 9.9799)\nParameters with gradients: 201, Global norm: 15.9754\nUpdated grad_norm_threshold: 9.5499 (MA grad_norm: 9.4993)\nParameters with gradients: 201, Global norm: 3.0330\nUpdated grad_norm_threshold: 9.4073 (MA grad_norm: 9.3915)\nParameters with gradients: 201, Global norm: 5.4252\nUpdated grad_norm_threshold: 9.1236 (MA grad_norm: 9.0921)\nParameters with gradients: 201, Global norm: 7.5997\nUpdated grad_norm_threshold: 9.1550 (MA grad_norm: 9.1585)\nParameters with gradients: 201, Global norm: 3.7627\nUpdated grad_norm_threshold: 8.9894 (MA grad_norm: 8.9710)\nParameters with gradients: 201, Global norm: 9.5013\nUpdated grad_norm_threshold: 9.2188 (MA grad_norm: 9.2443)\nParameters with gradients: 201, Global norm: 34.3659\nUpdated grad_norm_threshold: 9.9971 (MA grad_norm: 10.0836)\nParameters with gradients: 201, Global norm: 18.4478\nUpdated grad_norm_threshold: 9.8511 (MA grad_norm: 9.8349)\nParameters with gradients: 201, Global norm: 11.3517\nUpdated grad_norm_threshold: 9.6206 (MA grad_norm: 9.5950)\nParameters with gradients: 201, Global norm: 4.0910\nUpdated grad_norm_threshold: 8.8380 (MA grad_norm: 8.7510)\nAverage loss: 0.2268\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:17:02] Energy consumed for RAM : 0.000583 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:17:02] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:17:02] Energy consumed for All CPU : 0.001239 kWh\n[codecarbon INFO @ 08:17:02] Energy consumed for all GPUs : 0.002217 kWh. Total GPU Power : 76.17504631011842 W\n[codecarbon INFO @ 08:17:02] 0.004039 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"Validation - Accuracy: 0.5921, F1: 0.5922\n\nEpoch 5/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training Epoch 5:   0%|          | 0/156 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 5.1375\nUpdated grad_norm_threshold: 8.5569 (MA grad_norm: 8.5257)\nParameters with gradients: 201, Global norm: 4.7154\nUpdated grad_norm_threshold: 8.5677 (MA grad_norm: 8.5689)\nParameters with gradients: 201, Global norm: 1.4414\nUpdated grad_norm_threshold: 8.2579 (MA grad_norm: 8.2235)\nParameters with gradients: 201, Global norm: 2.2077\nUpdated grad_norm_threshold: 8.2124 (MA grad_norm: 8.2073)\nParameters with gradients: 201, Global norm: 3.8011\nUpdated grad_norm_threshold: 8.1860 (MA grad_norm: 8.1831)\nParameters with gradients: 201, Global norm: 3.9958\nUpdated grad_norm_threshold: 8.1815 (MA grad_norm: 8.1810)\nParameters with gradients: 201, Global norm: 5.9949\nUpdated grad_norm_threshold: 8.3640 (MA grad_norm: 8.3843)\nParameters with gradients: 201, Global norm: 6.1854\nUpdated grad_norm_threshold: 8.4809 (MA grad_norm: 8.4938)\nParameters with gradients: 201, Global norm: 6.3256\nUpdated grad_norm_threshold: 8.1391 (MA grad_norm: 8.1011)\nParameters with gradients: 201, Global norm: 3.7432\nUpdated grad_norm_threshold: 7.8835 (MA grad_norm: 7.8551)\nParameters with gradients: 201, Global norm: 5.9696\nUpdated grad_norm_threshold: 7.4077 (MA grad_norm: 7.3548)\nParameters with gradients: 201, Global norm: 4.4367\nUpdated grad_norm_threshold: 7.4233 (MA grad_norm: 7.4250)\nParameters with gradients: 201, Global norm: 1.4394\nUpdated grad_norm_threshold: 7.2455 (MA grad_norm: 7.2257)\nParameters with gradients: 201, Global norm: 7.7902\nUpdated grad_norm_threshold: 7.2362 (MA grad_norm: 7.2352)\nParameters with gradients: 201, Global norm: 1.5590\nUpdated grad_norm_threshold: 7.1362 (MA grad_norm: 7.1250)\nParameters with gradients: 201, Global norm: 3.2910\nUpdated grad_norm_threshold: 6.8467 (MA grad_norm: 6.8145)\nParameters with gradients: 201, Global norm: 2.5714\nUpdated grad_norm_threshold: 5.3870 (MA grad_norm: 5.2248)\nParameters with gradients: 201, Global norm: 2.7989\nUpdated grad_norm_threshold: 4.5368 (MA grad_norm: 4.4424)\nParameters with gradients: 201, Global norm: 9.2269\nUpdated grad_norm_threshold: 4.3562 (MA grad_norm: 4.3361)\nParameters with gradients: 201, Global norm: 1.6059\nUpdated grad_norm_threshold: 4.2263 (MA grad_norm: 4.2119)\nParameters with gradients: 201, Global norm: 1.7524\nUpdated grad_norm_threshold: 4.0610 (MA grad_norm: 4.0426)\nParameters with gradients: 201, Global norm: 3.0198\nUpdated grad_norm_threshold: 3.9681 (MA grad_norm: 3.9578)\nParameters with gradients: 201, Global norm: 1.9996\nUpdated grad_norm_threshold: 3.9840 (MA grad_norm: 3.9857)\nParameters with gradients: 201, Global norm: 1.2702\nUpdated grad_norm_threshold: 3.9434 (MA grad_norm: 3.9389)\nParameters with gradients: 201, Global norm: 4.6095\nUpdated grad_norm_threshold: 3.9757 (MA grad_norm: 3.9793)\nParameters with gradients: 201, Global norm: 2.6802\nUpdated grad_norm_threshold: 3.9197 (MA grad_norm: 3.9135)\nParameters with gradients: 201, Global norm: 9.3997\nUpdated grad_norm_threshold: 4.0673 (MA grad_norm: 4.0837)\nParameters with gradients: 201, Global norm: 1.8360\nUpdated grad_norm_threshold: 3.8864 (MA grad_norm: 3.8663)\nParameters with gradients: 201, Global norm: 1.9530\nUpdated grad_norm_threshold: 3.6715 (MA grad_norm: 3.6476)\nParameters with gradients: 201, Global norm: 2.0388\nUpdated grad_norm_threshold: 3.5733 (MA grad_norm: 3.5624)\nParameters with gradients: 201, Global norm: 2.2542\nUpdated grad_norm_threshold: 3.3963 (MA grad_norm: 3.3766)\nParameters with gradients: 201, Global norm: 7.6785\nUpdated grad_norm_threshold: 3.5245 (MA grad_norm: 3.5387)\nParameters with gradients: 201, Global norm: 1.5924\nUpdated grad_norm_threshold: 3.5442 (MA grad_norm: 3.5464)\nParameters with gradients: 201, Global norm: 2.0296\nUpdated grad_norm_threshold: 3.2869 (MA grad_norm: 3.2584)\nParameters with gradients: 201, Global norm: 2.5763\nUpdated grad_norm_threshold: 3.3070 (MA grad_norm: 3.3092)\nParameters with gradients: 201, Global norm: 1.7872\nUpdated grad_norm_threshold: 3.2413 (MA grad_norm: 3.2340)\nParameters with gradients: 201, Global norm: 1.7603\nUpdated grad_norm_threshold: 3.1983 (MA grad_norm: 3.1935)\nParameters with gradients: 201, Global norm: 1.2921\nUpdated grad_norm_threshold: 3.1262 (MA grad_norm: 3.1181)\nParameters with gradients: 201, Global norm: 4.1520\nUpdated grad_norm_threshold: 2.8906 (MA grad_norm: 2.8644)\nParameters with gradients: 201, Global norm: 2.0451\nUpdated grad_norm_threshold: 2.8868 (MA grad_norm: 2.8864)\nParameters with gradients: 201, Global norm: 5.0340\nUpdated grad_norm_threshold: 3.0341 (MA grad_norm: 3.0504)\nParameters with gradients: 201, Global norm: 1.4106\nUpdated grad_norm_threshold: 2.9764 (MA grad_norm: 2.9700)\nParameters with gradients: 201, Global norm: 3.6942\nUpdated grad_norm_threshold: 3.0469 (MA grad_norm: 3.0547)\nParameters with gradients: 201, Global norm: 1.9459\nUpdated grad_norm_threshold: 3.0843 (MA grad_norm: 3.0885)\nParameters with gradients: 201, Global norm: 1.4847\nUpdated grad_norm_threshold: 2.9475 (MA grad_norm: 2.9322)\nParameters with gradients: 201, Global norm: 2.2364\nUpdated grad_norm_threshold: 2.9138 (MA grad_norm: 2.9101)\nParameters with gradients: 201, Global norm: 6.0961\nUpdated grad_norm_threshold: 2.7618 (MA grad_norm: 2.7449)\nParameters with gradients: 201, Global norm: 1.7458\nUpdated grad_norm_threshold: 2.7425 (MA grad_norm: 2.7404)\nParameters with gradients: 201, Global norm: 1.9364\nUpdated grad_norm_threshold: 2.7398 (MA grad_norm: 2.7395)\nParameters with gradients: 201, Global norm: 1.7846\nUpdated grad_norm_threshold: 2.7281 (MA grad_norm: 2.7268)\nParameters with gradients: 201, Global norm: 1.8966\nUpdated grad_norm_threshold: 2.7109 (MA grad_norm: 2.7089)\nParameters with gradients: 201, Global norm: 1.7193\nUpdated grad_norm_threshold: 2.4410 (MA grad_norm: 2.4110)\nParameters with gradients: 201, Global norm: 2.5083\nUpdated grad_norm_threshold: 2.4552 (MA grad_norm: 2.4568)\nParameters with gradients: 201, Global norm: 2.9347\nUpdated grad_norm_threshold: 2.4973 (MA grad_norm: 2.5020)\nParameters with gradients: 201, Global norm: 1.6362\nUpdated grad_norm_threshold: 2.4593 (MA grad_norm: 2.4550)\nParameters with gradients: 201, Global norm: 1.4662\nUpdated grad_norm_threshold: 2.4410 (MA grad_norm: 2.4390)\nParameters with gradients: 201, Global norm: 10.5502\nUpdated grad_norm_threshold: 2.8347 (MA grad_norm: 2.8785)\nParameters with gradients: 201, Global norm: 1.5690\nUpdated grad_norm_threshold: 2.8865 (MA grad_norm: 2.8923)\nParameters with gradients: 201, Global norm: 1.2163\nUpdated grad_norm_threshold: 2.7596 (MA grad_norm: 2.7455)\nParameters with gradients: 201, Global norm: 2.0946\nUpdated grad_norm_threshold: 2.7492 (MA grad_norm: 2.7480)\nParameters with gradients: 201, Global norm: 1.3204\nUpdated grad_norm_threshold: 2.5810 (MA grad_norm: 2.5623)\nParameters with gradients: 201, Global norm: 1.3264\nUpdated grad_norm_threshold: 2.5604 (MA grad_norm: 2.5581)\nParameters with gradients: 201, Global norm: 5.3986\nUpdated grad_norm_threshold: 2.6350 (MA grad_norm: 2.6433)\nParameters with gradients: 201, Global norm: 1.6632\nUpdated grad_norm_threshold: 2.6298 (MA grad_norm: 2.6292)\nParameters with gradients: 201, Global norm: 6.8142\nUpdated grad_norm_threshold: 2.8691 (MA grad_norm: 2.8957)\nParameters with gradients: 201, Global norm: 2.6485\nUpdated grad_norm_threshold: 2.9116 (MA grad_norm: 2.9163)\nParameters with gradients: 201, Global norm: 2.1866\nUpdated grad_norm_threshold: 2.7399 (MA grad_norm: 2.7208)\nParameters with gradients: 201, Global norm: 1.3248\nUpdated grad_norm_threshold: 2.7038 (MA grad_norm: 2.6998)\nParameters with gradients: 201, Global norm: 1.2682\nUpdated grad_norm_threshold: 2.6701 (MA grad_norm: 2.6663)\nParameters with gradients: 201, Global norm: 1.8015\nUpdated grad_norm_threshold: 2.6675 (MA grad_norm: 2.6672)\nParameters with gradients: 201, Global norm: 1.5074\nUpdated grad_norm_threshold: 2.6497 (MA grad_norm: 2.6477)\nParameters with gradients: 201, Global norm: 1.6388\nUpdated grad_norm_threshold: 2.6443 (MA grad_norm: 2.6437)\nParameters with gradients: 201, Global norm: 1.6220\nUpdated grad_norm_threshold: 2.6039 (MA grad_norm: 2.5994)\nParameters with gradients: 201, Global norm: 11.0306\nUpdated grad_norm_threshold: 2.9642 (MA grad_norm: 3.0042)\nParameters with gradients: 201, Global norm: 7.6196\nUpdated grad_norm_threshold: 3.2694 (MA grad_norm: 3.3034)\nParameters with gradients: 201, Global norm: 1.7255\nUpdated grad_norm_threshold: 3.3116 (MA grad_norm: 3.3163)\nParameters with gradients: 201, Global norm: 6.0744\nUpdated grad_norm_threshold: 3.1144 (MA grad_norm: 3.0925)\nParameters with gradients: 201, Global norm: 1.8622\nUpdated grad_norm_threshold: 3.1079 (MA grad_norm: 3.1072)\nParameters with gradients: 201, Global norm: 3.2508\nUpdated grad_norm_threshold: 3.1988 (MA grad_norm: 3.2089)\nParameters with gradients: 201, Global norm: 1.2350\nUpdated grad_norm_threshold: 3.1692 (MA grad_norm: 3.1659)\nParameters with gradients: 201, Global norm: 2.4319\nUpdated grad_norm_threshold: 3.2163 (MA grad_norm: 3.2215)\nParameters with gradients: 201, Global norm: 1.0947\nUpdated grad_norm_threshold: 3.2106 (MA grad_norm: 3.2099)\nParameters with gradients: 201, Global norm: 1.7633\nUpdated grad_norm_threshold: 3.0464 (MA grad_norm: 3.0282)\nParameters with gradients: 201, Global norm: 3.3330\nUpdated grad_norm_threshold: 3.1051 (MA grad_norm: 3.1117)\nParameters with gradients: 201, Global norm: 2.9435\nUpdated grad_norm_threshold: 2.9368 (MA grad_norm: 2.9181)\nParameters with gradients: 201, Global norm: 1.7377\nUpdated grad_norm_threshold: 2.8790 (MA grad_norm: 2.8726)\nParameters with gradients: 201, Global norm: 5.1477\nUpdated grad_norm_threshold: 3.0065 (MA grad_norm: 3.0206)\nParameters with gradients: 201, Global norm: 2.1983\nUpdated grad_norm_threshold: 3.0585 (MA grad_norm: 3.0643)\nParameters with gradients: 201, Global norm: 1.4885\nUpdated grad_norm_threshold: 3.0737 (MA grad_norm: 3.0753)\nParameters with gradients: 201, Global norm: 1.3786\nUpdated grad_norm_threshold: 3.0561 (MA grad_norm: 3.0542)\nParameters with gradients: 201, Global norm: 1.6211\nUpdated grad_norm_threshold: 3.0595 (MA grad_norm: 3.0599)\nParameters with gradients: 201, Global norm: 4.6788\nUpdated grad_norm_threshold: 3.1966 (MA grad_norm: 3.2119)\nParameters with gradients: 201, Global norm: 6.0411\nUpdated grad_norm_threshold: 3.4092 (MA grad_norm: 3.4328)\nParameters with gradients: 201, Global norm: 1.1291\nUpdated grad_norm_threshold: 2.9849 (MA grad_norm: 2.9378)\nParameters with gradients: 201, Global norm: 1.3369\nUpdated grad_norm_threshold: 2.6597 (MA grad_norm: 2.6236)\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 08:17:17] Energy consumed for RAM : 0.000666 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:17:17] Delta energy consumed for CPU with constant : 0.000177 kWh, power : 42.5 W\n[codecarbon INFO @ 08:17:17] Energy consumed for All CPU : 0.001416 kWh\n[codecarbon INFO @ 08:17:17] Energy consumed for all GPUs : 0.002534 kWh. Total GPU Power : 76.22109949850827 W\n[codecarbon INFO @ 08:17:17] 0.004616 kWh of electricity used since the beginning.\n[codecarbon INFO @ 08:17:17] 0.014212 g.CO2eq/s mean an estimation of 448.1858174961543 kg.CO2eq/year\n","output_type":"stream"},{"name":"stdout","text":"Parameters with gradients: 201, Global norm: 1.4460\nUpdated grad_norm_threshold: 2.6146 (MA grad_norm: 2.6096)\nParameters with gradients: 201, Global norm: 1.5364\nUpdated grad_norm_threshold: 2.4059 (MA grad_norm: 2.3827)\nParameters with gradients: 201, Global norm: 6.1800\nUpdated grad_norm_threshold: 2.5794 (MA grad_norm: 2.5986)\nParameters with gradients: 201, Global norm: 1.3956\nUpdated grad_norm_threshold: 2.5132 (MA grad_norm: 2.5059)\nParameters with gradients: 201, Global norm: 1.5712\nUpdated grad_norm_threshold: 2.5217 (MA grad_norm: 2.5227)\nParameters with gradients: 201, Global norm: 1.5209\nUpdated grad_norm_threshold: 2.4816 (MA grad_norm: 2.4771)\nParameters with gradients: 201, Global norm: 4.9680\nUpdated grad_norm_threshold: 2.6519 (MA grad_norm: 2.6708)\nParameters with gradients: 201, Global norm: 1.3459\nUpdated grad_norm_threshold: 2.6501 (MA grad_norm: 2.6499)\nParameters with gradients: 201, Global norm: 1.2194\nUpdated grad_norm_threshold: 2.5548 (MA grad_norm: 2.5442)\nParameters with gradients: 201, Global norm: 1.2514\nUpdated grad_norm_threshold: 2.4692 (MA grad_norm: 2.4596)\nParameters with gradients: 201, Global norm: 1.6116\nUpdated grad_norm_threshold: 2.4549 (MA grad_norm: 2.4533)\nParameters with gradients: 201, Global norm: 1.2693\nUpdated grad_norm_threshold: 2.2790 (MA grad_norm: 2.2594)\nParameters with gradients: 201, Global norm: 2.4902\nUpdated grad_norm_threshold: 2.2745 (MA grad_norm: 2.2740)\nParameters with gradients: 201, Global norm: 1.4871\nUpdated grad_norm_threshold: 2.2740 (MA grad_norm: 2.2739)\nParameters with gradients: 201, Global norm: 1.3039\nUpdated grad_norm_threshold: 2.2706 (MA grad_norm: 2.2702)\nParameters with gradients: 201, Global norm: 1.7564\nUpdated grad_norm_threshold: 2.2763 (MA grad_norm: 2.2770)\nParameters with gradients: 201, Global norm: 3.9012\nUpdated grad_norm_threshold: 2.2419 (MA grad_norm: 2.2381)\nParameters with gradients: 201, Global norm: 13.7645\nUpdated grad_norm_threshold: 2.5860 (MA grad_norm: 2.6242)\nParameters with gradients: 201, Global norm: 2.3552\nUpdated grad_norm_threshold: 2.6756 (MA grad_norm: 2.6855)\nParameters with gradients: 201, Global norm: 0.9169\nUpdated grad_norm_threshold: 2.6656 (MA grad_norm: 2.6645)\nParameters with gradients: 201, Global norm: 1.4056\nUpdated grad_norm_threshold: 2.6628 (MA grad_norm: 2.6625)\nParameters with gradients: 201, Global norm: 1.2156\nUpdated grad_norm_threshold: 2.6481 (MA grad_norm: 2.6465)\nParameters with gradients: 201, Global norm: 5.1616\nUpdated grad_norm_threshold: 2.6008 (MA grad_norm: 2.5956)\nParameters with gradients: 201, Global norm: 2.1023\nUpdated grad_norm_threshold: 2.6279 (MA grad_norm: 2.6309)\nParameters with gradients: 201, Global norm: 8.3081\nUpdated grad_norm_threshold: 2.9338 (MA grad_norm: 2.9677)\nParameters with gradients: 201, Global norm: 5.6203\nUpdated grad_norm_threshold: 3.1488 (MA grad_norm: 3.1727)\nParameters with gradients: 201, Global norm: 1.0930\nUpdated grad_norm_threshold: 2.9959 (MA grad_norm: 2.9790)\nParameters with gradients: 201, Global norm: 1.5357\nUpdated grad_norm_threshold: 2.9892 (MA grad_norm: 2.9885)\nParameters with gradients: 201, Global norm: 1.2652\nUpdated grad_norm_threshold: 2.9906 (MA grad_norm: 2.9907)\nParameters with gradients: 201, Global norm: 1.7100\nUpdated grad_norm_threshold: 3.0114 (MA grad_norm: 3.0137)\nParameters with gradients: 201, Global norm: 7.5415\nUpdated grad_norm_threshold: 3.2803 (MA grad_norm: 3.3102)\nParameters with gradients: 201, Global norm: 2.0849\nUpdated grad_norm_threshold: 3.3439 (MA grad_norm: 3.3509)\nParameters with gradients: 201, Global norm: 3.1494\nUpdated grad_norm_threshold: 3.3799 (MA grad_norm: 3.3839)\nParameters with gradients: 201, Global norm: 50.5911\nUpdated grad_norm_threshold: 5.5932 (MA grad_norm: 5.8391)\nParameters with gradients: 201, Global norm: 3.1899\nUpdated grad_norm_threshold: 5.8994 (MA grad_norm: 5.9334)\nParameters with gradients: 201, Global norm: 1.7324\nUpdated grad_norm_threshold: 5.9289 (MA grad_norm: 5.9322)\nParameters with gradients: 201, Global norm: 3.2635\nUpdated grad_norm_threshold: 5.9032 (MA grad_norm: 5.9003)\nParameters with gradients: 201, Global norm: 2.4013\nUpdated grad_norm_threshold: 5.3893 (MA grad_norm: 5.3322)\nParameters with gradients: 201, Global norm: 1.4401\nUpdated grad_norm_threshold: 5.2967 (MA grad_norm: 5.2864)\nParameters with gradients: 201, Global norm: 9.5435\nUpdated grad_norm_threshold: 5.6756 (MA grad_norm: 5.7177)\nParameters with gradients: 201, Global norm: 16.7846\nUpdated grad_norm_threshold: 6.4056 (MA grad_norm: 6.4867)\nParameters with gradients: 201, Global norm: 5.5423\nUpdated grad_norm_threshold: 6.6733 (MA grad_norm: 6.7030)\nParameters with gradients: 201, Global norm: 1.2123\nUpdated grad_norm_threshold: 6.5223 (MA grad_norm: 6.5056)\nParameters with gradients: 201, Global norm: 1.2081\nUpdated grad_norm_threshold: 6.4670 (MA grad_norm: 6.4609)\nParameters with gradients: 201, Global norm: 1.3038\nUpdated grad_norm_threshold: 6.1463 (MA grad_norm: 6.1106)\nParameters with gradients: 201, Global norm: 1.5261\nUpdated grad_norm_threshold: 5.9300 (MA grad_norm: 5.9059)\nParameters with gradients: 201, Global norm: 2.0414\nUpdated grad_norm_threshold: 5.9510 (MA grad_norm: 5.9534)\nParameters with gradients: 201, Global norm: 4.2923\nUpdated grad_norm_threshold: 6.0772 (MA grad_norm: 6.0912)\nParameters with gradients: 201, Global norm: 4.2612\nUpdated grad_norm_threshold: 6.2246 (MA grad_norm: 6.2410)\nParameters with gradients: 201, Global norm: 1.1010\nUpdated grad_norm_threshold: 6.2119 (MA grad_norm: 6.2105)\nParameters with gradients: 201, Global norm: 11.3156\nUpdated grad_norm_threshold: 6.3805 (MA grad_norm: 6.3992)\nParameters with gradients: 201, Global norm: 1.2479\nUpdated grad_norm_threshold: 6.3597 (MA grad_norm: 6.3574)\nParameters with gradients: 201, Global norm: 2.2608\nUpdated grad_norm_threshold: 6.3176 (MA grad_norm: 6.3130)\nParameters with gradients: 201, Global norm: 1.3380\nUpdated grad_norm_threshold: 4.0970 (MA grad_norm: 3.8503)\nParameters with gradients: 201, Global norm: 2.0122\nUpdated grad_norm_threshold: 3.8220 (MA grad_norm: 3.7914)\nParameters with gradients: 201, Global norm: 2.1355\nUpdated grad_norm_threshold: 3.8126 (MA grad_norm: 3.8116)\nParameters with gradients: 201, Global norm: 9.3908\nUpdated grad_norm_threshold: 4.0874 (MA grad_norm: 4.1179)\nParameters with gradients: 201, Global norm: 4.5461\nUpdated grad_norm_threshold: 4.2114 (MA grad_norm: 4.2252)\nParameters with gradients: 201, Global norm: 1.6856\nUpdated grad_norm_threshold: 4.2348 (MA grad_norm: 4.2375)\nParameters with gradients: 201, Global norm: 2.7326\nUpdated grad_norm_threshold: 3.9307 (MA grad_norm: 3.8969)\nParameters with gradients: 201, Global norm: 11.8724\nUpdated grad_norm_threshold: 3.6792 (MA grad_norm: 3.6513)\nAverage loss: 0.0971\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Validation - Accuracy: 0.5993, F1: 0.5995\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"[codecarbon INFO @ 08:17:31] Energy consumed for RAM : 0.000740 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 08:17:31] Delta energy consumed for CPU with constant : 0.000157 kWh, power : 42.5 W\n[codecarbon INFO @ 08:17:31] Energy consumed for All CPU : 0.001573 kWh\n[codecarbon INFO @ 08:17:31] Energy consumed for all GPUs : 0.002816 kWh. Total GPU Power : 76.30913444911833 W\n[codecarbon INFO @ 08:17:31] 0.005130 kWh of electricity used since the beginning.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Final Results ---\n{\n  \"model_name\": \"Adaptive_Quantized_BERT_RTE\",\n  \"dataset\": \"RTE\",\n  \"accuracy\": 0.5992779783393501,\n  \"f1\": 0.5995078190306681,\n  \"scheduler_metrics\": {\n    \"adaptation_log\": [],\n    \"precision_switch_counts_int4\": 0,\n    \"precision_switch_counts_int8\": 0,\n    \"precision_switch_counts_fp32\": 0,\n    \"avg_precision_level\": 32.0,\n    \"precision_distribution\": {\n      \"INT4\": 0.0,\n      \"INT8\": 0.0,\n      \"FP32\": 1.0\n    }\n  },\n  \"performance_metrics\": {\n    \"total_duration_s\": 134.5682978630066,\n    \"total_emissions_kwh\": 0.0018952341736672006,\n    \"latency_ms_query\": 485.80612946933786,\n    \"throughput_tokens_sec\": 263.47959038684553,\n    \"energy_wh_token\": 5.3453129898104715e-05,\n    \"sci_gco2e_query\": 0.0017105001567393509,\n    \"wue_avg_liters_query\": 1.2315601128523325e-05\n  }\n}\n\n‚úÖ All experiments complete. Results saved to 'all_task_results.csv'\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Save all results to a JSON file\nwith open('adaptive_quantization_results.json', 'w') as f:\n    json.dump(all_results, f, indent=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}