{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Upgrade pip\n!pip install --upgrade pip\n\n# Install/upgrade required libraries\n!pip install torch --quiet\n!pip install torchvision --quiet\n!pip install torchaudio --quiet\n!pip install transformers==4.33.1 --quiet\n!pip install datasets --quiet\n!pip install scikit-learn --quiet\n!pip install codecarbon --quiet\n!pip install numpy==1.26.4 --quiet\n!pip install pandas --quiet\n!pip install tqdm --quiet\n!pip install  spikingjelly --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install  spikingjelly --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:34:48.751954Z","iopub.execute_input":"2025-09-25T12:34:48.752595Z","iopub.status.idle":"2025-09-25T12:34:51.356681Z","shell.execute_reply.started":"2025-09-25T12:34:48.752566Z","shell.execute_reply":"2025-09-25T12:34:51.355757Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertTokenizer, BertForSequenceClassification, get_polynomial_decay_schedule_with_warmup\nfrom datasets import load_dataset\nfrom codecarbon import OfflineEmissionsTracker\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport pandas as pd\nimport time\nimport json\nimport os\nimport random\nimport logging\nimport warnings\nfrom tqdm import tqdm\nfrom torch.optim import AdamW\n\n# Configuration\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"codecarbon\").setLevel(logging.INFO)\nDEVICE_CONFIG = {\n    'optimize_for_gpu': True,\n    'mixed_precision': False  # Disabled for SNN layer stability\n}\nDEVICE = \"cuda\" if torch.cuda.is_available() and DEVICE_CONFIG['optimize_for_gpu'] else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\nDATASETS = {\n    'glue_sst2': {'name': 'glue', 'config': 'sst2', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_mrpc': {'name': 'glue', 'config': 'mrpc', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_rte': {'name': 'glue', 'config': 'rte', 'split_train': 'train', 'split_val': 'validation'}\n}\nMAX_SAMPLES = 10000\nWATER_USAGE_FACTORS = {\"average_l_per_kwh\": 1.8}\nCARBON_INTENSITY = 250  # gCO2e/kWh\nBATCH_SIZE = 16\nGRADIENT_ACCUMULATION_STEPS = 2  # New: Gradient accumulation\nSEQ_LENGTH = 128\nNUM_LAYERS = 1\nNUM_EPOCHS = {'sst2': 7, 'mrpc': 5, 'rte': 3}  # Increased epochs\nLEARNING_RATE = 3e-5  # Increased learning rate\nWEIGHT_DECAY = 0.01  # New: Weight decay for regularization\nDROPOUT_RATE = 0.1  # New: Dropout rate for NSH layers\nPATIENCE = 3  # New: Early stopping patience\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef get_device():\n    device = torch.device(DEVICE)\n    if DEVICE == \"cuda\":\n        try:\n            print(f\"✓ Using GPU: {torch.cuda.get_device_name(0)} (CUDA)\")\n        except Exception:\n            print(\"✓ Using GPU (name unknown)\")\n    else:\n        print(f\"✓ Using CPU\")\n    return device\n\n# Advanced Neuromorphic & Sparse Layers\ndef gumbel_top_k_select(scores, k, temperature=1.0):\n    gumbel_noise = -torch.log(-torch.log(torch.rand_like(scores) + 1e-10) + 1e-10)\n    perturbed_scores = (scores + gumbel_noise) / temperature\n    _, top_k_indices = torch.topk(perturbed_scores, k=k, dim=-1)\n    mask = torch.zeros_like(scores, dtype=torch.bool)\n    mask.scatter_(-1, top_k_indices, 1)\n    return mask\n\nclass SurrogateSpike(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, mem, thresh=1.0):\n        ctx.save_for_backward(mem - thresh)\n        return (mem > thresh).float()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        (mem_minus_thresh,) = ctx.saved_tensors\n        grad = grad_output * (1 / (1 + torch.exp(-mem_minus_thresh * 5)))**2 * 5\n        return grad, None\n\nclass LIFLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, threshold=1.0, decay=0.9):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n        self.threshold = threshold\n        self.decay = decay\n        self.mem = None\n\n    def forward(self, x):\n        if self.mem is None or self.mem.shape != x.shape:\n            self.mem = torch.zeros_like(x, device=x.device)\n        current = self.linear(x)\n        self.mem = self.decay * self.mem + current\n        spikes = SurrogateSpike.apply(self.mem, self.threshold)\n        self.mem = self.mem.detach()\n        self.mem[spikes > 0] = 0\n        return spikes\n\n    def reset(self):\n        self.mem = None\n\nclass BigBirdStyleAttention(nn.Module):\n    def __init__(self, hidden_dim, num_heads, window_size=32, random_k=8, global_tokens=1):\n        super().__init__()\n        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n        self.hidden_dim = hidden_dim\n        self.num_heads, self.head_dim = num_heads, hidden_dim // num_heads\n        self.window_size, self.random_k, self.global_tokens = window_size, random_k, global_tokens\n        self.qkv_fused_linear = nn.Linear(hidden_dim, 3 * hidden_dim)\n        self.output_layer = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(DROPOUT_RATE)  # New: Dropout for regularization\n\n    def forward(self, x, attn_mask=None, output_attentions=False):\n        batch_size, seq_len, _ = x.shape\n        qkv = self.qkv_fused_linear(x)\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        attention_scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.head_dim)\n        window_mask = torch.ones(seq_len, seq_len, device=x.device).triu(self.window_size+1) + torch.ones(seq_len, seq_len, device=x.device).tril(-self.window_size-1)\n        global_mask = torch.ones(seq_len, seq_len, device=x.device)\n        global_mask[:self.global_tokens, :] = 0\n        global_mask[:, :self.global_tokens] = 0\n        random_mask_bool = gumbel_top_k_select(torch.randn_like(attention_scores), k=self.random_k)\n        sparse_mask = (window_mask + global_mask) > 0\n        sparse_mask = sparse_mask.unsqueeze(0).unsqueeze(1).expand(batch_size, self.num_heads, -1, -1)\n        sparse_mask = sparse_mask | ~random_mask_bool\n        attention_scores = attention_scores.masked_fill(sparse_mask, -1e9)\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                attn_mask = attn_mask.view(batch_size, 1, 1, seq_len).expand(-1, self.num_heads, seq_len, -1)\n            elif attn_mask.dim() == 4:\n                attn_mask = attn_mask.expand(-1, self.num_heads, -1, -1)\n            attention_scores = attention_scores + attn_mask\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)  # New: Apply dropout\n        attention_output = torch.matmul(attention_weights, v)\n        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n        output = self.output_layer(attention_output)\n        return (output,) if not output_attentions else (output, attention_weights)\n\nclass NSH_DistilBertLayer(nn.Module):\n    def __init__(self, original_layer):\n        super().__init__()\n        attn_module = original_layer.attention\n        hidden_dim = attn_module.q_lin.in_features\n        self.sparse_attention = BigBirdStyleAttention(hidden_dim, attn_module.n_heads)\n        self.ffn = original_layer.ffn\n        self.sa_layer_norm = original_layer.sa_layer_norm\n        self.output_layer_norm = original_layer.output_layer_norm\n        self.snn_layer = LIFLayer(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(DROPOUT_RATE)  # New: Dropout for regularization\n\n    def forward(self, x, attn_mask=None, head_mask=None, output_attentions=False):\n        normed_x = self.sa_layer_norm(x)\n        attn_output = self.sparse_attention(normed_x, attn_mask=attn_mask, output_attentions=output_attentions)\n        spiking_output = self.snn_layer(attn_output[0])\n        x = x + self.dropout(spiking_output)  # New: Apply dropout\n        ffn_output = self.ffn(self.output_layer_norm(x))\n        x = x + self.dropout(ffn_output)  # New: Apply dropout\n        return (x,) if not output_attentions else (x, attn_output[1])\n\nclass NSH_BertLayer(nn.Module):\n    def __init__(self, original_layer):\n        super().__init__()\n        attn_module = original_layer.attention.self\n        hidden_dim = attn_module.query.in_features\n        self.sparse_attention = BigBirdStyleAttention(hidden_dim, attn_module.num_attention_heads)\n        self.attention_output_dense = original_layer.attention.output\n        self.intermediate = original_layer.intermediate\n        self.output = original_layer.output\n        self.snn_layer = LIFLayer(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(DROPOUT_RATE)  # New: Dropout for regularization\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False, **kwargs):\n        attn_output_tuple = self.sparse_attention(hidden_states, attn_mask=attention_mask, output_attentions=output_attentions)\n        spiking_output = self.snn_layer(attn_output_tuple[0])\n        attention_output = self.attention_output_dense(self.dropout(spiking_output), hidden_states)  # New: Apply dropout\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return (layer_output,) if not output_attentions else (layer_output, attn_output_tuple[1])\n\n# Model Building\ndef build_baseline_model(device, model_type=\"distilbert\"):\n    print(f\"\\n🏗️ Building Baseline {model_type.capitalize()} Model...\")\n    model_class = DistilBertForSequenceClassification if model_type == \"distilbert\" else BertForSequenceClassification\n    model_name = \"distilbert-base-uncased\" if model_type == \"distilbert\" else \"bert-base-uncased\"\n    model = model_class.from_pretrained(model_name, num_labels=2).to(device)\n    print(f\"✓ Successfully created baseline {model_type.capitalize()} model.\")\n    return model\n\ndef build_nsh_model(device, model_type=\"distilbert\", num_layers=1):\n    print(f\"\\n🏗️ Building NSH {model_type.capitalize()} Model with {num_layers} layer(s)...\")\n    model_class = DistilBertForSequenceClassification if model_type == \"distilbert\" else BertForSequenceClassification\n    model_name = \"distilbert-base-uncased\" if model_type == \"distilbert\" else \"bert-base-uncased\"\n    model = model_class.from_pretrained(model_name, num_labels=2)\n    if model_type == \"distilbert\":\n        for i in range(min(num_layers, len(model.distilbert.transformer.layer))):\n            model.distilbert.transformer.layer[i] = NSH_DistilBertLayer(model.distilbert.transformer.layer[i])\n    else:\n        for i in range(min(num_layers, len(model.bert.encoder.layer))):\n            model.bert.encoder.layer[i] = NSH_BertLayer(model.bert.encoder.layer[i])\n    model = model.to(device)\n    print(f\"✓ Successfully created NSH {model_type.capitalize()} model.\")\n    return model\n\n# Data Loading\ndef get_dataloaders(model_type, seq_len, batch_size):\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') if model_type == 'distilbert' else BertTokenizer.from_pretrained('bert-base-uncased')\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    def preprocess_glue_sst2(examples):\n        enc = tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    def preprocess_glue_mrpc(examples):\n        enc = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    def preprocess_glue_rte(examples):\n        enc = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n        return {\"input_ids\": enc[\"input_ids\"], \"attention_mask\": enc[\"attention_mask\"], \"labels\": examples[\"label\"]}\n\n    dataloaders = {'train': {}, 'validation': {}}\n    for dataset_name, dataset_info in DATASETS.items():\n        train_max_samples = 2490 if dataset_name == 'glue_rte' else (10000 if dataset_name == 'glue_sst2' else MAX_SAMPLES)\n        val_max_samples = 277 if dataset_name == 'glue_rte' else MAX_SAMPLES\n        train_dataset = load_dataset(dataset_info['name'], dataset_info['config'], split=dataset_info['split_train']).select(range(min(len(load_dataset(dataset_info['name'], dataset_info['config'], split=dataset_info['split_train'])), train_max_samples)))\n        val_dataset = load_dataset(dataset_info['name'], dataset_info['config'], split=dataset_info['split_val']).select(range(min(len(load_dataset(dataset_info['name'], dataset_info['config'], split=dataset_info['split_val'])), val_max_samples)))\n        preprocess_fn = {'glue_sst2': preprocess_glue_sst2, 'glue_mrpc': preprocess_glue_mrpc, 'glue_rte': preprocess_glue_rte}[dataset_name]\n        remove_cols = {'glue_sst2': ['sentence', 'idx'], 'glue_mrpc': ['sentence1', 'sentence2', 'idx'], 'glue_rte': ['sentence1', 'sentence2', 'idx']}[dataset_name]\n        train_preprocessed = train_dataset.map(preprocess_fn, batched=True, remove_columns=remove_cols)\n        val_preprocessed = val_dataset.map(preprocess_fn, batched=True, remove_columns=remove_cols)\n        train_preprocessed.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n        val_preprocessed.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n        dataloaders['train'][dataset_name] = DataLoader(train_preprocessed, batch_size=batch_size, shuffle=True)\n        dataloaders['validation'][dataset_name] = DataLoader(val_preprocessed, batch_size=batch_size, shuffle=False)\n    print(f\"Dataloaders created for {model_type}: {', '.join(dataloaders['validation'].keys())}\")\n    return dataloaders\n\n# Evaluation Metrics\ndef evaluate_classification(model, dataloader, device):\n    model.eval()\n    predictions, labels = [], []\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])\n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            total_loss += outputs.loss.item()\n        predictions.extend(preds)\n        labels.extend(batch['labels'].cpu().numpy())\n    avg_loss = total_loss / len(dataloader)\n    return {\"accuracy\": accuracy_score(labels, predictions), \"f1\": f1_score(labels, predictions, average='weighted'), \"val_loss\": avg_loss}\n\n# Fine-Tuning with Early Stopping\ndef fine_tune(model, train_dataloader, val_dataloader, device, is_bert, task_name, epochs=NUM_EPOCHS, task_type=\"classification\", scheduler_callback=None):\n    model.train()\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_dataloader) * epochs[task_name]\n    num_warmup_steps = int(total_steps * 0.1)\n    scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps, power=1.0)\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    best_model_state = None\n\n    for epoch in range(epochs[task_name]):\n        print(f\"Epoch {epoch+1}/{epochs[task_name]}\")\n        total_loss = 0\n        for module in model.modules():\n            if isinstance(module, LIFLayer):\n                module.reset()\n        optimizer.zero_grad()\n        for i, batch in enumerate(tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")):\n            inputs = {k: v.to(device) for k, v in batch.items() if k in ['input_ids', 'attention_mask', 'token_type_ids', 'labels'] and isinstance(v, torch.Tensor)}\n            inputs[\"labels\"] = batch[\"labels\"].to(device)\n            if not is_bert:\n                inputs.pop('token_type_ids', None)\n            \n            outputs = model(**inputs)\n            loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n            loss.backward()\n            total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n\n            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                if scheduler_callback is not None:\n                    scheduler_callback()\n\n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Average training loss: {avg_loss:.4f}\")\n\n        # Validation step for early stopping\n        val_metrics = evaluate_classification(model, val_dataloader, device)\n        val_loss = val_metrics['val_loss']\n        print(f\"Validation loss: {val_loss:.4f}, Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_model_state = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                break\n\n    if best_model_state is not None:\n        model.load_state_dict(best_model_state)\n    return model\n\n# Experiment Runner\ndef run_experiment(model, model_name, dataloaders, device, is_bert, batch_size, seq_len, task_type, run_sst2=True, run_mrpc=True, run_rte=True):\n    results = {\n        'model_name': model_name,\n        'batch_size': batch_size,\n        'seq_length': seq_len,\n        'task_type': task_type,\n        'accuracy_metrics': {},\n        'performance_metrics': {},\n        'scheduler_metrics': {\n            'random_k_distribution': {'k_8': 1.0},\n            'avg_random_k': 8.0\n        }\n    }\n\n    def update_scheduler_metrics(model, num_forward_passes):\n        random_k_counts = {'k_8': 0}\n        total_layers = 0\n        total_random_k = 0\n        for layer in model.modules():\n            if isinstance(layer, (NSH_DistilBertLayer, NSH_BertLayer)):\n                total_layers += 1\n                random_k = layer.sparse_attention.random_k\n                total_random_k += random_k\n                random_k_counts['k_8'] = random_k_counts.get('k_8', 0) + (1 if random_k == 8 else 0)\n        if total_layers == 0:\n            total_layers = 1\n        return {\n            'random_k_distribution': {k: v / total_layers for k, v in random_k_counts.items()},\n            'avg_random_k': total_random_k / total_layers\n        }\n\n    print(f\"\\n--- 🚀 Measuring Metrics for {model_name} ({task_type}) ---\")\n    start_time = time.time()\n    num_queries = 0\n    num_forward_passes = 0\n    tracker = OfflineEmissionsTracker(\n        project_name=f\"Experiment_{model_name.replace(' ', '_')}\",\n        measure_power_secs=1,\n        output_dir=\".\",\n        log_level='info',\n        country_iso_code=\"USA\",\n        region=\"California\"\n    )\n    tracker.start()\n    try:\n        if run_sst2:\n            print(\"\\n--- Fine-tuning on GLUE SST-2 ---\")\n            model = fine_tune(model, dataloaders['train']['glue_sst2'], dataloaders['validation']['glue_sst2'], device, is_bert, task_name='sst2', task_type=task_type,\n                             scheduler_callback=lambda: results.update({'scheduler_metrics': update_scheduler_metrics(model, num_forward_passes)}))\n            num_forward_passes += len(dataloaders['train']['glue_sst2'].dataset) * NUM_EPOCHS['sst2']\n            print(\" Evaluating GLUE SST-2...\")\n            metrics = evaluate_classification(model, dataloaders['validation']['glue_sst2'], device)\n            results['accuracy_metrics']['sst2_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics']['sst2_f1'] = metrics['f1']\n            num_queries += len(dataloaders['validation']['glue_sst2'].dataset)\n            num_forward_passes += len(dataloaders['validation']['glue_sst2'].dataset)\n            print(f\" SST-2 Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        if run_mrpc:\n            print(\"\\n--- Fine-tuning on GLUE MRPC ---\")\n            model = fine_tune(model, dataloaders['train']['glue_mrpc'], dataloaders['validation']['glue_mrpc'], device, is_bert, task_name='mrpc', task_type=task_type,\n                             scheduler_callback=lambda: results.update({'scheduler_metrics': update_scheduler_metrics(model, num_forward_passes)}))\n            num_forward_passes += len(dataloaders['train']['glue_mrpc'].dataset) * NUM_EPOCHS['mrpc']\n            print(\" Evaluating GLUE MRPC...\")\n            metrics = evaluate_classification(model, dataloaders['validation']['glue_mrpc'], device)\n            results['accuracy_metrics']['mrpc_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics']['mrpc_f1'] = metrics['f1']\n            num_queries += len(dataloaders['validation']['glue_mrpc'].dataset)\n            num_forward_passes += len(dataloaders['validation']['glue_mrpc'].dataset)\n            print(f\" MRPC Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        if run_rte:\n            print(\"\\n--- Fine-tuning on GLUE RTE ---\")\n            model = fine_tune(model, dataloaders['train']['glue_rte'], dataloaders['validation']['glue_rte'], device, is_bert, task_name='rte', task_type=task_type,\n                             scheduler_callback=lambda: results.update({'scheduler_metrics': update_scheduler_metrics(model, num_forward_passes)}))\n            num_forward_passes += len(dataloaders['train']['glue_rte'].dataset) * NUM_EPOCHS['rte']\n            print(\" Evaluating GLUE RTE...\")\n            metrics = evaluate_classification(model, dataloaders['validation']['glue_rte'], device)\n            results['accuracy_metrics']['rte_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics']['rte_f1'] = metrics['f1']\n            num_queries += len(dataloaders['validation']['glue_rte'].dataset)\n            num_forward_passes += len(dataloaders['validation']['glue_rte'].dataset)\n            print(f\" RTE Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n        results['scheduler_metrics'] = update_scheduler_metrics(model, num_forward_passes)\n        emissions_kwh = tracker.stop() or 0.0\n    except Exception as e:\n        print(f\"🚨 Experiment failed for {model_name}: {e}\")\n        try:\n            emissions_kwh = tracker.stop() or 0.0\n        except Exception:\n            emissions_kwh = 0.0\n\n    total_duration_s = time.time() - start_time\n    total_tokens_processed = num_queries * seq_len\n    total_carbon_g = emissions_kwh * CARBON_INTENSITY\n    results['performance_metrics'] = {\n        'latency_ms_query': (total_duration_s / num_queries) * 1000 if num_queries > 0 else 0,\n        'throughput_tokens_sec': total_tokens_processed / total_duration_s if total_duration_s > 0 else 0,\n        'energy_wh_token': (emissions_kwh * 1000) / total_tokens_processed if total_tokens_processed > 0 else 0,\n        'sci_gco2e_query': total_carbon_g / num_queries if num_queries > 0 else 0,\n        'wue_avg_liters_query': (emissions_kwh * WATER_USAGE_FACTORS['average_l_per_kwh']) / num_queries if num_queries > 0 else 0,\n        'total_emissions_kgco2eq': total_carbon_g / 1000,\n        'total_energy_kwh': emissions_kwh\n    }\n    print(f\"\\n--- Results for {model_name} ---\")\n    print(f\" Duration: {total_duration_s:.2f}s | Emissions: {total_carbon_g / 1000:.6f} kg CO2eq | Energy: {emissions_kwh:.6f} kWh | Queries: {num_queries}\")\n    print(json.dumps(results, indent=2))\n    print(\"-\" * 40)\n    return results\n\n# --- SECTION: 8. MAIN EXECUTION ---\nif __name__ == \"__main__\":\n    DEVICE = get_device()\n    set_seed(42)\n    batch_size = BATCH_SIZE\n    num_layers = NUM_LAYERS\n    all_results = []\n    model_configs = [\n        # Baseline Distilbert Niche wala\n    #  {'model_type': 'distilbert', 'task_type': 'classification', 'is_nsh': False, 'name': 'Baseline_DistilBERT_Classification'},\n        # Baseline Bert Niche wala\n    #  {'model_type': 'bert', 'task_type': 'classification', 'is_nsh': False, 'name': 'Baseline_BERT_Classification'},\n    \n        # NSH Distilbert isko run krana hai\n     {'model_type': 'distilbert', 'task_type': 'classification', 'is_nsh': True, 'name': 'NSH_DistilBERT_Classification'},\n        # NSH Bert  ise bhi run krana hai\n    #{'model_type': 'bert', 'task_type': 'classification', 'is_nsh': True, 'name': 'NSH_BERT_Classification'}\n    ]\n    for config in model_configs:\n        model_type = config['model_type']\n        task_type = config['task_type']\n        is_nsh = config['is_nsh']\n        model_name = config['name']\n        seq_len = SEQ_LENGTH\n        dataloaders = get_dataloaders(model_type=model_type, seq_len=seq_len, batch_size=batch_size)\n        if is_nsh:\n            model = build_nsh_model(device=DEVICE, model_type=model_type, num_layers=num_layers)\n        else:\n            model = build_baseline_model(device=DEVICE, model_type=model_type)\n        results = run_experiment(model=model, model_name=model_name, dataloaders=dataloaders, device=DEVICE,\n            is_bert=(model_type == 'bert'),batch_size=batch_size, seq_len=seq_len,\n            task_type=task_type, run_sst2=True,run_mrpc=True,run_rte=True)\n        all_results.append(results)\n    with open('nsh_results.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n    df_out = pd.json_normalize(all_results, sep='_')\n    df_out.to_csv('nsh_results.csv', index=False)\n    print(\"✅ CSV saved to 'nsh_results.csv'\")\n    print(\"\\nAll experiments completed. Results saved.\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, BertTokenizer, BertForSequenceClassification, get_polynomial_decay_schedule_with_warmup\nfrom datasets import load_dataset\nfrom codecarbon import OfflineEmissionsTracker\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport pandas as pd\nimport time\nimport json\nimport random\nimport logging\nimport warnings\nfrom tqdm import tqdm\nfrom torch.optim import AdamW\nimport argparse # New: For easier experimentation\nfrom spikingjelly.activation_based import neuron\n\n\nfrom transformers import DistilBertConfig\n\n\n# --- STRATEGY 1: Import the highly optimized spikingjelly library ---\nfrom spikingjelly.activation_based import neuron\n# FIX: Import 'functional' with an alias to prevent name collisions.\nfrom spikingjelly.activation_based import functional as sj_functional\n\nfrom spikingjelly.activation_based import surrogate\n\n# --- SECTION: 1. CONFIGURATION ---\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nDEVICE_CONFIG = {\n    'optimize_for_gpu': True,\n    'mixed_precision': True\n}\nDEVICE = \"cuda\" if torch.cuda.is_available() and DEVICE_CONFIG['optimize_for_gpu'] else \"cpu\"\n\n# --- Constants ---\nDATASETS = {\n    'glue_sst2': {'name': 'glue', 'config': 'sst2', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_mrpc': {'name': 'glue', 'config': 'mrpc', 'split_train': 'train', 'split_val': 'validation'},\n    'glue_rte': {'name': 'glue', 'config': 'rte', 'split_train': 'train', 'split_val': 'validation'}\n}\nMAX_SAMPLES = 10000\nCARBON_INTENSITY = 250\nWATER_USAGE_FACTORS = {\"average_l_per_kwh\": 1.8}\n\n# --- Hyperparameters ---\nBATCH_SIZE = 16\nSEQ_LENGTH = 128\nNUM_LAYERS = 4\nNUM_EPOCHS = {'sst2': 5, 'mrpc': 5, 'rte': 3}\nLEARNING_RATE = 3e-5\nWEIGHT_DECAY = 0.01\nDROPOUT_RATE = 0.1\nPATIENCE = 3\nGRADIENT_ACCUMULATION_STEPS = 2\n\n# --- SECTION: 2. UTILITY FUNCTIONS ---\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef get_device():\n    device = torch.device(DEVICE)\n    logger.info(f\"✓ Using {'GPU: ' + torch.cuda.get_device_name(0) if DEVICE == 'cuda' else 'CPU'}\")\n    return device\n\n# --- SECTION: 3. ADVANCED NEUROMORPHIC & SPARSE LAYERS ---\ndef gumbel_top_k_select(scores, k, temperature=1.0):\n    gumbel_noise = -torch.log(-torch.log(torch.rand_like(scores) + 1e-10) + 1e-10)\n    perturbed_scores = (scores + gumbel_noise) / temperature\n    _, top_k_indices = torch.topk(perturbed_scores, k=k, dim=-1)\n    mask = torch.zeros_like(scores, dtype=torch.bool).scatter_(-1, top_k_indices, 1)\n    return mask\n\nclass Heaviside(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return (x > 0).float()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, = ctx.saved_tensors\n        grad = grad_output * torch.sigmoid(x) * (1 - torch.sigmoid(x))\n        return grad\n\nclass LIFLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, threshold=1.0, decay=0.9):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n        self.threshold = threshold\n        self.decay = decay\n        self.mem = None\n\n    def forward(self, x):\n        if self.mem is None or self.mem.shape != x.shape:\n            self.mem = torch.zeros_like(x, device=x.device)\n        current = self.linear(x)\n        self.mem = self.decay * self.mem + current\n        spikes = Heaviside.apply(self.mem - self.threshold)\n        self.mem = self.mem * (1 - spikes)  # Reset membrane where spikes occur\n        return spikes\n\n    def reset(self):\n        self.mem = None\n\nclass BigBirdStyleAttention(nn.Module):\n    def __init__(self, hidden_dim, num_heads, window_size=12, random_k=2, global_tokens=1, seq_len=128):\n        super().__init__()\n        self.hidden_dim, self.num_heads = hidden_dim, num_heads\n        self.head_dim = hidden_dim // num_heads\n        self.window_size, self.random_k, self.global_tokens = window_size, random_k, global_tokens\n        self.qkv_fused_linear = nn.Linear(hidden_dim, 3 * hidden_dim)\n        self.output_layer = nn.Linear(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n        self.static_mask = self._create_static_mask(seq_len, window_size, global_tokens)\n\n    def _create_static_mask(self, seq_len, window_size, global_tokens):\n        window_mask = torch.ones(seq_len, seq_len).triu(window_size + 1) + torch.ones(seq_len, seq_len).tril(-window_size - 1)\n        global_mask = torch.ones(seq_len, seq_len)\n        global_mask[:global_tokens, :] = 0\n        global_mask[:, :global_tokens] = 0\n        static_mask = (window_mask + global_mask) > 0\n        return static_mask.unsqueeze(0).unsqueeze(1).to(torch.bool)\n\n    def forward(self, x, attn_mask=None, output_attentions=False):\n        batch_size, seq_len, _ = x.shape\n        qkv = self.qkv_fused_linear(x)\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        attention_scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.head_dim)\n        if self.static_mask.device != x.device:\n            self.static_mask = self.static_mask.to(x.device)\n\n        random_mask_bool = gumbel_top_k_select(torch.randn_like(attention_scores), k=self.random_k)\n        final_mask = self.static_mask.expand(batch_size, self.num_heads, -1, -1) | random_mask_bool\n\n        if attn_mask is not None:\n            final_mask = final_mask | attn_mask\n\n        mask_value = torch.finfo(attention_scores.dtype).min\n        attention_scores = attention_scores.masked_fill(final_mask, mask_value)\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        attention_weights = self.dropout(attention_weights)\n        attention_output = torch.matmul(attention_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n        output = self.output_layer(attention_output)\n        return (output,) if not output_attentions else (output, attention_weights)\n\nclass NSH_DistilBertLayer(nn.Module):\n    def __init__(self, original_layer, seq_len=128):\n        super().__init__()\n        attn_module = original_layer.attention\n        hidden_dim = attn_module.q_lin.in_features\n        self.sparse_attention = BigBirdStyleAttention(hidden_dim, attn_module.n_heads, seq_len=seq_len)\n        self.ffn = original_layer.ffn\n        self.sa_layer_norm = original_layer.sa_layer_norm\n        self.output_layer_norm = original_layer.output_layer_norm\n        self.snn_layer = LIFLayer(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n\n    def forward(self, x, attn_mask=None, head_mask=None, output_attentions=False):\n        boolean_attn_mask = None\n        if attn_mask is not None:\n            boolean_attn_mask = (attn_mask == 0).unsqueeze(1).unsqueeze(2)\n        normed_x = self.sa_layer_norm(x)\n        attn_output_tuple = self.sparse_attention(normed_x, attn_mask=boolean_attn_mask, output_attentions=output_attentions)\n        spiking_output = self.snn_layer(attn_output_tuple[0])\n        x = x + self.dropout(spiking_output)\n        normed_x2 = self.output_layer_norm(x)\n        ffn_output = self.ffn(normed_x2)\n        x = x + self.dropout(ffn_output)\n        return (x,) if not output_attentions else (x, attn_output_tuple[1])\n\nclass NSH_BertLayer(nn.Module):\n    def __init__(self, original_layer, seq_len=128):\n        super().__init__()\n        attn_module = original_layer.attention.self\n        hidden_dim = attn_module.query.in_features\n        self.sparse_attention = BigBirdStyleAttention(hidden_dim, attn_module.num_attention_heads, seq_len=seq_len)\n        self.attention_output_dense = original_layer.attention.output\n        self.intermediate = original_layer.intermediate\n        self.output = original_layer.output\n        self.snn_layer = LIFLayer(hidden_dim, hidden_dim)\n        self.dropout = nn.Dropout(DROPOUT_RATE)\n\n    def forward(self, hidden_states, attention_mask=None, **kwargs):\n        output_attentions = kwargs.get('output_attentions', False)\n        boolean_attn_mask = None\n        if attention_mask is not None:\n            boolean_attn_mask = (attention_mask < 0)\n        attn_output_tuple = self.sparse_attention(hidden_states, attn_mask=boolean_attn_mask, output_attentions=output_attentions)\n        spiking_output = self.snn_layer(attn_output_tuple[0])\n        attention_output = self.attention_output_dense(self.dropout(spiking_output), hidden_states)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(self.dropout(intermediate_output), attention_output)\n        return (layer_output,) if not output_attentions else (layer_output, attn_output_tuple[1])\n\n# --- SECTION: 4. MODEL BUILDING ---\ndef build_baseline_model(device, model_type=\"distilbert\"):\n    logger.info(f\"Building Baseline {model_type.capitalize()} Model...\")\n    model_class = DistilBertForSequenceClassification if model_type == \"distilbert\" else BertForSequenceClassification\n    model = model_class.from_pretrained(f\"{model_type}-base-uncased\", num_labels=2).to(device)\n    logger.info(f\"Successfully created baseline {model_type.capitalize()} model.\")\n    return model\n\ndef build_nsh_model(device, model_type=\"distilbert\", num_layers=1, seq_len=128):\n    logger.info(f\"Building NSH {model_type.capitalize()} Model with {num_layers} layer(s)...\")\n    model_class = DistilBertForSequenceClassification if model_type == \"distilbert\" else BertForSequenceClassification\n    model = model_class.from_pretrained(f\"{model_type}-base-uncased\", num_labels=2)\n    if model_type == \"distilbert\":\n        for i in range(min(num_layers, len(model.distilbert.transformer.layer))):\n            model.distilbert.transformer.layer[i] = NSH_DistilBertLayer(model.distilbert.transformer.layer[i], seq_len)\n    else:\n        for i in range(min(num_layers, len(model.bert.encoder.layer))):\n            model.bert.encoder.layer[i] = NSH_BertLayer(model.bert.encoder.layer[i], seq_len)\n    model.to(device)\n    logger.info(f\"Successfully created NSH {model_type.capitalize()} model.\")\n    return model\n\n# --- SECTION: 5. DATA LOADING ---\ndef get_dataloaders(model_type, seq_len, batch_size):\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') if model_type == 'distilbert' else BertTokenizer.from_pretrained('bert-base-uncased')\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    preprocess_map = {\n        'glue_sst2': lambda exs: tokenizer(exs[\"sentence\"], padding=\"max_length\", max_length=seq_len, truncation=True),\n        'glue_mrpc': lambda exs: tokenizer(exs[\"sentence1\"], exs[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True),\n        'glue_rte': lambda exs: tokenizer(exs[\"sentence1\"], exs[\"sentence2\"], padding=\"max_length\", max_length=seq_len, truncation=True)\n    }\n    dataloaders = {'train': {}, 'validation': {}}\n    for dataset_name, dataset_info in DATASETS.items():\n        train_max_samples = 2490 if dataset_name == 'glue_rte' else (10000 if dataset_name == 'glue_sst2' else MAX_SAMPLES)\n        val_max_samples = 277 if dataset_name == 'glue_rte' else MAX_SAMPLES\n        train_dataset = load_dataset(dataset_info['name'], dataset_info['config'], split=dataset_info['split_train'])\n        val_dataset = load_dataset(dataset_info['name'], dataset_info['config'], split=dataset_info['split_val'])\n        train_dataset = train_dataset.select(range(min(len(train_dataset), train_max_samples)))\n        val_dataset = val_dataset.select(range(min(len(val_dataset), val_max_samples)))\n        train_dataset = train_dataset.map(preprocess_map[dataset_name], batched=True)\n        val_dataset = val_dataset.map(preprocess_map[dataset_name], batched=True)\n        train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n        val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n        train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n        val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n        dataloaders['train'][dataset_name] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        dataloaders['validation'][dataset_name] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    logger.info(f\"Dataloaders created for {model_type}: {', '.join(dataloaders['validation'].keys())}\")\n    return dataloaders\n\n# --- SECTION: 6. TRAINING & EVALUATION ---\ndef evaluate_classification(model, dataloader, device):\n    model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            with torch.cuda.amp.autocast(enabled=DEVICE_CONFIG['mixed_precision']):\n                outputs = model(**batch)\n            total_loss += outputs.loss.item()\n            preds = torch.argmax(outputs.logits, dim=-1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n    return {\n        \"accuracy\": accuracy_score(all_labels, all_preds),\n        \"f1\": f1_score(all_labels, all_preds, average='weighted'),\n        \"val_loss\": total_loss / len(dataloader)\n    }\n\ndef fine_tune(model, train_dataloader, val_dataloader, device, task_name, epochs, debug=False):\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_dataloader) * epochs[task_name] // GRADIENT_ACCUMULATION_STEPS\n    scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps)\n    scaler = torch.cuda.amp.GradScaler(enabled=DEVICE_CONFIG['mixed_precision'])\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(epochs.get(task_name, 3)):\n        logger.info(f\"Epoch {epoch+1}/{epochs.get(task_name, 3)} for {task_name}\")\n        model.train()\n        for module in model.modules():\n            if hasattr(module, 'reset'):\n                module.reset()\n        total_train_loss = 0\n        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Training Epoch {epoch+1}\")\n\n        for i, batch in progress_bar:\n            inputs = {k: v.to(device) for k, v in batch.items()}\n            with torch.cuda.amp.autocast(enabled=DEVICE_CONFIG['mixed_precision']):\n                outputs = model(**inputs)\n                loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n            try:\n                scaler.scale(loss).backward(retain_graph=(i + 1) % GRADIENT_ACCUMULATION_STEPS != 0)\n            except Exception as e:\n                if debug:\n                    logger.error(f\"Backward pass failed at batch {i}: {e}\")\n                raise\n            if (i + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                scaler.step(optimizer)\n                scaler.update()\n                scheduler.step()\n                optimizer.zero_grad(set_to_none=True)\n                for module in model.modules():\n                    if hasattr(module, 'reset'):\n                        module.reset()\n            total_train_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n            progress_bar.set_postfix({'loss': total_train_loss / (i + 1)})\n\n        val_metrics = evaluate_classification(model, val_dataloader, device)\n        logger.info(f\"Validation -> Loss: {val_metrics['val_loss']:.4f}, Acc: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n        if val_metrics['val_loss'] < best_val_loss:\n            best_val_loss = val_metrics['val_loss']\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                logger.info(f\"Early stopping triggered after {epoch+1} epochs.\")\n                break\n    return model\n\n# --- SECTION: 7. EXPERIMENT RUNNER ---\ndef run_experiment(model, model_name, dataloaders, device, batch_size, seq_len, tasks_to_run):\n    results = {\n        'model_name': model_name,\n        'batch_size': batch_size,\n        'seq_length': seq_len,\n        'accuracy_metrics': {},\n        'performance_metrics': {}\n    }\n\n    print(f\"\\n--- 🚀 Measuring Metrics for {model_name} ---\")\n    start_time = time.time()\n    num_queries = 0\n    #tracker = OfflineEmissionsTracker(project_name=f\"exp_{model_name.replace(' ', '_')}\", output_dir=\"emissions\", country_iso_code=\"USA\", log_level=\"warning\")\n    tracker = OfflineEmissionsTracker(\n    project_name=f\"Experiment_{model_name.replace(' ', '_')}\",\n    measure_power_secs=1,\n    output_dir=\".\",\n    log_level='info',\n    gpu_ids=[0],       # Track GPU ID 0 (your Tesla T4)\n    # cpu_power=False    # Optional: disable CPU measurement for accurate GPU tracking\n)\n    tracker.start()\n    \n    try:\n        for task in tasks_to_run:\n            print(f\"\\n--- Running task: {task} ---\")\n            task_epochs = {'sst2': NUM_EPOCHS['sst2'], 'mrpc': NUM_EPOCHS['mrpc'], 'rte': NUM_EPOCHS['rte']}\n            model = fine_tune(model, dataloaders['train'][f'glue_{task}'], dataloaders['validation'][f'glue_{task}'], device, task, task_epochs)\n            \n            print(f\"Final evaluation on GLUE {task}...\")\n            metrics = evaluate_classification(model, dataloaders['validation'][f'glue_{task}'], device)\n            results['accuracy_metrics'][f'{task}_accuracy'] = metrics['accuracy']\n            results['accuracy_metrics'][f'{task}_f1'] = metrics['f1']\n            num_queries += len(dataloaders['validation'][f'glue_{task}'].dataset)\n            print(f\"✓ {task.upper()} Results -> Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}\")\n\n    except Exception as e:\n        print(f\"🚨 Experiment failed for {model_name}: {e}\")\n    finally:\n        emissions_kwh = tracker.stop() or 0.0\n\n    total_duration_s = time.time() - start_time\n    total_tokens_processed = num_queries * seq_len\n    total_carbon_g = emissions_kwh * CARBON_INTENSITY\n\n    results['performance_metrics'] = {\n        'latency_ms_query': (total_duration_s / num_queries) * 1000 if num_queries > 0 else 0,\n        'throughput_tokens_sec': total_tokens_processed / total_duration_s if total_duration_s > 0 else 0,\n        'energy_wh_token': (emissions_kwh * 1000) / total_tokens_processed if total_tokens_processed > 0 else 0,\n         'wue_avg_liters_query': (emissions_kwh * WATER_USAGE_FACTORS['average_l_per_kwh']) / num_queries if num_queries > 0 else 0,\n        'sci_gco2e_query': total_carbon_g / num_queries if num_queries > 0 else 0,\n        'total_emissions_kgco2eq': total_carbon_g / 1000,\n        'total_energy_kwh': emissions_kwh\n    }\n    \n    print(f\"\\n--- ✅ Final Results for {model_name} ---\")\n    print(json.dumps(results, indent=2))\n    return results\n\n# --- SECTION: 8. MAIN EXECUTION & PROFILING ---\ndef main(args):\n    DEVICE = get_device()\n    set_seed(42)\n\n    dataloaders = get_dataloaders(model_type=args.model_type, seq_len=args.seq_len, batch_size=args.batch_size)\n    \n    # --- MODEL SELECTION ---\n\n    # === DISTILBERT BRANCH ===\n    # Uncomment this block to run DistilBERT models\n    if args.model_type == 'distilbert':\n    #     \n        model = build_nsh_model(DEVICE, 'distilbert', args.num_layers, args.seq_len)\n        model_name = \"NSH_DistilBERT\"\n    #     else:\n    #         model = build_baseline_model(DEVICE, 'distilbert')\n    #         model_name = \"Baseline_DistilBERT\"\n\n    # === BERT BRANCH ===\n    # Uncomment this block to run BERT models\n    # if args.model_type == 'bert':\n        \n    #     model = build_nsh_model(DEVICE, 'bert', args.num_layers, args.seq_len)\n    #     model_name = \"NSH_BERT\"\n    \n        # else:\n        #     model = build_baseline_model(DEVICE, 'bert')\n        #     model_name = \"Baseline_BERT\"\n    # --- STRATEGY 2: Add torch.compile() for a massive speedup ---\n    if args.use_compile:\n        print(\"\\n🔥 Activating torch.compile() for optimized performance...\")\n        # Note: 'max-autotune' is best for static shapes, which we have.\n        model = torch.compile(model, mode=\"max-autotune\")\n    # Safety check: if no model was selected\n    if 'model' not in locals():\n        raise ValueError(\"No model was selected! Please uncomment a model branch in main().\")\n\n    # --- PROFILING SECTION ---\n    if args.profile:\n        print(\"\\n--- 🔬 Starting Profiler ---\")\n        sample_batch = next(iter(dataloaders['train']['glue_sst2']))\n        sample_batch = {k: v.to(DEVICE) for k, v in sample_batch.items()}\n\n        with torch.profiler.profile(\n            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n            record_shapes=True,\n            profile_memory=True,\n            with_stack=True\n        ) as prof:\n            with torch.profiler.record_function(\"model_inference\"):\n                outputs = model(**sample_batch)\n                loss = outputs.loss\n                loss.backward()\n\n        print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=15))\n        print(\"--- Profiling Complete ---\")\n        return  # Exit after profiling\n\n  \n\n    # --- Full Experiment ---\n    all_results = []\n    tasks = [t.strip() for t in args.tasks.split(',')]\n    results = run_experiment(model=model, model_name=model_name, dataloaders=dataloaders, device=DEVICE,\n                             batch_size=args.batch_size, seq_len=args.seq_len, tasks_to_run=tasks)\n    all_results.append(results)\n    \n    output_filename = \"nsh_results\"\n    with open(f'{output_filename}.json', 'w') as f:\n        json.dump(all_results, f, indent=2)\n    df = pd.json_normalize(all_results, sep='_')\n    df.to_csv(f'{output_filename}.csv', index=False)\n    print(f\"\\n✅ All experiments completed. Results saved to '{output_filename}.json' and '{output_filename}.csv'\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Run Neuromorphic Sparse Transformer Experiments\")\n    #parser.add_argument(\"--model_type\", type=str, default=\"distilbert\", choices=[\"distilbert\", \"bert\"], help=\"Base model architecture.\")\n    parser.add_argument(\"--model_type\", type=str, default=\"distilbert\", choices=[\"distilbert\", \"bert\"])\n\n    parser.add_argument(\"--is_nsh\", action=\"store_true\", help=\"Flag to use the NSH model instead of the baseline.\")\n    parser.add_argument(\"--tasks\", type=str, default=\"sst2,mrpc,rte\", help=\"Comma-separated list of tasks to run (e.g., 'sst2,mrpc').\")\n    parser.add_argument(\"--batch_size\", type=int, default=BATCH_SIZE, help=\"Batch size for training and evaluation.\")\n    parser.add_argument(\"--seq_len\", type=int, default=SEQ_LENGTH, help=\"Sequence length for tokenization.\")\n    parser.add_argument(\"--num_layers\", type=int, default=NUM_LAYERS, help=\"Number of layers to replace with NSH layers.\")\n    parser.add_argument(\"--profile\", action=\"store_true\", help=\"Run profiler for one batch and exit.\")\n    parser.add_argument(\"--use_compile\", action=\"store_true\", help=\"Enable torch.compile for optimization.\")\n    \n    #args = parser.parse_args()\n    # Ignore unknown arguments (like '-f' injected by Jupyter)\n    args, _ = parser.parse_known_args()\n    main(args)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T12:34:55.005010Z","iopub.execute_input":"2025-09-25T12:34:55.005902Z","execution_failed":"2025-09-25T12:58:54.985Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779df77b6e9741e5a994256f9a4d7c96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08db703268a4191b6e5f3919cbd3285"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"048422f1673744ffbf298bd0d5435381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02479684cd1440e3a41a647dbf1e3d12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe2a55ec52a24bdbaab5ec1ff3d6da32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c2872397ad45f3b3f6d6a189922f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17393ec1dd98480cb7ef305b35cdd339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e511c981a0341379182eb5b7c5de4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48304e9bded84d77a1fbb729e42302d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd6f5f0906f46f4bb7e41cfdc5fdb26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ef3a73d3c3f48829bc68dc8a84daf23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"661ad50ac13a4bf2b0dd4f888cc310bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mrpc/train-00000-of-00001.parquet:   0%|          | 0.00/649k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de012d1416cf4a488dca2a27d12cf563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mrpc/validation-00000-of-00001.parquet:   0%|          | 0.00/75.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1ea15c8c9a486a9abcaf3e65f2d19d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"mrpc/test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d8febcee9445719779248404aa8322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1651ce343ea3428f9e45babc144e76fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c30aa7215ef496183dd469db6d94d6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"124034153eeb4ad3b64cebb7cf567a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"366317fa57af4feb82ebfc17b1e81089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a281a62260b458f82632a4ac1584343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rte/train-00000-of-00001.parquet:   0%|          | 0.00/584k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb6154f6927140ca8409844ae7512a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rte/validation-00000-of-00001.parquet:   0%|          | 0.00/69.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94c1a788d1e1483abfb109e22f7ab13b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rte/test-00000-of-00001.parquet:   0%|          | 0.00/621k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ddcc3c18c784256843365b790376723"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e264a5b50494795a68449febf55b71e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ee4e88ab6d490e8bbf8b7c40d6d188"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ff2254d6704195acaa8806c74a8fbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c2756197b34790b575a308e012187e"}},"metadata":{}},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbca6508c922465bbfc6ec8ecf0e591e"}},"metadata":{}},{"name":"stderr","text":"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d7dd239bba48459d1ccca197c0bf20"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[codecarbon WARNING @ 12:35:25] Multiple instances of codecarbon are allowed to run at the same time.\n[codecarbon INFO @ 12:35:25] [setup] RAM Tracking...\n[codecarbon INFO @ 12:35:25] [setup] CPU Tracking...\n","output_type":"stream"},{"name":"stdout","text":"\n--- 🚀 Measuring Metrics for NSH_DistilBERT ---\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon WARNING @ 12:35:26] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon WARNING @ 12:35:26] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n Linux OS detected: Please ensure RAPL files exist at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n\n[codecarbon INFO @ 12:35:26] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon WARNING @ 12:35:26] No CPU tracking mode found. Falling back on CPU constant mode.\n[codecarbon INFO @ 12:35:26] [setup] GPU Tracking...\n[codecarbon INFO @ 12:35:26] Tracking Nvidia GPU via pynvml\n[codecarbon WARNING @ 12:35:26] You have 2 GPUs but we will monitor only 1 ([0]) of them. Check your configuration.\n[codecarbon INFO @ 12:35:26] The below tracking methods have been set up:\n                RAM Tracking Method: RAM power estimation model\n                CPU Tracking Method: global constant\n                GPU Tracking Method: pynvml\n            \n[codecarbon INFO @ 12:35:26] >>> Tracker's metadata:\n[codecarbon INFO @ 12:35:26]   Platform system: Linux-6.6.56+-x86_64-with-glibc2.35\n[codecarbon INFO @ 12:35:26]   Python version: 3.11.13\n[codecarbon INFO @ 12:35:26]   CodeCarbon version: 3.0.5\n[codecarbon INFO @ 12:35:26]   Available RAM : 31.350 GB\n[codecarbon INFO @ 12:35:26]   CPU count: 4 thread(s) in 1 physical CPU(s)\n[codecarbon INFO @ 12:35:26]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 12:35:26]   GPU count: 1\n[codecarbon INFO @ 12:35:26]   GPU model: 2 x Tesla T4 BUT only tracking these GPU ids : ['0']\n[codecarbon INFO @ 12:35:26] Emissions data (if any) will be saved to file /kaggle/working/emissions.csv\n","output_type":"stream"},{"name":"stdout","text":"\n--- Running task: sst2 ---\n","output_type":"stream"},{"name":"stderr","text":"[codecarbon INFO @ 12:35:27] Energy consumed for RAM : 0.000006 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:27] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:27] Energy consumed for All CPU : 0.000012 kWh\n[codecarbon INFO @ 12:35:27] Energy consumed for all GPUs : 0.000007 kWh. Total GPU Power : 25.729004702349428 W\n[codecarbon INFO @ 12:35:28] 0.000025 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:35:28] Energy consumed for RAM : 0.000011 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:28] Delta energy consumed for CPU with constant : 0.000011 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:28] Energy consumed for All CPU : 0.000023 kWh\n[codecarbon INFO @ 12:35:28] Energy consumed for all GPUs : 0.000015 kWh. Total GPU Power : 28.84172936300526 W\n[codecarbon INFO @ 12:35:28] 0.000049 kWh of electricity used since the beginning.\nTraining Epoch 1:   0%|          | 0/625 [00:00<?, ?it/s][codecarbon INFO @ 12:35:29] Energy consumed for RAM : 0.000016 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:29] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:29] Energy consumed for All CPU : 0.000035 kWh\n[codecarbon INFO @ 12:35:29] Energy consumed for all GPUs : 0.000022 kWh. Total GPU Power : 27.044768624130814 W\n[codecarbon INFO @ 12:35:29] 0.000074 kWh of electricity used since the beginning.\nTraining Epoch 1:   0%|          | 1/625 [00:01<12:30,  1.20s/it, loss=0.709][codecarbon INFO @ 12:35:30] Energy consumed for RAM : 0.000022 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:30] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:30] Energy consumed for All CPU : 0.000047 kWh\n[codecarbon INFO @ 12:35:30] Energy consumed for all GPUs : 0.000030 kWh. Total GPU Power : 28.922821640536537 W\n[codecarbon INFO @ 12:35:30] 0.000099 kWh of electricity used since the beginning.\nTraining Epoch 1:   1%|▏         | 8/625 [00:02<01:41,  6.06it/s, loss=0.695][codecarbon INFO @ 12:35:31] Energy consumed for RAM : 0.000027 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:31] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:31] Energy consumed for All CPU : 0.000058 kWh\n[codecarbon INFO @ 12:35:31] Energy consumed for all GPUs : 0.000044 kWh. Total GPU Power : 49.8602826756582 W\n[codecarbon INFO @ 12:35:31] 0.000130 kWh of electricity used since the beginning.\nTraining Epoch 1:   3%|▎         | 20/625 [00:03<00:54, 11.17it/s, loss=0.693][codecarbon INFO @ 12:35:32] Energy consumed for RAM : 0.000033 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:32] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:32] Energy consumed for All CPU : 0.000070 kWh\n[codecarbon INFO @ 12:35:32] Energy consumed for all GPUs : 0.000063 kWh. Total GPU Power : 69.82304229122302 W\n[codecarbon INFO @ 12:35:32] 0.000166 kWh of electricity used since the beginning.\nTraining Epoch 1:   5%|▌         | 32/625 [00:04<00:49, 12.09it/s, loss=0.696][codecarbon INFO @ 12:35:33] Energy consumed for RAM : 0.000038 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:33] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:33] Energy consumed for All CPU : 0.000082 kWh\n[codecarbon INFO @ 12:35:33] Energy consumed for all GPUs : 0.000083 kWh. Total GPU Power : 70.13874173691899 W\n[codecarbon INFO @ 12:35:33] 0.000203 kWh of electricity used since the beginning.\nTraining Epoch 1:   7%|▋         | 44/625 [00:05<00:47, 12.18it/s, loss=0.692][codecarbon INFO @ 12:35:34] Energy consumed for RAM : 0.000044 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:34] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:34] Energy consumed for All CPU : 0.000093 kWh\n[codecarbon INFO @ 12:35:34] Energy consumed for all GPUs : 0.000102 kWh. Total GPU Power : 69.42194668048408 W\n[codecarbon INFO @ 12:35:34] 0.000239 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:35:34] We do not have data for None, using world average.\n[codecarbon INFO @ 12:35:34] 0.014126 g.CO2eq/s mean an estimation of 445.4628942414763 kg.CO2eq/year\nTraining Epoch 1:   9%|▉         | 56/625 [00:06<00:46, 12.18it/s, loss=0.69] [codecarbon INFO @ 12:35:35] Energy consumed for RAM : 0.000049 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:35] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:35] Energy consumed for All CPU : 0.000105 kWh\n[codecarbon INFO @ 12:35:35] Energy consumed for all GPUs : 0.000121 kWh. Total GPU Power : 70.11042059958375 W\n[codecarbon INFO @ 12:35:35] 0.000276 kWh of electricity used since the beginning.\nTraining Epoch 1:  11%|█         | 68/625 [00:07<00:45, 12.20it/s, loss=0.689][codecarbon INFO @ 12:35:36] Energy consumed for RAM : 0.000055 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:36] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:36] Energy consumed for All CPU : 0.000117 kWh\n[codecarbon INFO @ 12:35:36] Energy consumed for all GPUs : 0.000140 kWh. Total GPU Power : 69.49603251565136 W\n[codecarbon INFO @ 12:35:36] 0.000312 kWh of electricity used since the beginning.\nTraining Epoch 1:  13%|█▎        | 82/625 [00:08<00:44, 12.17it/s, loss=0.69] [codecarbon INFO @ 12:35:37] Energy consumed for RAM : 0.000060 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:37] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:37] Energy consumed for All CPU : 0.000129 kWh\n[codecarbon INFO @ 12:35:37] Energy consumed for all GPUs : 0.000158 kWh. Total GPU Power : 62.563073232328804 W\n[codecarbon INFO @ 12:35:37] 0.000347 kWh of electricity used since the beginning.\nTraining Epoch 1:  15%|█▌        | 94/625 [00:09<00:43, 12.14it/s, loss=0.69] [codecarbon INFO @ 12:35:38] Energy consumed for RAM : 0.000066 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:38] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:38] Energy consumed for All CPU : 0.000140 kWh\n[codecarbon INFO @ 12:35:38] Energy consumed for all GPUs : 0.000177 kWh. Total GPU Power : 69.5895048841753 W\n[codecarbon INFO @ 12:35:38] 0.000383 kWh of electricity used since the beginning.\nTraining Epoch 1:  17%|█▋        | 106/625 [00:10<00:42, 12.19it/s, loss=0.689][codecarbon INFO @ 12:35:39] Energy consumed for RAM : 0.000071 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:39] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:39] Energy consumed for All CPU : 0.000152 kWh\n[codecarbon INFO @ 12:35:39] Energy consumed for all GPUs : 0.000196 kWh. Total GPU Power : 69.6049925912286 W\n[codecarbon INFO @ 12:35:39] 0.000419 kWh of electricity used since the beginning.\nTraining Epoch 1:  19%|█▉        | 118/625 [00:11<00:41, 12.14it/s, loss=0.688][codecarbon INFO @ 12:35:40] Energy consumed for RAM : 0.000077 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:40] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:40] Energy consumed for All CPU : 0.000164 kWh\n[codecarbon INFO @ 12:35:40] Energy consumed for all GPUs : 0.000215 kWh. Total GPU Power : 68.94648850827271 W\n[codecarbon INFO @ 12:35:40] 0.000456 kWh of electricity used since the beginning.\nTraining Epoch 1:  21%|██        | 130/625 [00:12<00:40, 12.10it/s, loss=0.689][codecarbon INFO @ 12:35:41] Energy consumed for RAM : 0.000082 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:41] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:41] Energy consumed for All CPU : 0.000175 kWh\n[codecarbon INFO @ 12:35:41] Energy consumed for all GPUs : 0.000234 kWh. Total GPU Power : 69.84356856145547 W\nTraining Epoch 1:  21%|██        | 130/625 [00:12<00:40, 12.10it/s, loss=0.689][codecarbon INFO @ 12:35:41] 0.000492 kWh of electricity used since the beginning.\nTraining Epoch 1:  23%|██▎       | 142/625 [00:13<00:39, 12.09it/s, loss=0.688][codecarbon INFO @ 12:35:42] Energy consumed for RAM : 0.000088 kWh. RAM Power : 20.0 W\nTraining Epoch 1:  23%|██▎       | 142/625 [00:13<00:39, 12.09it/s, loss=0.688][codecarbon INFO @ 12:35:42] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:42] Energy consumed for All CPU : 0.000187 kWh\n[codecarbon INFO @ 12:35:42] Energy consumed for all GPUs : 0.000253 kWh. Total GPU Power : 69.6062019055078 W\n[codecarbon INFO @ 12:35:42] 0.000528 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:35:42] We do not have data for None, using world average.\n[codecarbon INFO @ 12:35:42] 0.017169 g.CO2eq/s mean an estimation of 541.4429339718864 kg.CO2eq/year\nTraining Epoch 1:  25%|██▍       | 154/625 [00:14<00:39, 12.06it/s, loss=0.688][codecarbon INFO @ 12:35:43] Energy consumed for RAM : 0.000093 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:43] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:43] Energy consumed for All CPU : 0.000199 kWh\n[codecarbon INFO @ 12:35:43] Energy consumed for all GPUs : 0.000272 kWh. Total GPU Power : 69.27056406257104 W\n[codecarbon INFO @ 12:35:43] 0.000565 kWh of electricity used since the beginning.\nTraining Epoch 1:  27%|██▋       | 166/625 [00:15<00:37, 12.11it/s, loss=0.687][codecarbon INFO @ 12:35:44] Energy consumed for RAM : 0.000099 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:44] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:44] Energy consumed for All CPU : 0.000210 kWh\n[codecarbon INFO @ 12:35:44] Energy consumed for all GPUs : 0.000292 kWh. Total GPU Power : 69.40224797480583 W\n[codecarbon INFO @ 12:35:44] 0.000601 kWh of electricity used since the beginning.\nTraining Epoch 1:  28%|██▊       | 178/625 [00:16<00:36, 12.11it/s, loss=0.688][codecarbon INFO @ 12:35:45] Energy consumed for RAM : 0.000104 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:45] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:45] Energy consumed for All CPU : 0.000222 kWh\n[codecarbon INFO @ 12:35:45] Energy consumed for all GPUs : 0.000309 kWh. Total GPU Power : 61.94460660898423 W\n[codecarbon INFO @ 12:35:45] 0.000635 kWh of electricity used since the beginning.\nTraining Epoch 1:  30%|███       | 190/625 [00:17<00:35, 12.10it/s, loss=0.688][codecarbon INFO @ 12:35:46] Energy consumed for RAM : 0.000110 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:46] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:46] Energy consumed for All CPU : 0.000234 kWh\n[codecarbon INFO @ 12:35:46] Energy consumed for all GPUs : 0.000328 kWh. Total GPU Power : 69.58149412359424 W\n[codecarbon INFO @ 12:35:46] 0.000671 kWh of electricity used since the beginning.\nTraining Epoch 1:  32%|███▏      | 202/625 [00:18<00:34, 12.09it/s, loss=0.689][codecarbon INFO @ 12:35:47] Energy consumed for RAM : 0.000115 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:47] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:47] Energy consumed for All CPU : 0.000245 kWh\n[codecarbon INFO @ 12:35:47] Energy consumed for all GPUs : 0.000347 kWh. Total GPU Power : 69.23040617436922 W\n[codecarbon INFO @ 12:35:47] 0.000708 kWh of electricity used since the beginning.\nTraining Epoch 1:  34%|███▍      | 214/625 [00:19<00:33, 12.11it/s, loss=0.689][codecarbon INFO @ 12:35:48] Energy consumed for RAM : 0.000121 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:48] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:48] Energy consumed for All CPU : 0.000257 kWh\n[codecarbon INFO @ 12:35:48] Energy consumed for all GPUs : 0.000366 kWh. Total GPU Power : 69.34237981958637 W\n[codecarbon INFO @ 12:35:48] 0.000744 kWh of electricity used since the beginning.\nTraining Epoch 1:  36%|███▌      | 226/625 [00:20<00:32, 12.09it/s, loss=0.689][codecarbon INFO @ 12:35:49] Energy consumed for RAM : 0.000126 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:49] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:49] Energy consumed for All CPU : 0.000269 kWh\n[codecarbon INFO @ 12:35:49] Energy consumed for all GPUs : 0.000385 kWh. Total GPU Power : 69.3026355679134 W\n[codecarbon INFO @ 12:35:49] 0.000780 kWh of electricity used since the beginning.\nTraining Epoch 1:  38%|███▊      | 238/625 [00:21<00:32, 12.09it/s, loss=0.689][codecarbon INFO @ 12:35:50] Energy consumed for RAM : 0.000132 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:50] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:50] Energy consumed for All CPU : 0.000281 kWh\n[codecarbon INFO @ 12:35:50] Energy consumed for all GPUs : 0.000404 kWh. Total GPU Power : 69.11307842249883 W\n[codecarbon INFO @ 12:35:50] 0.000817 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:35:50] We do not have data for None, using world average.\n[codecarbon INFO @ 12:35:50] 0.017118 g.CO2eq/s mean an estimation of 539.8414149713675 kg.CO2eq/year\nTraining Epoch 1:  40%|████      | 250/625 [00:22<00:31, 12.04it/s, loss=0.689][codecarbon INFO @ 12:35:51] Energy consumed for RAM : 0.000137 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:51] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:51] Energy consumed for All CPU : 0.000292 kWh\n[codecarbon INFO @ 12:35:51] Energy consumed for all GPUs : 0.000423 kWh. Total GPU Power : 69.01143061902117 W\n[codecarbon INFO @ 12:35:51] 0.000853 kWh of electricity used since the beginning.\nTraining Epoch 1:  42%|████▏     | 262/625 [00:23<00:30, 12.00it/s, loss=0.689][codecarbon INFO @ 12:35:52] Energy consumed for RAM : 0.000143 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:52] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:52] Energy consumed for All CPU : 0.000304 kWh\n[codecarbon INFO @ 12:35:52] Energy consumed for all GPUs : 0.000442 kWh. Total GPU Power : 69.1477178133321 W\n[codecarbon INFO @ 12:35:52] 0.000889 kWh of electricity used since the beginning.\nTraining Epoch 1:  44%|████▍     | 274/625 [00:24<00:29, 11.98it/s, loss=0.69] [codecarbon INFO @ 12:35:53] Energy consumed for RAM : 0.000148 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:53] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:53] Energy consumed for All CPU : 0.000316 kWh\n[codecarbon INFO @ 12:35:53] Energy consumed for all GPUs : 0.000459 kWh. Total GPU Power : 61.81665600510541 W\n[codecarbon INFO @ 12:35:53] 0.000923 kWh of electricity used since the beginning.\nTraining Epoch 1:  46%|████▌     | 286/625 [00:25<00:28, 12.03it/s, loss=0.69][codecarbon INFO @ 12:35:54] Energy consumed for RAM : 0.000154 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:54] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:54] Energy consumed for All CPU : 0.000327 kWh\n[codecarbon INFO @ 12:35:54] Energy consumed for all GPUs : 0.000478 kWh. Total GPU Power : 69.1169679910059 W\n[codecarbon INFO @ 12:35:54] 0.000960 kWh of electricity used since the beginning.\nTraining Epoch 1:  48%|████▊     | 298/625 [00:26<00:27, 11.98it/s, loss=0.69][codecarbon INFO @ 12:35:55] Energy consumed for RAM : 0.000159 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:55] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:55] Energy consumed for All CPU : 0.000339 kWh\n[codecarbon INFO @ 12:35:55] Energy consumed for all GPUs : 0.000497 kWh. Total GPU Power : 68.87410180131216 W\n[codecarbon INFO @ 12:35:55] 0.000996 kWh of electricity used since the beginning.\nTraining Epoch 1:  50%|████▉     | 310/625 [00:27<00:26, 12.03it/s, loss=0.69][codecarbon INFO @ 12:35:56] Energy consumed for RAM : 0.000165 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:56] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:56] Energy consumed for All CPU : 0.000351 kWh\n[codecarbon INFO @ 12:35:56] Energy consumed for all GPUs : 0.000516 kWh. Total GPU Power : 68.92228277390653 W\n[codecarbon INFO @ 12:35:56] 0.001032 kWh of electricity used since the beginning.\nTraining Epoch 1:  52%|█████▏    | 322/625 [00:28<00:25, 11.86it/s, loss=0.69][codecarbon INFO @ 12:35:57] Energy consumed for RAM : 0.000170 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:57] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:57] Energy consumed for All CPU : 0.000363 kWh\n[codecarbon INFO @ 12:35:57] Energy consumed for all GPUs : 0.000535 kWh. Total GPU Power : 68.61011383806336 W\n[codecarbon INFO @ 12:35:57] 0.001068 kWh of electricity used since the beginning.\nTraining Epoch 1:  53%|█████▎    | 334/625 [00:29<00:24, 11.96it/s, loss=0.69][codecarbon INFO @ 12:35:58] Energy consumed for RAM : 0.000176 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:58] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:58] Energy consumed for All CPU : 0.000374 kWh\n[codecarbon INFO @ 12:35:58] Energy consumed for all GPUs : 0.000554 kWh. Total GPU Power : 68.66060471692438 W\n[codecarbon INFO @ 12:35:58] 0.001105 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:35:58] We do not have data for None, using world average.\n[codecarbon INFO @ 12:35:58] 0.017080 g.CO2eq/s mean an estimation of 538.6389933020145 kg.CO2eq/year\nTraining Epoch 1:  55%|█████▌    | 346/625 [00:30<00:23, 11.96it/s, loss=0.689][codecarbon INFO @ 12:35:59] Energy consumed for RAM : 0.000181 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:35:59] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:35:59] Energy consumed for All CPU : 0.000386 kWh\n[codecarbon INFO @ 12:35:59] Energy consumed for all GPUs : 0.000573 kWh. Total GPU Power : 69.15467675489411 W\n[codecarbon INFO @ 12:35:59] 0.001141 kWh of electricity used since the beginning.\nTraining Epoch 1:  57%|█████▋    | 358/625 [00:31<00:22, 11.97it/s, loss=0.688][codecarbon INFO @ 12:36:00] Energy consumed for RAM : 0.000187 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:00] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:00] Energy consumed for All CPU : 0.000398 kWh\n[codecarbon INFO @ 12:36:00] Energy consumed for all GPUs : 0.000592 kWh. Total GPU Power : 68.87640988219678 W\n[codecarbon INFO @ 12:36:00] 0.001177 kWh of electricity used since the beginning.\nTraining Epoch 1:  59%|█████▉    | 370/625 [00:32<00:21, 11.94it/s, loss=0.689][codecarbon INFO @ 12:36:01] Energy consumed for RAM : 0.000192 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:01] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:01] Energy consumed for All CPU : 0.000409 kWh\n[codecarbon INFO @ 12:36:01] Energy consumed for all GPUs : 0.000609 kWh. Total GPU Power : 61.47263799347167 W\n[codecarbon INFO @ 12:36:01] 0.001211 kWh of electricity used since the beginning.\nTraining Epoch 1:  61%|██████    | 382/625 [00:33<00:20, 11.92it/s, loss=0.69] [codecarbon INFO @ 12:36:02] Energy consumed for RAM : 0.000198 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:02] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:02] Energy consumed for All CPU : 0.000421 kWh\n[codecarbon INFO @ 12:36:02] Energy consumed for all GPUs : 0.000628 kWh. Total GPU Power : 68.46604511486677 W\n[codecarbon INFO @ 12:36:02] 0.001247 kWh of electricity used since the beginning.\nTraining Epoch 1:  63%|██████▎   | 394/625 [00:34<00:19, 11.90it/s, loss=0.689][codecarbon INFO @ 12:36:03] Energy consumed for RAM : 0.000203 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:03] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:03] Energy consumed for All CPU : 0.000433 kWh\n[codecarbon INFO @ 12:36:03] Energy consumed for all GPUs : 0.000647 kWh. Total GPU Power : 68.77080466281453 W\n[codecarbon INFO @ 12:36:03] 0.001283 kWh of electricity used since the beginning.\nTraining Epoch 1:  65%|██████▍   | 406/625 [00:35<00:18, 11.95it/s, loss=0.689][codecarbon INFO @ 12:36:04] Energy consumed for RAM : 0.000209 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:04] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:04] Energy consumed for All CPU : 0.000444 kWh\n[codecarbon INFO @ 12:36:04] Energy consumed for all GPUs : 0.000666 kWh. Total GPU Power : 69.37191897764953 W\n[codecarbon INFO @ 12:36:04] 0.001320 kWh of electricity used since the beginning.\nTraining Epoch 1:  67%|██████▋   | 418/625 [00:36<00:17, 11.94it/s, loss=0.689][codecarbon INFO @ 12:36:05] Energy consumed for RAM : 0.000214 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:05] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:05] Energy consumed for All CPU : 0.000456 kWh\n[codecarbon INFO @ 12:36:05] Energy consumed for all GPUs : 0.000685 kWh. Total GPU Power : 68.87257860913934 W\n[codecarbon INFO @ 12:36:05] 0.001356 kWh of electricity used since the beginning.\nTraining Epoch 1:  69%|██████▉   | 430/625 [00:37<00:16, 11.93it/s, loss=0.689][codecarbon INFO @ 12:36:06] Energy consumed for RAM : 0.000220 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:06] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:06] Energy consumed for All CPU : 0.000468 kWh\n[codecarbon INFO @ 12:36:06] Energy consumed for all GPUs : 0.000704 kWh. Total GPU Power : 68.56955135752179 W\n[codecarbon INFO @ 12:36:06] 0.001392 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:06] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:06] 0.017058 g.CO2eq/s mean an estimation of 537.9432115283713 kg.CO2eq/year\nTraining Epoch 1:  71%|███████   | 442/625 [00:38<00:15, 11.91it/s, loss=0.689][codecarbon INFO @ 12:36:07] Energy consumed for RAM : 0.000225 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:07] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:07] Energy consumed for All CPU : 0.000480 kWh\n[codecarbon INFO @ 12:36:07] Energy consumed for all GPUs : 0.000723 kWh. Total GPU Power : 68.84090938160492 W\n[codecarbon INFO @ 12:36:07] 0.001428 kWh of electricity used since the beginning.\nTraining Epoch 1:  73%|███████▎  | 454/625 [00:39<00:14, 11.87it/s, loss=0.689][codecarbon INFO @ 12:36:08] Energy consumed for RAM : 0.000231 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:08] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:08] Energy consumed for All CPU : 0.000491 kWh\nTraining Epoch 1:  73%|███████▎  | 454/625 [00:39<00:14, 11.87it/s, loss=0.689][codecarbon INFO @ 12:36:08] Energy consumed for all GPUs : 0.000742 kWh. Total GPU Power : 68.57650345831695 W\n[codecarbon INFO @ 12:36:08] 0.001464 kWh of electricity used since the beginning.\nTraining Epoch 1:  75%|███████▍  | 466/625 [00:40<00:13, 11.82it/s, loss=0.688][codecarbon INFO @ 12:36:09] Energy consumed for RAM : 0.000236 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:09] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:09] Energy consumed for All CPU : 0.000503 kWh\n[codecarbon INFO @ 12:36:09] Energy consumed for all GPUs : 0.000759 kWh. Total GPU Power : 61.69547977885037 W\n[codecarbon INFO @ 12:36:09] 0.001498 kWh of electricity used since the beginning.\nTraining Epoch 1:  76%|███████▋  | 478/625 [00:41<00:12, 11.82it/s, loss=0.688][codecarbon INFO @ 12:36:10] Energy consumed for RAM : 0.000242 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:10] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:10] Energy consumed for All CPU : 0.000515 kWh\n[codecarbon INFO @ 12:36:10] Energy consumed for all GPUs : 0.000778 kWh. Total GPU Power : 69.32502660625698 W\n[codecarbon INFO @ 12:36:10] 0.001534 kWh of electricity used since the beginning.\nTraining Epoch 1:  78%|███████▊  | 490/625 [00:42<00:11, 11.87it/s, loss=0.687][codecarbon INFO @ 12:36:11] Energy consumed for RAM : 0.000247 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:11] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:11] Energy consumed for All CPU : 0.000526 kWh\n[codecarbon INFO @ 12:36:11] Energy consumed for all GPUs : 0.000797 kWh. Total GPU Power : 68.67088009883048 W\n[codecarbon INFO @ 12:36:11] 0.001571 kWh of electricity used since the beginning.\nTraining Epoch 1:  80%|████████  | 502/625 [00:43<00:10, 11.87it/s, loss=0.686][codecarbon INFO @ 12:36:12] Energy consumed for RAM : 0.000253 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:12] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:12] Energy consumed for All CPU : 0.000538 kWh\n[codecarbon INFO @ 12:36:12] Energy consumed for all GPUs : 0.000816 kWh. Total GPU Power : 68.64668694445207 W\n[codecarbon INFO @ 12:36:12] 0.001607 kWh of electricity used since the beginning.\nTraining Epoch 1:  82%|████████▏ | 512/625 [00:44<00:09, 11.82it/s, loss=0.686][codecarbon INFO @ 12:36:13] Energy consumed for RAM : 0.000258 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:13] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:13] Energy consumed for All CPU : 0.000550 kWh\nTraining Epoch 1:  82%|████████▏ | 514/625 [00:44<00:09, 11.81it/s, loss=0.686][codecarbon INFO @ 12:36:13] Energy consumed for all GPUs : 0.000835 kWh. Total GPU Power : 68.39915226371342 W\n[codecarbon INFO @ 12:36:13] 0.001643 kWh of electricity used since the beginning.\nTraining Epoch 1:  84%|████████▍ | 524/625 [00:45<00:08, 11.82it/s, loss=0.685][codecarbon INFO @ 12:36:14] Energy consumed for RAM : 0.000264 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:14] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:14] Energy consumed for All CPU : 0.000561 kWh\n[codecarbon INFO @ 12:36:14] Energy consumed for all GPUs : 0.000854 kWh. Total GPU Power : 69.30451427441605 W\n[codecarbon INFO @ 12:36:14] 0.001679 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:14] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:14] 0.017048 g.CO2eq/s mean an estimation of 537.6228012154877 kg.CO2eq/year\nTraining Epoch 1:  86%|████████▌ | 536/625 [00:46<00:07, 11.80it/s, loss=0.684][codecarbon INFO @ 12:36:15] Energy consumed for RAM : 0.000269 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:15] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:15] Energy consumed for All CPU : 0.000573 kWh\n[codecarbon INFO @ 12:36:15] Energy consumed for all GPUs : 0.000873 kWh. Total GPU Power : 68.47160845342374 W\n[codecarbon INFO @ 12:36:15] 0.001715 kWh of electricity used since the beginning.\nTraining Epoch 1:  88%|████████▊ | 548/625 [00:47<00:06, 11.82it/s, loss=0.683][codecarbon INFO @ 12:36:16] Energy consumed for RAM : 0.000275 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:16] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:16] Energy consumed for All CPU : 0.000585 kWh\n[codecarbon INFO @ 12:36:16] Energy consumed for all GPUs : 0.000890 kWh. Total GPU Power : 61.37425222935345 W\n[codecarbon INFO @ 12:36:16] 0.001749 kWh of electricity used since the beginning.\nTraining Epoch 1:  90%|████████▉ | 560/625 [00:48<00:05, 11.78it/s, loss=0.683][codecarbon INFO @ 12:36:17] Energy consumed for RAM : 0.000280 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:17] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:17] Energy consumed for All CPU : 0.000596 kWh\n[codecarbon INFO @ 12:36:17] Energy consumed for all GPUs : 0.000908 kWh. Total GPU Power : 68.77009483153809 W\n[codecarbon INFO @ 12:36:17] 0.001785 kWh of electricity used since the beginning.\nTraining Epoch 1:  92%|█████████▏| 572/625 [00:49<00:04, 11.79it/s, loss=0.682][codecarbon INFO @ 12:36:18] Energy consumed for RAM : 0.000286 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:18] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:18] Energy consumed for All CPU : 0.000608 kWh\n[codecarbon INFO @ 12:36:18] Energy consumed for all GPUs : 0.000927 kWh. Total GPU Power : 68.28892176961348 W\n[codecarbon INFO @ 12:36:18] 0.001821 kWh of electricity used since the beginning.\nTraining Epoch 1:  93%|█████████▎| 584/625 [00:50<00:03, 11.76it/s, loss=0.681][codecarbon INFO @ 12:36:19] Energy consumed for RAM : 0.000291 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:19] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:19] Energy consumed for All CPU : 0.000620 kWh\nTraining Epoch 1:  93%|█████████▎| 584/625 [00:50<00:03, 11.76it/s, loss=0.68] [codecarbon INFO @ 12:36:19] Energy consumed for all GPUs : 0.000946 kWh. Total GPU Power : 68.27837903128473 W\n[codecarbon INFO @ 12:36:19] 0.001857 kWh of electricity used since the beginning.\nTraining Epoch 1:  95%|█████████▌| 596/625 [00:51<00:02, 11.83it/s, loss=0.68] [codecarbon INFO @ 12:36:20] Energy consumed for RAM : 0.000297 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:20] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:20] Energy consumed for All CPU : 0.000632 kWh\n[codecarbon INFO @ 12:36:20] Energy consumed for all GPUs : 0.000965 kWh. Total GPU Power : 69.0972967001326 W\n[codecarbon INFO @ 12:36:20] 0.001894 kWh of electricity used since the beginning.\nTraining Epoch 1:  97%|█████████▋| 608/625 [00:52<00:01, 11.90it/s, loss=0.679][codecarbon INFO @ 12:36:21] Energy consumed for RAM : 0.000302 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:21] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:21] Energy consumed for All CPU : 0.000643 kWh\n[codecarbon INFO @ 12:36:21] Energy consumed for all GPUs : 0.000984 kWh. Total GPU Power : 68.4122626642542 W\n[codecarbon INFO @ 12:36:21] 0.001930 kWh of electricity used since the beginning.\nTraining Epoch 1:  99%|█████████▉| 620/625 [00:53<00:00, 11.75it/s, loss=0.677][codecarbon INFO @ 12:36:22] Energy consumed for RAM : 0.000308 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:22] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:22] Energy consumed for All CPU : 0.000655 kWh\n[codecarbon INFO @ 12:36:22] Energy consumed for all GPUs : 0.001003 kWh. Total GPU Power : 68.16348516057514 W\n[codecarbon INFO @ 12:36:22] 0.001966 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:22] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:22] 0.017018 g.CO2eq/s mean an estimation of 536.6705679702169 kg.CO2eq/year\nTraining Epoch 1: 100%|██████████| 625/625 [00:53<00:00, 11.65it/s, loss=0.675]\nEvaluating:  36%|███▋      | 20/55 [00:00<00:00, 37.11it/s][codecarbon INFO @ 12:36:23] Energy consumed for RAM : 0.000313 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:23] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:23] Energy consumed for All CPU : 0.000667 kWh\n[codecarbon INFO @ 12:36:23] Energy consumed for all GPUs : 0.001022 kWh. Total GPU Power : 68.43814654705203 W\n[codecarbon INFO @ 12:36:23] 0.002002 kWh of electricity used since the beginning.\nTraining Epoch 2:   0%|          | 0/625 [00:00<?, ?it/s, loss=0.657][codecarbon INFO @ 12:36:24] Energy consumed for RAM : 0.000319 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:24] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:24] Energy consumed for All CPU : 0.000678 kWh\n[codecarbon INFO @ 12:36:24] Energy consumed for all GPUs : 0.001039 kWh. Total GPU Power : 61.73088402869575 W\n[codecarbon INFO @ 12:36:24] 0.002036 kWh of electricity used since the beginning.\nTraining Epoch 2:   2%|▏         | 12/625 [00:01<00:52, 11.65it/s, loss=0.588][codecarbon INFO @ 12:36:25] Energy consumed for RAM : 0.000324 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:25] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:25] Energy consumed for All CPU : 0.000690 kWh\n[codecarbon INFO @ 12:36:25] Energy consumed for all GPUs : 0.001057 kWh. Total GPU Power : 67.93587420114035 W\n[codecarbon INFO @ 12:36:25] 0.002072 kWh of electricity used since the beginning.\nTraining Epoch 2:   4%|▍         | 24/625 [00:02<00:51, 11.69it/s, loss=0.577][codecarbon INFO @ 12:36:26] Energy consumed for RAM : 0.000330 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:26] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:26] Energy consumed for All CPU : 0.000702 kWh\n[codecarbon INFO @ 12:36:26] Energy consumed for all GPUs : 0.001076 kWh. Total GPU Power : 67.54648029559135 W\n[codecarbon INFO @ 12:36:26] 0.002108 kWh of electricity used since the beginning.\nTraining Epoch 2:   6%|▌         | 36/625 [00:03<00:50, 11.66it/s, loss=0.559][codecarbon INFO @ 12:36:27] Energy consumed for RAM : 0.000335 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:27] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:27] Energy consumed for All CPU : 0.000713 kWh\n[codecarbon INFO @ 12:36:27] Energy consumed for all GPUs : 0.001095 kWh. Total GPU Power : 68.24297708938468 W\n[codecarbon INFO @ 12:36:27] 0.002144 kWh of electricity used since the beginning.\nTraining Epoch 2:   8%|▊         | 48/625 [00:04<00:49, 11.64it/s, loss=0.555][codecarbon INFO @ 12:36:28] Energy consumed for RAM : 0.000341 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:28] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:28] Energy consumed for All CPU : 0.000725 kWh\n[codecarbon INFO @ 12:36:28] Energy consumed for all GPUs : 0.001114 kWh. Total GPU Power : 67.74890600782268 W\n[codecarbon INFO @ 12:36:28] 0.002180 kWh of electricity used since the beginning.\nTraining Epoch 2:   9%|▉         | 58/625 [00:05<00:48, 11.64it/s, loss=0.547][codecarbon INFO @ 12:36:29] Energy consumed for RAM : 0.000346 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:29] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:29] Energy consumed for All CPU : 0.000737 kWh\n[codecarbon INFO @ 12:36:29] Energy consumed for all GPUs : 0.001132 kWh. Total GPU Power : 68.39329532068379 W\n[codecarbon INFO @ 12:36:29] 0.002215 kWh of electricity used since the beginning.\nTraining Epoch 2:  11%|█         | 70/625 [00:06<00:47, 11.60it/s, loss=0.555][codecarbon INFO @ 12:36:30] Energy consumed for RAM : 0.000352 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:30] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:30] Energy consumed for All CPU : 0.000748 kWh\n[codecarbon INFO @ 12:36:30] Energy consumed for all GPUs : 0.001151 kWh. Total GPU Power : 67.836892382794 W\n[codecarbon INFO @ 12:36:30] 0.002251 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:30] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:30] 0.016955 g.CO2eq/s mean an estimation of 534.688159845294 kg.CO2eq/year\nTraining Epoch 2:  13%|█▎        | 82/625 [00:07<00:46, 11.57it/s, loss=0.552][codecarbon INFO @ 12:36:31] Energy consumed for RAM : 0.000357 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:31] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:31] Energy consumed for All CPU : 0.000760 kWh\n[codecarbon INFO @ 12:36:31] Energy consumed for all GPUs : 0.001170 kWh. Total GPU Power : 67.70457422448425 W\n[codecarbon INFO @ 12:36:31] 0.002287 kWh of electricity used since the beginning.\nTraining Epoch 2:  15%|█▌        | 94/625 [00:08<00:45, 11.59it/s, loss=0.542][codecarbon INFO @ 12:36:32] Energy consumed for RAM : 0.000363 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:32] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:32] Energy consumed for All CPU : 0.000772 kWh\n[codecarbon INFO @ 12:36:32] Energy consumed for all GPUs : 0.001187 kWh. Total GPU Power : 61.3765417890169 W\n[codecarbon INFO @ 12:36:32] 0.002321 kWh of electricity used since the beginning.\nTraining Epoch 2:  17%|█▋        | 106/625 [00:09<00:44, 11.57it/s, loss=0.546][codecarbon INFO @ 12:36:33] Energy consumed for RAM : 0.000368 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:33] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:33] Energy consumed for All CPU : 0.000784 kWh\n[codecarbon INFO @ 12:36:33] Energy consumed for all GPUs : 0.001205 kWh. Total GPU Power : 67.81994785164063 W\n[codecarbon INFO @ 12:36:33] 0.002357 kWh of electricity used since the beginning.\nTraining Epoch 2:  19%|█▊        | 116/625 [00:10<00:44, 11.54it/s, loss=0.555][codecarbon INFO @ 12:36:34] Energy consumed for RAM : 0.000374 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:34] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:34] Energy consumed for All CPU : 0.000795 kWh\n[codecarbon INFO @ 12:36:34] Energy consumed for all GPUs : 0.001224 kWh. Total GPU Power : 67.51328062554293 W\n[codecarbon INFO @ 12:36:34] 0.002393 kWh of electricity used since the beginning.\nTraining Epoch 2:  20%|██        | 128/625 [00:11<00:43, 11.55it/s, loss=0.555][codecarbon INFO @ 12:36:35] Energy consumed for RAM : 0.000379 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:35] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:35] Energy consumed for All CPU : 0.000807 kWh\n[codecarbon INFO @ 12:36:35] Energy consumed for all GPUs : 0.001243 kWh. Total GPU Power : 67.8368419639831 W\n[codecarbon INFO @ 12:36:35] 0.002429 kWh of electricity used since the beginning.\nTraining Epoch 2:  22%|██▏       | 140/625 [00:12<00:42, 11.54it/s, loss=0.554][codecarbon INFO @ 12:36:36] Energy consumed for RAM : 0.000385 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:36] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:36] Energy consumed for All CPU : 0.000819 kWh\n[codecarbon INFO @ 12:36:36] Energy consumed for all GPUs : 0.001262 kWh. Total GPU Power : 68.58655152635853 W\n[codecarbon INFO @ 12:36:36] 0.002465 kWh of electricity used since the beginning.\nTraining Epoch 2:  24%|██▍       | 152/625 [00:13<00:40, 11.56it/s, loss=0.557][codecarbon INFO @ 12:36:37] Energy consumed for RAM : 0.000390 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:37] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:37] Energy consumed for All CPU : 0.000830 kWh\n[codecarbon INFO @ 12:36:37] Energy consumed for all GPUs : 0.001280 kWh. Total GPU Power : 67.95002479374534 W\n[codecarbon INFO @ 12:36:37] 0.002501 kWh of electricity used since the beginning.\nTraining Epoch 2:  26%|██▌       | 164/625 [00:14<00:39, 11.57it/s, loss=0.557][codecarbon INFO @ 12:36:38] Energy consumed for RAM : 0.000396 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:38] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:38] Energy consumed for All CPU : 0.000842 kWh\n[codecarbon INFO @ 12:36:38] Energy consumed for all GPUs : 0.001299 kWh. Total GPU Power : 68.82302923874275 W\n[codecarbon INFO @ 12:36:38] 0.002537 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:38] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:38] 0.016967 g.CO2eq/s mean an estimation of 535.0609377621465 kg.CO2eq/year\nTraining Epoch 2:  28%|██▊       | 174/625 [00:15<00:39, 11.53it/s, loss=0.559][codecarbon INFO @ 12:36:39] Energy consumed for RAM : 0.000401 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:39] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:39] Energy consumed for All CPU : 0.000854 kWh\n[codecarbon INFO @ 12:36:39] Energy consumed for all GPUs : 0.001318 kWh. Total GPU Power : 66.79199460681119 W\n[codecarbon INFO @ 12:36:39] 0.002573 kWh of electricity used since the beginning.\nTraining Epoch 2:  30%|██▉       | 186/625 [00:16<00:38, 11.48it/s, loss=0.557][codecarbon INFO @ 12:36:40] Energy consumed for RAM : 0.000407 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:40] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:40] Energy consumed for All CPU : 0.000865 kWh\n[codecarbon INFO @ 12:36:40] Energy consumed for all GPUs : 0.001335 kWh. Total GPU Power : 60.71878214046077 W\n[codecarbon INFO @ 12:36:40] 0.002607 kWh of electricity used since the beginning.\nTraining Epoch 2:  32%|███▏      | 198/625 [00:17<00:37, 11.43it/s, loss=0.557][codecarbon INFO @ 12:36:41] Energy consumed for RAM : 0.000412 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:41] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:41] Energy consumed for All CPU : 0.000877 kWh\n[codecarbon INFO @ 12:36:41] Energy consumed for all GPUs : 0.001353 kWh. Total GPU Power : 67.7299762883068 W\n[codecarbon INFO @ 12:36:41] 0.002643 kWh of electricity used since the beginning.\nTraining Epoch 2:  33%|███▎      | 208/625 [00:18<00:36, 11.47it/s, loss=0.553][codecarbon INFO @ 12:36:42] Energy consumed for RAM : 0.000418 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:42] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:42] Energy consumed for All CPU : 0.000889 kWh\n[codecarbon INFO @ 12:36:42] Energy consumed for all GPUs : 0.001372 kWh. Total GPU Power : 67.64801347952387 W\n[codecarbon INFO @ 12:36:42] 0.002678 kWh of electricity used since the beginning.\nTraining Epoch 2:  35%|███▌      | 220/625 [00:19<00:35, 11.44it/s, loss=0.551][codecarbon INFO @ 12:36:43] Energy consumed for RAM : 0.000423 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:43] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:43] Energy consumed for All CPU : 0.000901 kWh\n[codecarbon INFO @ 12:36:43] Energy consumed for all GPUs : 0.001390 kWh. Total GPU Power : 67.45481254512973 W\n[codecarbon INFO @ 12:36:43] 0.002714 kWh of electricity used since the beginning.\nTraining Epoch 2:  37%|███▋      | 232/625 [00:20<00:34, 11.41it/s, loss=0.55] [codecarbon INFO @ 12:36:44] Energy consumed for RAM : 0.000429 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:44] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:44] Energy consumed for All CPU : 0.000912 kWh\n[codecarbon INFO @ 12:36:44] Energy consumed for all GPUs : 0.001409 kWh. Total GPU Power : 67.39485392621098 W\n[codecarbon INFO @ 12:36:44] 0.002750 kWh of electricity used since the beginning.\nTraining Epoch 2:  39%|███▉      | 244/625 [00:21<00:33, 11.44it/s, loss=0.548][codecarbon INFO @ 12:36:45] Energy consumed for RAM : 0.000434 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:45] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:45] Energy consumed for All CPU : 0.000924 kWh\n[codecarbon INFO @ 12:36:45] Energy consumed for all GPUs : 0.001427 kWh. Total GPU Power : 67.37786559031156 W\n[codecarbon INFO @ 12:36:45] 0.002786 kWh of electricity used since the beginning.\nTraining Epoch 2:  41%|████      | 254/625 [00:22<00:32, 11.44it/s, loss=0.545][codecarbon INFO @ 12:36:46] Energy consumed for RAM : 0.000440 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:46] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:46] Energy consumed for All CPU : 0.000936 kWh\n[codecarbon INFO @ 12:36:46] Energy consumed for all GPUs : 0.001446 kWh. Total GPU Power : 67.36354163335922 W\n[codecarbon INFO @ 12:36:46] 0.002821 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:46] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:46] 0.016873 g.CO2eq/s mean an estimation of 532.1217246155883 kg.CO2eq/year\nTraining Epoch 2:  43%|████▎     | 266/625 [00:23<00:31, 11.43it/s, loss=0.545][codecarbon INFO @ 12:36:47] Energy consumed for RAM : 0.000445 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:47] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:47] Energy consumed for All CPU : 0.000947 kWh\n[codecarbon INFO @ 12:36:47] Energy consumed for all GPUs : 0.001465 kWh. Total GPU Power : 67.4046298175288 W\n[codecarbon INFO @ 12:36:47] 0.002857 kWh of electricity used since the beginning.\nTraining Epoch 2:  44%|████▍     | 278/625 [00:24<00:30, 11.39it/s, loss=0.545][codecarbon INFO @ 12:36:48] Energy consumed for RAM : 0.000451 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:48] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:48] Energy consumed for All CPU : 0.000959 kWh\n[codecarbon INFO @ 12:36:48] Energy consumed for all GPUs : 0.001483 kWh. Total GPU Power : 67.16152100432724 W\n[codecarbon INFO @ 12:36:48] 0.002893 kWh of electricity used since the beginning.\nTraining Epoch 2:  46%|████▌     | 288/625 [00:25<00:29, 11.42it/s, loss=0.545][codecarbon INFO @ 12:36:49] Energy consumed for RAM : 0.000456 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:49] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:49] Energy consumed for All CPU : 0.000971 kWh\n[codecarbon INFO @ 12:36:49] Energy consumed for all GPUs : 0.001502 kWh. Total GPU Power : 67.38243687957481 W\n[codecarbon INFO @ 12:36:49] 0.002929 kWh of electricity used since the beginning.\nTraining Epoch 2:  48%|████▊     | 300/625 [00:26<00:28, 11.55it/s, loss=0.543][codecarbon INFO @ 12:36:50] Energy consumed for RAM : 0.000462 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:50] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:50] Energy consumed for All CPU : 0.000982 kWh\n[codecarbon INFO @ 12:36:50] Energy consumed for all GPUs : 0.001519 kWh. Total GPU Power : 60.6070634891575 W\n[codecarbon INFO @ 12:36:50] 0.002963 kWh of electricity used since the beginning.\nTraining Epoch 2:  50%|████▉     | 312/625 [00:27<00:27, 11.38it/s, loss=0.541][codecarbon INFO @ 12:36:51] Energy consumed for RAM : 0.000467 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:51] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:51] Energy consumed for All CPU : 0.000994 kWh\n[codecarbon INFO @ 12:36:51] Energy consumed for all GPUs : 0.001537 kWh. Total GPU Power : 67.2941688976979 W\n[codecarbon INFO @ 12:36:51] 0.002998 kWh of electricity used since the beginning.\nTraining Epoch 2:  52%|█████▏    | 324/625 [00:28<00:26, 11.33it/s, loss=0.537][codecarbon INFO @ 12:36:52] Energy consumed for RAM : 0.000473 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:52] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:52] Energy consumed for All CPU : 0.001006 kWh\n[codecarbon INFO @ 12:36:52] Energy consumed for all GPUs : 0.001556 kWh. Total GPU Power : 67.20516666871963 W\n[codecarbon INFO @ 12:36:52] 0.003034 kWh of electricity used since the beginning.\nTraining Epoch 2:  53%|█████▎    | 334/625 [00:29<00:25, 11.26it/s, loss=0.538][codecarbon INFO @ 12:36:53] Energy consumed for RAM : 0.000478 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:53] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:53] Energy consumed for All CPU : 0.001018 kWh\n[codecarbon INFO @ 12:36:53] Energy consumed for all GPUs : 0.001574 kWh. Total GPU Power : 67.3077322875825 W\n[codecarbon INFO @ 12:36:53] 0.003070 kWh of electricity used since the beginning.\nTraining Epoch 2:  55%|█████▌    | 346/625 [00:30<00:24, 11.33it/s, loss=0.535][codecarbon INFO @ 12:36:54] Energy consumed for RAM : 0.000484 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:54] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:54] Energy consumed for All CPU : 0.001029 kWh\n[codecarbon INFO @ 12:36:54] Energy consumed for all GPUs : 0.001593 kWh. Total GPU Power : 66.9995582798941 W\n[codecarbon INFO @ 12:36:54] 0.003106 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:36:54] We do not have data for None, using world average.\n[codecarbon INFO @ 12:36:54] 0.016865 g.CO2eq/s mean an estimation of 531.8620440469713 kg.CO2eq/year\nTraining Epoch 2:  57%|█████▋    | 358/625 [00:31<00:23, 11.29it/s, loss=0.535][codecarbon INFO @ 12:36:55] Energy consumed for RAM : 0.000489 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:55] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:55] Energy consumed for All CPU : 0.001041 kWh\n[codecarbon INFO @ 12:36:55] Energy consumed for all GPUs : 0.001611 kWh. Total GPU Power : 67.00849060628018 W\n[codecarbon INFO @ 12:36:55] 0.003141 kWh of electricity used since the beginning.\nTraining Epoch 2:  59%|█████▉    | 368/625 [00:32<00:22, 11.39it/s, loss=0.533][codecarbon INFO @ 12:36:56] Energy consumed for RAM : 0.000495 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:56] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:56] Energy consumed for All CPU : 0.001053 kWh\n[codecarbon INFO @ 12:36:56] Energy consumed for all GPUs : 0.001630 kWh. Total GPU Power : 67.4576736016813 W\n[codecarbon INFO @ 12:36:56] 0.003177 kWh of electricity used since the beginning.\nTraining Epoch 2:  61%|██████    | 380/625 [00:33<00:21, 11.29it/s, loss=0.528][codecarbon INFO @ 12:36:57] Energy consumed for RAM : 0.000500 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:57] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:57] Energy consumed for All CPU : 0.001064 kWh\n[codecarbon INFO @ 12:36:58] Energy consumed for all GPUs : 0.001648 kWh. Total GPU Power : 66.81652500075086 W\n[codecarbon INFO @ 12:36:58] 0.003213 kWh of electricity used since the beginning.\nTraining Epoch 2:  63%|██████▎   | 392/625 [00:34<00:20, 11.28it/s, loss=0.524][codecarbon INFO @ 12:36:58] Energy consumed for RAM : 0.000506 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:58] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:58] Energy consumed for All CPU : 0.001076 kWh\n[codecarbon INFO @ 12:36:58] Energy consumed for all GPUs : 0.001665 kWh. Total GPU Power : 60.713535973209495 W\n[codecarbon INFO @ 12:36:58] 0.003247 kWh of electricity used since the beginning.\nTraining Epoch 2:  64%|██████▍   | 402/625 [00:35<00:19, 11.29it/s, loss=0.524][codecarbon INFO @ 12:36:59] Energy consumed for RAM : 0.000511 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:36:59] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:36:59] Energy consumed for All CPU : 0.001088 kWh\n[codecarbon INFO @ 12:36:59] Energy consumed for all GPUs : 0.001683 kWh. Total GPU Power : 66.85992301530861 W\n[codecarbon INFO @ 12:36:59] 0.003282 kWh of electricity used since the beginning.\nTraining Epoch 2:  66%|██████▌   | 414/625 [00:36<00:18, 11.26it/s, loss=0.523][codecarbon INFO @ 12:37:00] Energy consumed for RAM : 0.000517 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:37:00] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:37:00] Energy consumed for All CPU : 0.001099 kWh\nTraining Epoch 2:  66%|██████▌   | 414/625 [00:36<00:18, 11.26it/s, loss=0.523][codecarbon INFO @ 12:37:01] Energy consumed for all GPUs : 0.001702 kWh. Total GPU Power : 66.90737530987441 W\n[codecarbon INFO @ 12:37:01] 0.003318 kWh of electricity used since the beginning.\nTraining Epoch 2:  68%|██████▊   | 424/625 [00:37<00:17, 11.22it/s, loss=0.522][codecarbon INFO @ 12:37:01] Energy consumed for RAM : 0.000522 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:37:01] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:37:01] Energy consumed for All CPU : 0.001111 kWh\n[codecarbon INFO @ 12:37:01] Energy consumed for all GPUs : 0.001720 kWh. Total GPU Power : 67.19659224740138 W\n[codecarbon INFO @ 12:37:01] 0.003354 kWh of electricity used since the beginning.\nTraining Epoch 2:  70%|██████▉   | 436/625 [00:38<00:16, 11.13it/s, loss=0.522][codecarbon INFO @ 12:37:02] Energy consumed for RAM : 0.000528 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:37:02] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:37:02] Energy consumed for All CPU : 0.001123 kWh\n[codecarbon INFO @ 12:37:02] Energy consumed for all GPUs : 0.001739 kWh. Total GPU Power : 66.48484454413658 W\n[codecarbon INFO @ 12:37:02] 0.003389 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:37:03] We do not have data for None, using world average.\n[codecarbon INFO @ 12:37:03] 0.016827 g.CO2eq/s mean an estimation of 530.6457562756774 kg.CO2eq/year\nTraining Epoch 2:  72%|███████▏  | 448/625 [00:39<00:15, 11.19it/s, loss=0.522][codecarbon INFO @ 12:37:03] Energy consumed for RAM : 0.000533 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:37:03] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:37:03] Energy consumed for All CPU : 0.001135 kWh\n[codecarbon INFO @ 12:37:04] Energy consumed for all GPUs : 0.001757 kWh. Total GPU Power : 66.84762659915664 W\n[codecarbon INFO @ 12:37:04] 0.003425 kWh of electricity used since the beginning.\nTraining Epoch 2:  73%|███████▎  | 458/625 [00:40<00:14, 11.14it/s, loss=0.52] [codecarbon INFO @ 12:37:04] Energy consumed for RAM : 0.000539 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:37:04] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:37:04] Energy consumed for All CPU : 0.001146 kWh\n[codecarbon INFO @ 12:37:04] Energy consumed for all GPUs : 0.001775 kWh. Total GPU Power : 67.03886403445034 W\n[codecarbon INFO @ 12:37:04] 0.003460 kWh of electricity used since the beginning.\nTraining Epoch 2:  75%|███████▌  | 470/625 [00:41<00:13, 11.16it/s, loss=0.519][codecarbon INFO @ 12:37:05] Energy consumed for RAM : 0.000544 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:37:05] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:37:05] Energy consumed for All CPU : 0.001158 kWh\n[codecarbon INFO @ 12:37:06] Energy consumed for all GPUs : 0.001794 kWh. Total GPU Power : 66.39233617191937 W\n[codecarbon INFO @ 12:37:06] 0.003496 kWh of electricity used since the beginning.\nTraining Epoch 4:  52%|█████▏    | 324/625 [00:29<00:27, 11.03it/s, loss=0.365][codecarbon INFO @ 12:38:49] Energy consumed for RAM : 0.001111 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:49] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:49] Energy consumed for All CPU : 0.002363 kWh\nTraining Epoch 4:  52%|█████▏    | 324/625 [00:29<00:27, 11.03it/s, loss=0.365][codecarbon INFO @ 12:38:49] Energy consumed for all GPUs : 0.003661 kWh. Total GPU Power : 66.54679216700791 W\n[codecarbon INFO @ 12:38:49] 0.007135 kWh of electricity used since the beginning.\nTraining Epoch 4:  53%|█████▎    | 334/625 [00:30<00:26, 11.05it/s, loss=0.365][codecarbon INFO @ 12:38:50] Energy consumed for RAM : 0.001116 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:50] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:50] Energy consumed for All CPU : 0.002374 kWh\n[codecarbon INFO @ 12:38:50] Energy consumed for all GPUs : 0.003678 kWh. Total GPU Power : 60.061106738757886 W\n[codecarbon INFO @ 12:38:50] 0.007168 kWh of electricity used since the beginning.\nTraining Epoch 4:  55%|█████▌    | 346/625 [00:31<00:25, 11.06it/s, loss=0.364][codecarbon INFO @ 12:38:51] Energy consumed for RAM : 0.001122 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:51] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:51] Energy consumed for All CPU : 0.002386 kWh\n[codecarbon INFO @ 12:38:51] Energy consumed for all GPUs : 0.003696 kWh. Total GPU Power : 66.20038864778967 W\n[codecarbon INFO @ 12:38:51] 0.007204 kWh of electricity used since the beginning.\nTraining Epoch 4:  57%|█████▋    | 356/625 [00:32<00:24, 11.00it/s, loss=0.366][codecarbon INFO @ 12:38:52] Energy consumed for RAM : 0.001127 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:52] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:52] Energy consumed for All CPU : 0.002398 kWh\n[codecarbon INFO @ 12:38:52] Energy consumed for all GPUs : 0.003714 kWh. Total GPU Power : 66.215863293775 W\n[codecarbon INFO @ 12:38:52] 0.007239 kWh of electricity used since the beginning.\nTraining Epoch 4:  59%|█████▉    | 368/625 [00:33<00:23, 11.01it/s, loss=0.365][codecarbon INFO @ 12:38:53] Energy consumed for RAM : 0.001133 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:53] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:53] Energy consumed for All CPU : 0.002410 kWh\n[codecarbon INFO @ 12:38:53] Energy consumed for all GPUs : 0.003733 kWh. Total GPU Power : 66.14733336948461 W\n[codecarbon INFO @ 12:38:53] 0.007275 kWh of electricity used since the beginning.\nTraining Epoch 4:  60%|██████    | 378/625 [00:34<00:22, 10.99it/s, loss=0.365][codecarbon INFO @ 12:38:54] Energy consumed for RAM : 0.001138 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:54] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:54] Energy consumed for All CPU : 0.002421 kWh\n[codecarbon INFO @ 12:38:54] Energy consumed for all GPUs : 0.003751 kWh. Total GPU Power : 66.24714015382585 W\n[codecarbon INFO @ 12:38:54] 0.007310 kWh of electricity used since the beginning.\nTraining Epoch 4:  62%|██████▏   | 390/625 [00:35<00:21, 11.03it/s, loss=0.365][codecarbon INFO @ 12:38:55] Energy consumed for RAM : 0.001144 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:55] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:55] Energy consumed for All CPU : 0.002433 kWh\n[codecarbon INFO @ 12:38:55] Energy consumed for all GPUs : 0.003769 kWh. Total GPU Power : 66.34124177592402 W\n[codecarbon INFO @ 12:38:55] 0.007346 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:38:55] We do not have data for None, using world average.\n[codecarbon INFO @ 12:38:55] 0.016748 g.CO2eq/s mean an estimation of 528.1796910158863 kg.CO2eq/year\nTraining Epoch 4:  64%|██████▍   | 400/625 [00:36<00:20, 11.01it/s, loss=0.364][codecarbon INFO @ 12:38:56] Energy consumed for RAM : 0.001149 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:56] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:56] Energy consumed for All CPU : 0.002445 kWh\n[codecarbon INFO @ 12:38:56] Energy consumed for all GPUs : 0.003788 kWh. Total GPU Power : 66.70585795679608 W\n[codecarbon INFO @ 12:38:56] 0.007381 kWh of electricity used since the beginning.\nTraining Epoch 4:  66%|██████▌   | 412/625 [00:37<00:19, 11.01it/s, loss=0.364][codecarbon INFO @ 12:38:57] Energy consumed for RAM : 0.001155 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:57] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:57] Energy consumed for All CPU : 0.002456 kWh\n[codecarbon INFO @ 12:38:57] Energy consumed for all GPUs : 0.003806 kWh. Total GPU Power : 66.76474706069233 W\n[codecarbon INFO @ 12:38:57] 0.007417 kWh of electricity used since the beginning.\nTraining Epoch 4:  68%|██████▊   | 422/625 [00:38<00:18, 11.02it/s, loss=0.363][codecarbon INFO @ 12:38:58] Energy consumed for RAM : 0.001160 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:58] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:58] Energy consumed for All CPU : 0.002468 kWh\n[codecarbon INFO @ 12:38:58] Energy consumed for all GPUs : 0.003824 kWh. Total GPU Power : 66.45211495079329 W\n[codecarbon INFO @ 12:38:58] 0.007452 kWh of electricity used since the beginning.\nTraining Epoch 4:  69%|██████▉   | 434/625 [00:39<00:17, 11.02it/s, loss=0.363][codecarbon INFO @ 12:38:59] Energy consumed for RAM : 0.001166 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:38:59] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:38:59] Energy consumed for All CPU : 0.002480 kWh\n[codecarbon INFO @ 12:38:59] Energy consumed for all GPUs : 0.003841 kWh. Total GPU Power : 60.45070914921512 W\n[codecarbon INFO @ 12:38:59] 0.007486 kWh of electricity used since the beginning.\nTraining Epoch 4:  71%|███████   | 444/625 [00:40<00:16, 11.01it/s, loss=0.361][codecarbon INFO @ 12:39:00] Energy consumed for RAM : 0.001171 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:00] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:00] Energy consumed for All CPU : 0.002491 kWh\n[codecarbon INFO @ 12:39:00] Energy consumed for all GPUs : 0.003859 kWh. Total GPU Power : 66.52704381365282 W\n[codecarbon INFO @ 12:39:00] 0.007522 kWh of electricity used since the beginning.\nTraining Epoch 4:  73%|███████▎  | 456/625 [00:41<00:15, 11.06it/s, loss=0.365][codecarbon INFO @ 12:39:01] Energy consumed for RAM : 0.001177 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:01] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:01] Energy consumed for All CPU : 0.002503 kWh\n[codecarbon INFO @ 12:39:01] Energy consumed for all GPUs : 0.003877 kWh. Total GPU Power : 67.05020669423531 W\n[codecarbon INFO @ 12:39:01] 0.007557 kWh of electricity used since the beginning.\nTraining Epoch 4:  75%|███████▍  | 466/625 [00:42<00:14, 11.01it/s, loss=0.364][codecarbon INFO @ 12:39:02] Energy consumed for RAM : 0.001182 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:02] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:02] Energy consumed for All CPU : 0.002515 kWh\n[codecarbon INFO @ 12:39:02] Energy consumed for all GPUs : 0.003896 kWh. Total GPU Power : 66.37803380933578 W\n[codecarbon INFO @ 12:39:02] 0.007592 kWh of electricity used since the beginning.\nTraining Epoch 4:  76%|███████▋  | 478/625 [00:43<00:13, 10.99it/s, loss=0.365][codecarbon INFO @ 12:39:03] Energy consumed for RAM : 0.001188 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:03] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:03] Energy consumed for All CPU : 0.002526 kWh\n[codecarbon INFO @ 12:39:03] Energy consumed for all GPUs : 0.003914 kWh. Total GPU Power : 65.99025787005729 W\n[codecarbon INFO @ 12:39:03] 0.007628 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:03] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:03] 0.016736 g.CO2eq/s mean an estimation of 527.7921510556766 kg.CO2eq/year\nTraining Epoch 4:  78%|███████▊  | 488/625 [00:44<00:12, 10.99it/s, loss=0.365][codecarbon INFO @ 12:39:04] Energy consumed for RAM : 0.001193 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:04] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:04] Energy consumed for All CPU : 0.002538 kWh\n[codecarbon INFO @ 12:39:04] Energy consumed for all GPUs : 0.003932 kWh. Total GPU Power : 66.48807427959714 W\n[codecarbon INFO @ 12:39:04] 0.007663 kWh of electricity used since the beginning.\nTraining Epoch 4:  80%|████████  | 500/625 [00:45<00:11, 10.99it/s, loss=0.363][codecarbon INFO @ 12:39:05] Energy consumed for RAM : 0.001199 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:05] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:05] Energy consumed for All CPU : 0.002550 kWh\n[codecarbon INFO @ 12:39:05] Energy consumed for all GPUs : 0.003950 kWh. Total GPU Power : 66.17722671258173 W\n[codecarbon INFO @ 12:39:05] 0.007699 kWh of electricity used since the beginning.\nTraining Epoch 4:  82%|████████▏ | 510/625 [00:46<00:10, 10.98it/s, loss=0.363][codecarbon INFO @ 12:39:06] Energy consumed for RAM : 0.001204 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:06] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:06] Energy consumed for All CPU : 0.002561 kWh\n[codecarbon INFO @ 12:39:06] Energy consumed for all GPUs : 0.003969 kWh. Total GPU Power : 66.5177714827622 W\n[codecarbon INFO @ 12:39:06] 0.007734 kWh of electricity used since the beginning.\nTraining Epoch 4:  84%|████████▎ | 522/625 [00:47<00:09, 10.95it/s, loss=0.364][codecarbon INFO @ 12:39:07] Energy consumed for RAM : 0.001210 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:07] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:07] Energy consumed for All CPU : 0.002573 kWh\n[codecarbon INFO @ 12:39:07] Energy consumed for all GPUs : 0.003987 kWh. Total GPU Power : 66.33251517773347 W\n[codecarbon INFO @ 12:39:07] 0.007770 kWh of electricity used since the beginning.\nTraining Epoch 4:  85%|████████▌ | 532/625 [00:48<00:08, 10.97it/s, loss=0.365][codecarbon INFO @ 12:39:08] Energy consumed for RAM : 0.001215 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:08] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:08] Energy consumed for All CPU : 0.002585 kWh\n[codecarbon INFO @ 12:39:08] Energy consumed for all GPUs : 0.004004 kWh. Total GPU Power : 59.86363987779927 W\n[codecarbon INFO @ 12:39:08] 0.007804 kWh of electricity used since the beginning.\nTraining Epoch 4:  87%|████████▋ | 544/625 [00:49<00:07, 10.97it/s, loss=0.365][codecarbon INFO @ 12:39:09] Energy consumed for RAM : 0.001221 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:09] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\nTraining Epoch 4:  87%|████████▋ | 544/625 [00:49<00:07, 10.97it/s, loss=0.365][codecarbon INFO @ 12:39:09] Energy consumed for All CPU : 0.002597 kWh\n[codecarbon INFO @ 12:39:09] Energy consumed for all GPUs : 0.004022 kWh. Total GPU Power : 66.27504532157222 W\n[codecarbon INFO @ 12:39:09] 0.007839 kWh of electricity used since the beginning.\nTraining Epoch 4:  89%|████████▊ | 554/625 [00:50<00:06, 10.99it/s, loss=0.365][codecarbon INFO @ 12:39:10] Energy consumed for RAM : 0.001226 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:10] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:10] Energy consumed for All CPU : 0.002608 kWh\n[codecarbon INFO @ 12:39:10] Energy consumed for all GPUs : 0.004040 kWh. Total GPU Power : 66.31991477988241 W\n[codecarbon INFO @ 12:39:10] 0.007874 kWh of electricity used since the beginning.\nTraining Epoch 4:  91%|█████████ | 566/625 [00:51<00:05, 10.97it/s, loss=0.365][codecarbon INFO @ 12:39:11] Energy consumed for RAM : 0.001232 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:11] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:11] Energy consumed for All CPU : 0.002620 kWh\nTraining Epoch 4:  91%|█████████ | 566/625 [00:51<00:05, 10.97it/s, loss=0.365][codecarbon INFO @ 12:39:11] Energy consumed for all GPUs : 0.004058 kWh. Total GPU Power : 66.5414775769073 W\n[codecarbon INFO @ 12:39:11] 0.007910 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:11] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:11] 0.016741 g.CO2eq/s mean an estimation of 527.9315193904458 kg.CO2eq/year\nTraining Epoch 4:  92%|█████████▏| 576/625 [00:52<00:04, 10.99it/s, loss=0.364][codecarbon INFO @ 12:39:12] Energy consumed for RAM : 0.001237 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:12] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:12] Energy consumed for All CPU : 0.002632 kWh\n[codecarbon INFO @ 12:39:12] Energy consumed for all GPUs : 0.004077 kWh. Total GPU Power : 66.77922180895467 W\n[codecarbon INFO @ 12:39:12] 0.007945 kWh of electricity used since the beginning.\nTraining Epoch 4:  94%|█████████▍| 588/625 [00:53<00:03, 10.98it/s, loss=0.364][codecarbon INFO @ 12:39:13] Energy consumed for RAM : 0.001243 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:13] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:13] Energy consumed for All CPU : 0.002643 kWh\nTraining Epoch 4:  94%|█████████▍| 588/625 [00:53<00:03, 10.98it/s, loss=0.364][codecarbon INFO @ 12:39:13] Energy consumed for all GPUs : 0.004095 kWh. Total GPU Power : 66.36037466432595 W\n[codecarbon INFO @ 12:39:13] 0.007981 kWh of electricity used since the beginning.\nTraining Epoch 4:  96%|█████████▌| 598/625 [00:54<00:02, 10.98it/s, loss=0.364][codecarbon INFO @ 12:39:14] Energy consumed for RAM : 0.001248 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:14] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:14] Energy consumed for All CPU : 0.002655 kWh\n[codecarbon INFO @ 12:39:14] Energy consumed for all GPUs : 0.004114 kWh. Total GPU Power : 66.82865225597274 W\n[codecarbon INFO @ 12:39:14] 0.008017 kWh of electricity used since the beginning.\nTraining Epoch 4:  98%|█████████▊| 610/625 [00:55<00:01, 11.01it/s, loss=0.364][codecarbon INFO @ 12:39:15] Energy consumed for RAM : 0.001254 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:15] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:15] Energy consumed for All CPU : 0.002667 kWh\nTraining Epoch 4:  98%|█████████▊| 610/625 [00:55<00:01, 11.01it/s, loss=0.364][codecarbon INFO @ 12:39:15] Energy consumed for all GPUs : 0.004132 kWh. Total GPU Power : 66.46141956855892 W\n[codecarbon INFO @ 12:39:15] 0.008052 kWh of electricity used since the beginning.\nTraining Epoch 4:  99%|█████████▉| 620/625 [00:56<00:00, 11.00it/s, loss=0.364][codecarbon INFO @ 12:39:16] Energy consumed for RAM : 0.001259 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:16] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:16] Energy consumed for All CPU : 0.002678 kWh\n[codecarbon INFO @ 12:39:16] Energy consumed for all GPUs : 0.004148 kWh. Total GPU Power : 60.16840027940675 W\n[codecarbon INFO @ 12:39:16] 0.008086 kWh of electricity used since the beginning.\nTraining Epoch 4: 100%|██████████| 625/625 [00:56<00:00, 11.06it/s, loss=0.363]\nEvaluating:  44%|████▎     | 24/55 [00:00<00:00, 35.10it/s][codecarbon INFO @ 12:39:17] Energy consumed for RAM : 0.001265 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:17] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:17] Energy consumed for All CPU : 0.002690 kWh\n[codecarbon INFO @ 12:39:17] Energy consumed for all GPUs : 0.004167 kWh. Total GPU Power : 66.37384404300612 W\n[codecarbon INFO @ 12:39:17] 0.008121 kWh of electricity used since the beginning.\nTraining Epoch 5:   0%|          | 0/625 [00:00<?, ?it/s, loss=0.332][codecarbon INFO @ 12:39:18] Energy consumed for RAM : 0.001270 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:18] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:18] Energy consumed for All CPU : 0.002702 kWh\n[codecarbon INFO @ 12:39:18] Energy consumed for all GPUs : 0.004185 kWh. Total GPU Power : 66.82893576837 W\n[codecarbon INFO @ 12:39:18] 0.008157 kWh of electricity used since the beginning.\nTraining Epoch 5:   2%|▏         | 12/625 [00:01<00:55, 10.99it/s, loss=0.349][codecarbon INFO @ 12:39:19] Energy consumed for RAM : 0.001275 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:19] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:19] Energy consumed for All CPU : 0.002713 kWh\n[codecarbon INFO @ 12:39:19] Energy consumed for all GPUs : 0.004203 kWh. Total GPU Power : 66.50587650904063 W\n[codecarbon INFO @ 12:39:19] 0.008192 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:19] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:19] 0.016767 g.CO2eq/s mean an estimation of 528.7488740748264 kg.CO2eq/year\nTraining Epoch 5:   4%|▎         | 22/625 [00:02<00:54, 11.03it/s, loss=0.353][codecarbon INFO @ 12:39:20] Energy consumed for RAM : 0.001281 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:20] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:20] Energy consumed for All CPU : 0.002725 kWh\n[codecarbon INFO @ 12:39:20] Energy consumed for all GPUs : 0.004222 kWh. Total GPU Power : 66.36317016717885 W\n[codecarbon INFO @ 12:39:20] 0.008228 kWh of electricity used since the beginning.\nTraining Epoch 5:   5%|▌         | 34/625 [00:03<00:53, 11.01it/s, loss=0.364][codecarbon INFO @ 12:39:21] Energy consumed for RAM : 0.001286 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:21] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:21] Energy consumed for All CPU : 0.002737 kWh\n[codecarbon INFO @ 12:39:21] Energy consumed for all GPUs : 0.004240 kWh. Total GPU Power : 66.27921621331761 W\n[codecarbon INFO @ 12:39:21] 0.008263 kWh of electricity used since the beginning.\nTraining Epoch 5:   7%|▋         | 44/625 [00:04<00:52, 11.02it/s, loss=0.363][codecarbon INFO @ 12:39:22] Energy consumed for RAM : 0.001292 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:22] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:22] Energy consumed for All CPU : 0.002749 kWh\n[codecarbon INFO @ 12:39:22] Energy consumed for all GPUs : 0.004258 kWh. Total GPU Power : 66.60795258393951 W\n[codecarbon INFO @ 12:39:22] 0.008299 kWh of electricity used since the beginning.\nTraining Epoch 5:   9%|▉         | 56/625 [00:05<00:51, 11.02it/s, loss=0.352][codecarbon INFO @ 12:39:23] Energy consumed for RAM : 0.001297 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:23] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:23] Energy consumed for All CPU : 0.002760 kWh\n[codecarbon INFO @ 12:39:23] Energy consumed for all GPUs : 0.004277 kWh. Total GPU Power : 66.93368032950264 W\n[codecarbon INFO @ 12:39:23] 0.008334 kWh of electricity used since the beginning.\nTraining Epoch 5:  11%|█         | 66/625 [00:06<00:50, 11.12it/s, loss=0.341][codecarbon INFO @ 12:39:24] Energy consumed for RAM : 0.001303 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:24] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:24] Energy consumed for All CPU : 0.002772 kWh\n[codecarbon INFO @ 12:39:24] Energy consumed for all GPUs : 0.004295 kWh. Total GPU Power : 66.64054720180089 W\n[codecarbon INFO @ 12:39:24] 0.008370 kWh of electricity used since the beginning.\nTraining Epoch 5:  12%|█▏        | 78/625 [00:07<00:49, 11.04it/s, loss=0.349][codecarbon INFO @ 12:39:25] Energy consumed for RAM : 0.001308 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:25] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:25] Energy consumed for All CPU : 0.002784 kWh\n[codecarbon INFO @ 12:39:25] Energy consumed for all GPUs : 0.004312 kWh. Total GPU Power : 59.98988225156914 W\n[codecarbon INFO @ 12:39:25] 0.008404 kWh of electricity used since the beginning.\nTraining Epoch 5:  14%|█▍        | 88/625 [00:08<00:48, 11.02it/s, loss=0.349][codecarbon INFO @ 12:39:26] Energy consumed for RAM : 0.001314 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:26] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:26] Energy consumed for All CPU : 0.002795 kWh\n[codecarbon INFO @ 12:39:26] Energy consumed for all GPUs : 0.004330 kWh. Total GPU Power : 66.9162069862185 W\n[codecarbon INFO @ 12:39:26] 0.008439 kWh of electricity used since the beginning.\nTraining Epoch 5:  16%|█▌        | 100/625 [00:09<00:47, 11.10it/s, loss=0.351][codecarbon INFO @ 12:39:27] Energy consumed for RAM : 0.001319 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:27] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:27] Energy consumed for All CPU : 0.002807 kWh\n[codecarbon INFO @ 12:39:27] Energy consumed for all GPUs : 0.004348 kWh. Total GPU Power : 66.48372134837622 W\n[codecarbon INFO @ 12:39:27] 0.008475 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:27] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:27] 0.016757 g.CO2eq/s mean an estimation of 528.4446569925572 kg.CO2eq/year\nTraining Epoch 5:  18%|█▊        | 112/625 [00:10<00:46, 11.03it/s, loss=0.352][codecarbon INFO @ 12:39:28] Energy consumed for RAM : 0.001325 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:28] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:28] Energy consumed for All CPU : 0.002819 kWh\n[codecarbon INFO @ 12:39:28] Energy consumed for all GPUs : 0.004367 kWh. Total GPU Power : 66.98878198334504 W\n[codecarbon INFO @ 12:39:28] 0.008510 kWh of electricity used since the beginning.\nTraining Epoch 5:  20%|█▉        | 122/625 [00:11<00:45, 10.99it/s, loss=0.353][codecarbon INFO @ 12:39:29] Energy consumed for RAM : 0.001330 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:29] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:29] Energy consumed for All CPU : 0.002830 kWh\n[codecarbon INFO @ 12:39:29] Energy consumed for all GPUs : 0.004385 kWh. Total GPU Power : 66.35364602381017 W\n[codecarbon INFO @ 12:39:29] 0.008546 kWh of electricity used since the beginning.\nTraining Epoch 5:  21%|██▏       | 134/625 [00:12<00:44, 11.07it/s, loss=0.352][codecarbon INFO @ 12:39:30] Energy consumed for RAM : 0.001336 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:30] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:30] Energy consumed for All CPU : 0.002842 kWh\n[codecarbon INFO @ 12:39:30] Energy consumed for all GPUs : 0.004404 kWh. Total GPU Power : 66.74401953614827 W\n[codecarbon INFO @ 12:39:30] 0.008581 kWh of electricity used since the beginning.\nTraining Epoch 5:  23%|██▎       | 144/625 [00:13<00:43, 11.05it/s, loss=0.354][codecarbon INFO @ 12:39:31] Energy consumed for RAM : 0.001341 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:31] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:31] Energy consumed for All CPU : 0.002854 kWh\n[codecarbon INFO @ 12:39:31] Energy consumed for all GPUs : 0.004422 kWh. Total GPU Power : 66.71427483271202 W\n[codecarbon INFO @ 12:39:31] 0.008617 kWh of electricity used since the beginning.\nTraining Epoch 5:  25%|██▍       | 156/625 [00:14<00:42, 11.04it/s, loss=0.35] [codecarbon INFO @ 12:39:32] Energy consumed for RAM : 0.001347 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:32] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:32] Energy consumed for All CPU : 0.002865 kWh\n[codecarbon INFO @ 12:39:32] Energy consumed for all GPUs : 0.004440 kWh. Total GPU Power : 66.26307369650402 W\n[codecarbon INFO @ 12:39:32] 0.008652 kWh of electricity used since the beginning.\nTraining Epoch 5:  27%|██▋       | 166/625 [00:15<00:41, 11.10it/s, loss=0.348][codecarbon INFO @ 12:39:33] Energy consumed for RAM : 0.001352 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:33] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:33] Energy consumed for All CPU : 0.002877 kWh\n[codecarbon INFO @ 12:39:33] Energy consumed for all GPUs : 0.004459 kWh. Total GPU Power : 66.80569429743872 W\n[codecarbon INFO @ 12:39:33] 0.008688 kWh of electricity used since the beginning.\nTraining Epoch 5:  28%|██▊       | 178/625 [00:16<00:40, 11.05it/s, loss=0.346][codecarbon INFO @ 12:39:34] Energy consumed for RAM : 0.001358 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:34] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:34] Energy consumed for All CPU : 0.002889 kWh\n[codecarbon INFO @ 12:39:34] Energy consumed for all GPUs : 0.004475 kWh. Total GPU Power : 59.82424394836943 W\n[codecarbon INFO @ 12:39:34] 0.008722 kWh of electricity used since the beginning.\nTraining Epoch 5:  30%|███       | 188/625 [00:17<00:39, 11.13it/s, loss=0.344][codecarbon INFO @ 12:39:35] Energy consumed for RAM : 0.001363 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:35] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:35] Energy consumed for All CPU : 0.002901 kWh\n[codecarbon INFO @ 12:39:35] Energy consumed for all GPUs : 0.004494 kWh. Total GPU Power : 66.78054024780629 W\n[codecarbon INFO @ 12:39:35] 0.008757 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:35] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:35] 0.016790 g.CO2eq/s mean an estimation of 529.4913488181138 kg.CO2eq/year\nTraining Epoch 5:  32%|███▏      | 200/625 [00:18<00:38, 11.05it/s, loss=0.349][codecarbon INFO @ 12:39:36] Energy consumed for RAM : 0.001369 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:36] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:36] Energy consumed for All CPU : 0.002912 kWh\n[codecarbon INFO @ 12:39:36] Energy consumed for all GPUs : 0.004512 kWh. Total GPU Power : 66.4193897695638 W\n[codecarbon INFO @ 12:39:36] 0.008793 kWh of electricity used since the beginning.\nTraining Epoch 5:  34%|███▎      | 210/625 [00:19<00:37, 11.13it/s, loss=0.348][codecarbon INFO @ 12:39:37] Energy consumed for RAM : 0.001374 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:37] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:37] Energy consumed for All CPU : 0.002924 kWh\n[codecarbon INFO @ 12:39:37] Energy consumed for all GPUs : 0.004530 kWh. Total GPU Power : 66.63148368035459 W\n[codecarbon INFO @ 12:39:37] 0.008829 kWh of electricity used since the beginning.\nTraining Epoch 5:  36%|███▌      | 222/625 [00:20<00:36, 11.04it/s, loss=0.351][codecarbon INFO @ 12:39:38] Energy consumed for RAM : 0.001380 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:38] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:38] Energy consumed for All CPU : 0.002936 kWh\nTraining Epoch 5:  36%|███▌      | 222/625 [00:20<00:36, 11.04it/s, loss=0.353][codecarbon INFO @ 12:39:38] Energy consumed for all GPUs : 0.004549 kWh. Total GPU Power : 66.29784369742363 W\n[codecarbon INFO @ 12:39:38] 0.008864 kWh of electricity used since the beginning.\nTraining Epoch 5:  37%|███▋      | 232/625 [00:21<00:35, 10.98it/s, loss=0.35] [codecarbon INFO @ 12:39:39] Energy consumed for RAM : 0.001385 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:39] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:39] Energy consumed for All CPU : 0.002947 kWh\n[codecarbon INFO @ 12:39:39] Energy consumed for all GPUs : 0.004567 kWh. Total GPU Power : 66.58649841203088 W\n[codecarbon INFO @ 12:39:39] 0.008900 kWh of electricity used since the beginning.\nTraining Epoch 5:  39%|███▉      | 244/625 [00:22<00:34, 11.04it/s, loss=0.349][codecarbon INFO @ 12:39:40] Energy consumed for RAM : 0.001391 kWh. RAM Power : 20.0 W\nTraining Epoch 5:  39%|███▉      | 244/625 [00:22<00:34, 11.04it/s, loss=0.348][codecarbon INFO @ 12:39:40] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:40] Energy consumed for All CPU : 0.002959 kWh\n[codecarbon INFO @ 12:39:40] Energy consumed for all GPUs : 0.004585 kWh. Total GPU Power : 66.70242888349942 W\n[codecarbon INFO @ 12:39:40] 0.008935 kWh of electricity used since the beginning.\nTraining Epoch 5:  41%|████      | 254/625 [00:23<00:33, 11.12it/s, loss=0.348][codecarbon INFO @ 12:39:41] Energy consumed for RAM : 0.001396 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:41] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:41] Energy consumed for All CPU : 0.002971 kWh\n[codecarbon INFO @ 12:39:41] Energy consumed for all GPUs : 0.004604 kWh. Total GPU Power : 66.93106831934846 W\n[codecarbon INFO @ 12:39:41] 0.008971 kWh of electricity used since the beginning.\nTraining Epoch 5:  43%|████▎     | 266/625 [00:24<00:32, 11.09it/s, loss=0.346][codecarbon INFO @ 12:39:42] Energy consumed for RAM : 0.001402 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:42] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:42] Energy consumed for All CPU : 0.002982 kWh\n[codecarbon INFO @ 12:39:42] Energy consumed for all GPUs : 0.004620 kWh. Total GPU Power : 60.01296569252117 W\n[codecarbon INFO @ 12:39:42] 0.009005 kWh of electricity used since the beginning.\nTraining Epoch 5:  44%|████▍     | 276/625 [00:25<00:31, 11.12it/s, loss=0.344][codecarbon INFO @ 12:39:43] Energy consumed for RAM : 0.001407 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:43] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:43] Energy consumed for All CPU : 0.002994 kWh\nTraining Epoch 5:  44%|████▍     | 278/625 [00:25<00:31, 11.10it/s, loss=0.345][codecarbon INFO @ 12:39:43] Energy consumed for all GPUs : 0.004639 kWh. Total GPU Power : 66.74362562807288 W\n[codecarbon INFO @ 12:39:43] 0.009040 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:43] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:43] 0.016782 g.CO2eq/s mean an estimation of 529.2460216515775 kg.CO2eq/year\nTraining Epoch 5:  46%|████▌     | 288/625 [00:26<00:30, 11.07it/s, loss=0.342][codecarbon INFO @ 12:39:44] Energy consumed for RAM : 0.001413 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:44] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:44] Energy consumed for All CPU : 0.003006 kWh\n[codecarbon INFO @ 12:39:44] Energy consumed for all GPUs : 0.004657 kWh. Total GPU Power : 66.414394366566 W\n[codecarbon INFO @ 12:39:44] 0.009076 kWh of electricity used since the beginning.\nTraining Epoch 5:  48%|████▊     | 300/625 [00:27<00:29, 11.11it/s, loss=0.345][codecarbon INFO @ 12:39:45] Energy consumed for RAM : 0.001418 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:45] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:45] Energy consumed for All CPU : 0.003018 kWh\n[codecarbon INFO @ 12:39:45] Energy consumed for all GPUs : 0.004675 kWh. Total GPU Power : 66.85722616790795 W\n[codecarbon INFO @ 12:39:45] 0.009111 kWh of electricity used since the beginning.\nTraining Epoch 5:  50%|████▉     | 310/625 [00:28<00:28, 11.04it/s, loss=0.343][codecarbon INFO @ 12:39:46] Energy consumed for RAM : 0.001424 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:46] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:46] Energy consumed for All CPU : 0.003029 kWh\n[codecarbon INFO @ 12:39:46] Energy consumed for all GPUs : 0.004694 kWh. Total GPU Power : 66.9122766891057 W\n[codecarbon INFO @ 12:39:46] 0.009147 kWh of electricity used since the beginning.\nTraining Epoch 5:  52%|█████▏    | 322/625 [00:29<00:27, 11.09it/s, loss=0.343][codecarbon INFO @ 12:39:47] Energy consumed for RAM : 0.001429 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:47] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:47] Energy consumed for All CPU : 0.003041 kWh\n[codecarbon INFO @ 12:39:47] Energy consumed for all GPUs : 0.004712 kWh. Total GPU Power : 66.47210953508845 W\n[codecarbon INFO @ 12:39:47] 0.009183 kWh of electricity used since the beginning.\nTraining Epoch 5:  53%|█████▎    | 332/625 [00:30<00:26, 11.12it/s, loss=0.346][codecarbon INFO @ 12:39:48] Energy consumed for RAM : 0.001435 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:48] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:48] Energy consumed for All CPU : 0.003053 kWh\n[codecarbon INFO @ 12:39:48] Energy consumed for all GPUs : 0.004731 kWh. Total GPU Power : 66.79473934394181 W\n[codecarbon INFO @ 12:39:48] 0.009218 kWh of electricity used since the beginning.\nTraining Epoch 5:  55%|█████▌    | 344/625 [00:31<00:25, 11.05it/s, loss=0.345][codecarbon INFO @ 12:39:49] Energy consumed for RAM : 0.001440 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:49] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:49] Energy consumed for All CPU : 0.003064 kWh\n[codecarbon INFO @ 12:39:49] Energy consumed for all GPUs : 0.004749 kWh. Total GPU Power : 66.42131197357622 W\n[codecarbon INFO @ 12:39:49] 0.009254 kWh of electricity used since the beginning.\nTraining Epoch 5:  57%|█████▋    | 354/625 [00:32<00:24, 11.03it/s, loss=0.343][codecarbon INFO @ 12:39:50] Energy consumed for RAM : 0.001446 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:50] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:50] Energy consumed for All CPU : 0.003076 kWh\n[codecarbon INFO @ 12:39:50] Energy consumed for all GPUs : 0.004767 kWh. Total GPU Power : 66.07310718571476 W\n[codecarbon INFO @ 12:39:50] 0.009289 kWh of electricity used since the beginning.\nTraining Epoch 5:  59%|█████▊    | 366/625 [00:33<00:23, 11.08it/s, loss=0.342][codecarbon INFO @ 12:39:51] Energy consumed for RAM : 0.001451 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:51] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:51] Energy consumed for All CPU : 0.003088 kWh\nTraining Epoch 5:  59%|█████▊    | 366/625 [00:33<00:23, 11.08it/s, loss=0.342][codecarbon INFO @ 12:39:51] Energy consumed for all GPUs : 0.004784 kWh. Total GPU Power : 59.987184729466186 W\n[codecarbon INFO @ 12:39:51] 0.009323 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:51] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:51] 0.016785 g.CO2eq/s mean an estimation of 529.3273486465899 kg.CO2eq/year\nTraining Epoch 5:  60%|██████    | 376/625 [00:34<00:22, 11.03it/s, loss=0.342][codecarbon INFO @ 12:39:52] Energy consumed for RAM : 0.001457 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:52] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:52] Energy consumed for All CPU : 0.003099 kWh\n[codecarbon INFO @ 12:39:52] Energy consumed for all GPUs : 0.004802 kWh. Total GPU Power : 66.87046532252896 W\n[codecarbon INFO @ 12:39:52] 0.009359 kWh of electricity used since the beginning.\nTraining Epoch 5:  62%|██████▏   | 388/625 [00:35<00:21, 11.08it/s, loss=0.341][codecarbon INFO @ 12:39:53] Energy consumed for RAM : 0.001462 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:53] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:53] Energy consumed for All CPU : 0.003111 kWh\n[codecarbon INFO @ 12:39:53] Energy consumed for all GPUs : 0.004821 kWh. Total GPU Power : 66.4612994666721 W\n[codecarbon INFO @ 12:39:53] 0.009394 kWh of electricity used since the beginning.\nTraining Epoch 5:  64%|██████▎   | 398/625 [00:36<00:20, 11.08it/s, loss=0.342][codecarbon INFO @ 12:39:54] Energy consumed for RAM : 0.001468 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:54] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:54] Energy consumed for All CPU : 0.003123 kWh\n[codecarbon INFO @ 12:39:54] Energy consumed for all GPUs : 0.004839 kWh. Total GPU Power : 66.77649484422037 W\n[codecarbon INFO @ 12:39:54] 0.009430 kWh of electricity used since the beginning.\nTraining Epoch 5:  66%|██████▌   | 410/625 [00:37<00:19, 11.05it/s, loss=0.34] [codecarbon INFO @ 12:39:55] Energy consumed for RAM : 0.001474 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:55] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:55] Energy consumed for All CPU : 0.003135 kWh\n[codecarbon INFO @ 12:39:55] Energy consumed for all GPUs : 0.004857 kWh. Total GPU Power : 66.738629685703 W\n[codecarbon INFO @ 12:39:55] 0.009466 kWh of electricity used since the beginning.\nTraining Epoch 5:  67%|██████▋   | 420/625 [00:38<00:18, 11.11it/s, loss=0.342][codecarbon INFO @ 12:39:56] Energy consumed for RAM : 0.001479 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:56] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:56] Energy consumed for All CPU : 0.003146 kWh\nTraining Epoch 5:  68%|██████▊   | 422/625 [00:38<00:18, 11.06it/s, loss=0.342][codecarbon INFO @ 12:39:56] Energy consumed for all GPUs : 0.004876 kWh. Total GPU Power : 67.05160014233097 W\n[codecarbon INFO @ 12:39:56] 0.009501 kWh of electricity used since the beginning.\nTraining Epoch 5:  69%|██████▉   | 432/625 [00:39<00:17, 10.98it/s, loss=0.343][codecarbon INFO @ 12:39:57] Energy consumed for RAM : 0.001484 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:57] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:57] Energy consumed for All CPU : 0.003158 kWh\n[codecarbon INFO @ 12:39:57] Energy consumed for all GPUs : 0.004894 kWh. Total GPU Power : 66.4328909466634 W\n[codecarbon INFO @ 12:39:57] 0.009537 kWh of electricity used since the beginning.\nTraining Epoch 5:  71%|███████   | 444/625 [00:40<00:16, 11.13it/s, loss=0.341][codecarbon INFO @ 12:39:58] Energy consumed for RAM : 0.001490 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:58] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:58] Energy consumed for All CPU : 0.003170 kWh\n[codecarbon INFO @ 12:39:58] Energy consumed for all GPUs : 0.004912 kWh. Total GPU Power : 66.44322365207319 W\n[codecarbon INFO @ 12:39:58] 0.009572 kWh of electricity used since the beginning.\nTraining Epoch 5:  73%|███████▎  | 454/625 [00:41<00:15, 11.09it/s, loss=0.34] [codecarbon INFO @ 12:39:59] Energy consumed for RAM : 0.001495 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:39:59] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:39:59] Energy consumed for All CPU : 0.003181 kWh\n[codecarbon INFO @ 12:39:59] Energy consumed for all GPUs : 0.004929 kWh. Total GPU Power : 60.28697790393716 W\n[codecarbon INFO @ 12:39:59] 0.009606 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:39:59] We do not have data for None, using world average.\n[codecarbon INFO @ 12:39:59] 0.016788 g.CO2eq/s mean an estimation of 529.4357083918028 kg.CO2eq/year\nTraining Epoch 5:  75%|███████▍  | 466/625 [00:42<00:14, 11.12it/s, loss=0.339][codecarbon INFO @ 12:40:00] Energy consumed for RAM : 0.001501 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:00] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:00] Energy consumed for All CPU : 0.003193 kWh\n[codecarbon INFO @ 12:40:00] Energy consumed for all GPUs : 0.004947 kWh. Total GPU Power : 66.62500566159981 W\n[codecarbon INFO @ 12:40:00] 0.009641 kWh of electricity used since the beginning.\nTraining Epoch 5:  76%|███████▌  | 476/625 [00:43<00:13, 11.03it/s, loss=0.338][codecarbon INFO @ 12:40:01] Energy consumed for RAM : 0.001506 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:01] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:01] Energy consumed for All CPU : 0.003205 kWh\n[codecarbon INFO @ 12:40:01] Energy consumed for all GPUs : 0.004966 kWh. Total GPU Power : 66.99711225354831 W\n[codecarbon INFO @ 12:40:01] 0.009677 kWh of electricity used since the beginning.\nTraining Epoch 5:  78%|███████▊  | 488/625 [00:44<00:12, 11.02it/s, loss=0.341][codecarbon INFO @ 12:40:02] Energy consumed for RAM : 0.001512 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:02] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:02] Energy consumed for All CPU : 0.003216 kWh\n[codecarbon INFO @ 12:40:02] Energy consumed for all GPUs : 0.004984 kWh. Total GPU Power : 66.37898648334388 W\n[codecarbon INFO @ 12:40:02] 0.009712 kWh of electricity used since the beginning.\nTraining Epoch 5:  80%|███████▉  | 498/625 [00:45<00:11, 11.01it/s, loss=0.34] [codecarbon INFO @ 12:40:03] Energy consumed for RAM : 0.001517 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:03] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:03] Energy consumed for All CPU : 0.003228 kWh\n[codecarbon INFO @ 12:40:03] Energy consumed for all GPUs : 0.005003 kWh. Total GPU Power : 66.63939785692243 W\n[codecarbon INFO @ 12:40:03] 0.009748 kWh of electricity used since the beginning.\nTraining Epoch 5:  82%|████████▏ | 510/625 [00:46<00:10, 11.09it/s, loss=0.339][codecarbon INFO @ 12:40:04] Energy consumed for RAM : 0.001523 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:04] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:04] Energy consumed for All CPU : 0.003240 kWh\n[codecarbon INFO @ 12:40:04] Energy consumed for all GPUs : 0.005021 kWh. Total GPU Power : 66.73241373619784 W\n[codecarbon INFO @ 12:40:04] 0.009784 kWh of electricity used since the beginning.\nTraining Epoch 5:  83%|████████▎ | 520/625 [00:47<00:09, 11.12it/s, loss=0.34] [codecarbon INFO @ 12:40:05] Energy consumed for RAM : 0.001528 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:05] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:05] Energy consumed for All CPU : 0.003252 kWh\n[codecarbon INFO @ 12:40:05] Energy consumed for all GPUs : 0.005039 kWh. Total GPU Power : 66.70362078251341 W\n[codecarbon INFO @ 12:40:05] 0.009819 kWh of electricity used since the beginning.\nTraining Epoch 5:  85%|████████▌ | 532/625 [00:48<00:08, 11.07it/s, loss=0.34][codecarbon INFO @ 12:40:06] Energy consumed for RAM : 0.001534 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:06] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:06] Energy consumed for All CPU : 0.003263 kWh\n[codecarbon INFO @ 12:40:06] Energy consumed for all GPUs : 0.005058 kWh. Total GPU Power : 66.99544951798188 W\n[codecarbon INFO @ 12:40:06] 0.009855 kWh of electricity used since the beginning.\nTraining Epoch 5:  87%|████████▋ | 542/625 [00:49<00:07, 11.08it/s, loss=0.338][codecarbon INFO @ 12:40:07] Energy consumed for RAM : 0.001539 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:07] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:07] Energy consumed for All CPU : 0.003275 kWh\n[codecarbon INFO @ 12:40:07] Energy consumed for all GPUs : 0.005076 kWh. Total GPU Power : 66.78152468381002 W\n[codecarbon INFO @ 12:40:07] 0.009891 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:07] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:07] 0.016896 g.CO2eq/s mean an estimation of 532.8422168053321 kg.CO2eq/year\nTraining Epoch 5:  89%|████████▊ | 554/625 [00:50<00:06, 11.12it/s, loss=0.337][codecarbon INFO @ 12:40:08] Energy consumed for RAM : 0.001545 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:08] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:08] Energy consumed for All CPU : 0.003287 kWh\n[codecarbon INFO @ 12:40:08] Energy consumed for all GPUs : 0.005093 kWh. Total GPU Power : 59.77947497512894 W\n[codecarbon INFO @ 12:40:08] 0.009924 kWh of electricity used since the beginning.\nTraining Epoch 5:  91%|█████████ | 566/625 [00:51<00:05, 11.12it/s, loss=0.336][codecarbon INFO @ 12:40:09] Energy consumed for RAM : 0.001550 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:09] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:09] Energy consumed for All CPU : 0.003298 kWh\n[codecarbon INFO @ 12:40:09] Energy consumed for all GPUs : 0.005111 kWh. Total GPU Power : 66.71097155069226 W\n[codecarbon INFO @ 12:40:09] 0.009960 kWh of electricity used since the beginning.\nTraining Epoch 5:  92%|█████████▏| 576/625 [00:52<00:04, 11.10it/s, loss=0.335][codecarbon INFO @ 12:40:10] Energy consumed for RAM : 0.001556 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:10] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:10] Energy consumed for All CPU : 0.003310 kWh\n[codecarbon INFO @ 12:40:10] Energy consumed for all GPUs : 0.005129 kWh. Total GPU Power : 66.67246569161293 W\n[codecarbon INFO @ 12:40:10] 0.009995 kWh of electricity used since the beginning.\nTraining Epoch 5:  94%|█████████▍| 588/625 [00:53<00:03, 11.09it/s, loss=0.336][codecarbon INFO @ 12:40:11] Energy consumed for RAM : 0.001561 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:11] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:11] Energy consumed for All CPU : 0.003322 kWh\n[codecarbon INFO @ 12:40:11] Energy consumed for all GPUs : 0.005148 kWh. Total GPU Power : 66.94856009057497 W\n[codecarbon INFO @ 12:40:11] 0.010031 kWh of electricity used since the beginning.\nTraining Epoch 5:  96%|█████████▌| 598/625 [00:54<00:02, 11.07it/s, loss=0.337][codecarbon INFO @ 12:40:12] Energy consumed for RAM : 0.001567 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:12] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:12] Energy consumed for All CPU : 0.003333 kWh\n[codecarbon INFO @ 12:40:12] Energy consumed for all GPUs : 0.005166 kWh. Total GPU Power : 66.36404874350104 W\n[codecarbon INFO @ 12:40:12] 0.010066 kWh of electricity used since the beginning.\nTraining Epoch 5:  98%|█████████▊| 610/625 [00:55<00:01, 11.10it/s, loss=0.337][codecarbon INFO @ 12:40:13] Energy consumed for RAM : 0.001572 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:13] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:13] Energy consumed for All CPU : 0.003345 kWh\n[codecarbon INFO @ 12:40:13] Energy consumed for all GPUs : 0.005184 kWh. Total GPU Power : 66.70154293012874 W\n[codecarbon INFO @ 12:40:13] 0.010102 kWh of electricity used since the beginning.\nTraining Epoch 5:  99%|█████████▉| 620/625 [00:56<00:00, 11.04it/s, loss=0.337][codecarbon INFO @ 12:40:14] Energy consumed for RAM : 0.001578 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:14] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:14] Energy consumed for All CPU : 0.003357 kWh\n[codecarbon INFO @ 12:40:14] Energy consumed for all GPUs : 0.005203 kWh. Total GPU Power : 66.5367825899137 W\n[codecarbon INFO @ 12:40:14] 0.010137 kWh of electricity used since the beginning.\nTraining Epoch 5: 100%|██████████| 625/625 [00:56<00:00, 11.07it/s, loss=0.337]\nEvaluating:  44%|████▎     | 24/55 [00:00<00:00, 35.29it/s][codecarbon INFO @ 12:40:15] Energy consumed for RAM : 0.001583 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:15] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:15] Energy consumed for All CPU : 0.003368 kWh\n[codecarbon INFO @ 12:40:15] Energy consumed for all GPUs : 0.005221 kWh. Total GPU Power : 66.51434320482271 W\n[codecarbon INFO @ 12:40:15] 0.010173 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:15] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:15] 0.016760 g.CO2eq/s mean an estimation of 528.5375681080014 kg.CO2eq/year\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"Final evaluation on GLUE sst2...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:   7%|▋         | 4/55 [00:00<00:01, 35.95it/s][codecarbon INFO @ 12:40:16] Energy consumed for RAM : 0.001589 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:16] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:16] Energy consumed for All CPU : 0.003380 kWh\n[codecarbon INFO @ 12:40:16] Energy consumed for all GPUs : 0.005240 kWh. Total GPU Power : 66.61228285970783 W\n[codecarbon INFO @ 12:40:16] 0.010209 kWh of electricity used since the beginning.\nEvaluating:  73%|███████▎  | 40/55 [00:01<00:00, 35.25it/s][codecarbon INFO @ 12:40:17] Energy consumed for RAM : 0.001594 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:17] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:17] Energy consumed for All CPU : 0.003392 kWh\n[codecarbon INFO @ 12:40:17] Energy consumed for all GPUs : 0.005256 kWh. Total GPU Power : 59.765847225404755 W\n[codecarbon INFO @ 12:40:17] 0.010242 kWh of electricity used since the beginning.\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"✓ SST2 Results -> Accuracy: 0.7683, F1: 0.7683\n\n--- Running task: mrpc ---\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   3%|▎         | 6/230 [00:00<00:19, 11.63it/s, loss=0.995][codecarbon INFO @ 12:40:18] Energy consumed for RAM : 0.001600 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:18] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:18] Energy consumed for All CPU : 0.003404 kWh\n[codecarbon INFO @ 12:40:18] Energy consumed for all GPUs : 0.005274 kWh. Total GPU Power : 66.14093163241185 W\n[codecarbon INFO @ 12:40:18] 0.010278 kWh of electricity used since the beginning.\nTraining Epoch 1:   7%|▋         | 16/230 [00:01<00:19, 11.05it/s, loss=0.947][codecarbon INFO @ 12:40:19] Energy consumed for RAM : 0.001605 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:19] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:19] Energy consumed for All CPU : 0.003415 kWh\n[codecarbon INFO @ 12:40:19] Energy consumed for all GPUs : 0.005293 kWh. Total GPU Power : 66.38748509056354 W\n[codecarbon INFO @ 12:40:19] 0.010313 kWh of electricity used since the beginning.\nTraining Epoch 1:  12%|█▏        | 28/230 [00:02<00:18, 11.00it/s, loss=0.872][codecarbon INFO @ 12:40:20] Energy consumed for RAM : 0.001611 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:20] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:20] Energy consumed for All CPU : 0.003427 kWh\n[codecarbon INFO @ 12:40:20] Energy consumed for all GPUs : 0.005311 kWh. Total GPU Power : 66.26063953656828 W\n[codecarbon INFO @ 12:40:20] 0.010349 kWh of electricity used since the beginning.\nTraining Epoch 1:  17%|█▋        | 38/230 [00:03<00:17, 11.01it/s, loss=0.848][codecarbon INFO @ 12:40:21] Energy consumed for RAM : 0.001616 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:21] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:21] Energy consumed for All CPU : 0.003439 kWh\n[codecarbon INFO @ 12:40:21] Energy consumed for all GPUs : 0.005329 kWh. Total GPU Power : 66.57022294382149 W\n[codecarbon INFO @ 12:40:21] 0.010384 kWh of electricity used since the beginning.\nTraining Epoch 1:  22%|██▏       | 50/230 [00:04<00:16, 10.97it/s, loss=0.828][codecarbon INFO @ 12:40:22] Energy consumed for RAM : 0.001622 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:22] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:22] Energy consumed for All CPU : 0.003450 kWh\nTraining Epoch 1:  22%|██▏       | 50/230 [00:04<00:16, 10.97it/s, loss=0.823][codecarbon INFO @ 12:40:22] Energy consumed for all GPUs : 0.005348 kWh. Total GPU Power : 66.6381079267094 W\n[codecarbon INFO @ 12:40:22] 0.010420 kWh of electricity used since the beginning.\nTraining Epoch 1:  26%|██▌       | 60/230 [00:05<00:15, 10.98it/s, loss=0.799][codecarbon INFO @ 12:40:23] Energy consumed for RAM : 0.001627 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:23] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:23] Energy consumed for All CPU : 0.003462 kWh\n[codecarbon INFO @ 12:40:23] Energy consumed for all GPUs : 0.005366 kWh. Total GPU Power : 66.71025998353385 W\n[codecarbon INFO @ 12:40:23] 0.010455 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:23] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:23] 0.016759 g.CO2eq/s mean an estimation of 528.5129371392084 kg.CO2eq/year\nTraining Epoch 1:  31%|███▏      | 72/230 [00:06<00:14, 10.98it/s, loss=0.777][codecarbon INFO @ 12:40:24] Energy consumed for RAM : 0.001633 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:24] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:24] Energy consumed for All CPU : 0.003474 kWh\nTraining Epoch 1:  31%|███▏      | 72/230 [00:06<00:14, 10.98it/s, loss=0.777][codecarbon INFO @ 12:40:24] Energy consumed for all GPUs : 0.005384 kWh. Total GPU Power : 66.98496692977326 W\n[codecarbon INFO @ 12:40:24] 0.010491 kWh of electricity used since the beginning.\nTraining Epoch 1:  36%|███▌      | 82/230 [00:07<00:13, 10.93it/s, loss=0.762][codecarbon INFO @ 12:40:25] Energy consumed for RAM : 0.001638 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:25] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:25] Energy consumed for All CPU : 0.003485 kWh\n[codecarbon INFO @ 12:40:25] Energy consumed for all GPUs : 0.005403 kWh. Total GPU Power : 66.50205581472287 W\n[codecarbon INFO @ 12:40:25] 0.010527 kWh of electricity used since the beginning.\nTraining Epoch 1:  41%|████      | 94/230 [00:08<00:12, 10.99it/s, loss=0.741][codecarbon INFO @ 12:40:26] Energy consumed for RAM : 0.001644 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:26] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:26] Energy consumed for All CPU : 0.003497 kWh\n[codecarbon INFO @ 12:40:26] Energy consumed for all GPUs : 0.005419 kWh. Total GPU Power : 59.654122818771505 W\n[codecarbon INFO @ 12:40:26] 0.010560 kWh of electricity used since the beginning.\nTraining Epoch 1:  45%|████▌     | 104/230 [00:09<00:11, 10.99it/s, loss=0.733][codecarbon INFO @ 12:40:27] Energy consumed for RAM : 0.001649 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:27] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:27] Energy consumed for All CPU : 0.003509 kWh\n[codecarbon INFO @ 12:40:27] Energy consumed for all GPUs : 0.005438 kWh. Total GPU Power : 66.66263327834534 W\n[codecarbon INFO @ 12:40:27] 0.010596 kWh of electricity used since the beginning.\nTraining Epoch 1:  50%|█████     | 116/230 [00:10<00:10, 10.98it/s, loss=0.723][codecarbon INFO @ 12:40:28] Energy consumed for RAM : 0.001655 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:28] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:28] Energy consumed for All CPU : 0.003521 kWh\nTraining Epoch 1:  50%|█████     | 116/230 [00:10<00:10, 10.98it/s, loss=0.723][codecarbon INFO @ 12:40:28] Energy consumed for all GPUs : 0.005456 kWh. Total GPU Power : 66.55581250121193 W\n[codecarbon INFO @ 12:40:28] 0.010631 kWh of electricity used since the beginning.\nTraining Epoch 1:  55%|█████▍    | 126/230 [00:11<00:09, 10.99it/s, loss=0.715][codecarbon INFO @ 12:40:29] Energy consumed for RAM : 0.001660 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:29] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:29] Energy consumed for All CPU : 0.003532 kWh\n[codecarbon INFO @ 12:40:29] Energy consumed for all GPUs : 0.005474 kWh. Total GPU Power : 66.87551924411922 W\n[codecarbon INFO @ 12:40:29] 0.010667 kWh of electricity used since the beginning.\nTraining Epoch 1:  60%|██████    | 138/230 [00:12<00:08, 11.00it/s, loss=0.709][codecarbon INFO @ 12:40:30] Energy consumed for RAM : 0.001666 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:30] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:30] Energy consumed for All CPU : 0.003544 kWh\n[codecarbon INFO @ 12:40:30] Energy consumed for all GPUs : 0.005493 kWh. Total GPU Power : 66.3832161303921 W\n[codecarbon INFO @ 12:40:30] 0.010703 kWh of electricity used since the beginning.\nTraining Epoch 1:  64%|██████▍   | 148/230 [00:13<00:07, 10.96it/s, loss=0.704][codecarbon INFO @ 12:40:31] Energy consumed for RAM : 0.001671 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:31] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:31] Energy consumed for All CPU : 0.003556 kWh\n[codecarbon INFO @ 12:40:31] Energy consumed for all GPUs : 0.005511 kWh. Total GPU Power : 67.16637232141963 W\n[codecarbon INFO @ 12:40:31] 0.010738 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:31] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:31] 0.016786 g.CO2eq/s mean an estimation of 529.3549278096317 kg.CO2eq/year\nTraining Epoch 1:  70%|██████▉   | 160/230 [00:14<00:06, 10.97it/s, loss=0.7]  [codecarbon INFO @ 12:40:32] Energy consumed for RAM : 0.001677 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:32] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:32] Energy consumed for All CPU : 0.003567 kWh\n[codecarbon INFO @ 12:40:32] Energy consumed for all GPUs : 0.005529 kWh. Total GPU Power : 66.3919063319175 W\n[codecarbon INFO @ 12:40:32] 0.010774 kWh of electricity used since the beginning.\nTraining Epoch 1:  74%|███████▍  | 170/230 [00:15<00:05, 10.96it/s, loss=0.696][codecarbon INFO @ 12:40:33] Energy consumed for RAM : 0.001682 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:33] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:33] Energy consumed for All CPU : 0.003579 kWh\n[codecarbon INFO @ 12:40:33] Energy consumed for all GPUs : 0.005548 kWh. Total GPU Power : 66.62976239668316 W\n[codecarbon INFO @ 12:40:33] 0.010809 kWh of electricity used since the beginning.\nTraining Epoch 1:  79%|███████▉  | 182/230 [00:16<00:04, 10.99it/s, loss=0.69] [codecarbon INFO @ 12:40:34] Energy consumed for RAM : 0.001688 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:34] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:34] Energy consumed for All CPU : 0.003591 kWh\n[codecarbon INFO @ 12:40:34] Energy consumed for all GPUs : 0.005566 kWh. Total GPU Power : 66.58239786239089 W\n[codecarbon INFO @ 12:40:34] 0.010845 kWh of electricity used since the beginning.\nTraining Epoch 1:  83%|████████▎ | 192/230 [00:17<00:03, 10.99it/s, loss=0.687][codecarbon INFO @ 12:40:35] Energy consumed for RAM : 0.001693 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:35] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:35] Energy consumed for All CPU : 0.003602 kWh\n[codecarbon INFO @ 12:40:35] Energy consumed for all GPUs : 0.005583 kWh. Total GPU Power : 60.04831187972655 W\n[codecarbon INFO @ 12:40:35] 0.010879 kWh of electricity used since the beginning.\nTraining Epoch 1:  89%|████████▊ | 204/230 [00:18<00:02, 10.99it/s, loss=0.683][codecarbon INFO @ 12:40:36] Energy consumed for RAM : 0.001699 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:36] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:36] Energy consumed for All CPU : 0.003614 kWh\n[codecarbon INFO @ 12:40:36] Energy consumed for all GPUs : 0.005601 kWh. Total GPU Power : 66.92048354421686 W\n[codecarbon INFO @ 12:40:36] 0.010914 kWh of electricity used since the beginning.\nTraining Epoch 1:  93%|█████████▎| 214/230 [00:19<00:01, 10.98it/s, loss=0.679][codecarbon INFO @ 12:40:37] Energy consumed for RAM : 0.001704 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:37] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:37] Energy consumed for All CPU : 0.003626 kWh\n[codecarbon INFO @ 12:40:37] Energy consumed for all GPUs : 0.005620 kWh. Total GPU Power : 66.68534217445821 W\n[codecarbon INFO @ 12:40:37] 0.010950 kWh of electricity used since the beginning.\nTraining Epoch 1:  98%|█████████▊| 226/230 [00:20<00:00, 10.96it/s, loss=0.677][codecarbon INFO @ 12:40:38] Energy consumed for RAM : 0.001710 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:38] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:38] Energy consumed for All CPU : 0.003638 kWh\n[codecarbon INFO @ 12:40:38] Energy consumed for all GPUs : 0.005638 kWh. Total GPU Power : 66.61053289378879 W\n[codecarbon INFO @ 12:40:38] 0.010985 kWh of electricity used since the beginning.\nTraining Epoch 1: 100%|██████████| 230/230 [00:20<00:00, 11.01it/s, loss=0.676]\nEvaluating:  92%|█████████▏| 24/26 [00:00<00:00, 35.13it/s][codecarbon INFO @ 12:40:39] Energy consumed for RAM : 0.001715 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:39] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:39] Energy consumed for All CPU : 0.003649 kWh\n[codecarbon INFO @ 12:40:39] Energy consumed for all GPUs : 0.005656 kWh. Total GPU Power : 66.52631688021927 W\n[codecarbon INFO @ 12:40:39] 0.011021 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:39] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:39] 0.016773 g.CO2eq/s mean an estimation of 528.9506234980593 kg.CO2eq/year\nTraining Epoch 2:   4%|▍         | 10/230 [00:00<00:20, 10.95it/s, loss=0.595][codecarbon INFO @ 12:40:40] Energy consumed for RAM : 0.001721 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:40] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:40] Energy consumed for All CPU : 0.003661 kWh\n[codecarbon INFO @ 12:40:40] Energy consumed for all GPUs : 0.005675 kWh. Total GPU Power : 66.82691716492674 W\n[codecarbon INFO @ 12:40:40] 0.011056 kWh of electricity used since the beginning.\nTraining Epoch 2:   9%|▊         | 20/230 [00:01<00:19, 10.96it/s, loss=0.591][codecarbon INFO @ 12:40:41] Energy consumed for RAM : 0.001726 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:41] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:41] Energy consumed for All CPU : 0.003672 kWh\n[codecarbon INFO @ 12:40:41] Energy consumed for all GPUs : 0.005693 kWh. Total GPU Power : 67.25653508910757 W\n[codecarbon INFO @ 12:40:41] 0.011092 kWh of electricity used since the beginning.\nTraining Epoch 2:  14%|█▍        | 32/230 [00:02<00:18, 10.96it/s, loss=0.591][codecarbon INFO @ 12:40:42] Energy consumed for RAM : 0.001732 kWh. RAM Power : 20.0 W\nTraining Epoch 2:  14%|█▍        | 32/230 [00:02<00:18, 10.96it/s, loss=0.591][codecarbon INFO @ 12:40:42] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:42] Energy consumed for All CPU : 0.003684 kWh\n[codecarbon INFO @ 12:40:42] Energy consumed for all GPUs : 0.005711 kWh. Total GPU Power : 66.84911749123813 W\n[codecarbon INFO @ 12:40:42] 0.011128 kWh of electricity used since the beginning.\nTraining Epoch 2:  18%|█▊        | 42/230 [00:03<00:17, 10.94it/s, loss=0.585][codecarbon INFO @ 12:40:43] Energy consumed for RAM : 0.001737 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:43] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:43] Energy consumed for All CPU : 0.003696 kWh\n[codecarbon INFO @ 12:40:43] Energy consumed for all GPUs : 0.005728 kWh. Total GPU Power : 60.2008608586291 W\n[codecarbon INFO @ 12:40:43] 0.011161 kWh of electricity used since the beginning.\nTraining Epoch 2:  23%|██▎       | 54/230 [00:04<00:16, 10.94it/s, loss=0.599][codecarbon INFO @ 12:40:44] Energy consumed for RAM : 0.001743 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:44] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:44] Energy consumed for All CPU : 0.003708 kWh\n[codecarbon INFO @ 12:40:44] Energy consumed for all GPUs : 0.005747 kWh. Total GPU Power : 67.3048559613575 W\nTraining Epoch 2:  23%|██▎       | 54/230 [00:04<00:16, 10.94it/s, loss=0.599][codecarbon INFO @ 12:40:44] 0.011197 kWh of electricity used since the beginning.\nTraining Epoch 2:  28%|██▊       | 64/230 [00:05<00:15, 10.97it/s, loss=0.608][codecarbon INFO @ 12:40:45] Energy consumed for RAM : 0.001748 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:45] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:45] Energy consumed for All CPU : 0.003719 kWh\n[codecarbon INFO @ 12:40:45] Energy consumed for all GPUs : 0.005765 kWh. Total GPU Power : 67.04638444845034 W\n[codecarbon INFO @ 12:40:45] 0.011233 kWh of electricity used since the beginning.\nTraining Epoch 2:  33%|███▎      | 76/230 [00:06<00:14, 10.97it/s, loss=0.609][codecarbon INFO @ 12:40:46] Energy consumed for RAM : 0.001754 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:46] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:46] Energy consumed for All CPU : 0.003731 kWh\n[codecarbon INFO @ 12:40:46] Energy consumed for all GPUs : 0.005783 kWh. Total GPU Power : 66.36823972511291 W\n[codecarbon INFO @ 12:40:46] 0.011268 kWh of electricity used since the beginning.\nTraining Epoch 2:  37%|███▋      | 86/230 [00:07<00:13, 10.97it/s, loss=0.611][codecarbon INFO @ 12:40:47] Energy consumed for RAM : 0.001759 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:47] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:47] Energy consumed for All CPU : 0.003743 kWh\n[codecarbon INFO @ 12:40:47] Energy consumed for all GPUs : 0.005802 kWh. Total GPU Power : 66.6886820723696 W\n[codecarbon INFO @ 12:40:47] 0.011304 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:47] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:47] 0.016800 g.CO2eq/s mean an estimation of 529.7923368126378 kg.CO2eq/year\nTraining Epoch 2:  43%|████▎     | 98/230 [00:08<00:12, 10.95it/s, loss=0.611][codecarbon INFO @ 12:40:48] Energy consumed for RAM : 0.001765 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:48] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:48] Energy consumed for All CPU : 0.003754 kWh\n[codecarbon INFO @ 12:40:48] Energy consumed for all GPUs : 0.005820 kWh. Total GPU Power : 66.67823127394912 W\n[codecarbon INFO @ 12:40:48] 0.011339 kWh of electricity used since the beginning.\nTraining Epoch 2:  47%|████▋     | 108/230 [00:09<00:11, 10.96it/s, loss=0.611][codecarbon INFO @ 12:40:49] Energy consumed for RAM : 0.001770 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:49] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:49] Energy consumed for All CPU : 0.003766 kWh\n[codecarbon INFO @ 12:40:49] Energy consumed for all GPUs : 0.005839 kWh. Total GPU Power : 66.9983513213295 W\n[codecarbon INFO @ 12:40:49] 0.011375 kWh of electricity used since the beginning.\nTraining Epoch 2:  52%|█████▏    | 120/230 [00:10<00:10, 10.95it/s, loss=0.614][codecarbon INFO @ 12:40:50] Energy consumed for RAM : 0.001776 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:50] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:50] Energy consumed for All CPU : 0.003778 kWh\n[codecarbon INFO @ 12:40:50] Energy consumed for all GPUs : 0.005857 kWh. Total GPU Power : 66.9683953446862 W\n[codecarbon INFO @ 12:40:50] 0.011411 kWh of electricity used since the beginning.\nTraining Epoch 2:  57%|█████▋    | 130/230 [00:11<00:09, 10.96it/s, loss=0.614][codecarbon INFO @ 12:40:51] Energy consumed for RAM : 0.001781 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:51] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:51] Energy consumed for All CPU : 0.003789 kWh\n[codecarbon INFO @ 12:40:51] Energy consumed for all GPUs : 0.005874 kWh. Total GPU Power : 60.41675527377338 W\n[codecarbon INFO @ 12:40:51] 0.011444 kWh of electricity used since the beginning.\nTraining Epoch 2:  62%|██████▏   | 142/230 [00:12<00:08, 10.96it/s, loss=0.614][codecarbon INFO @ 12:40:52] Energy consumed for RAM : 0.001787 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:52] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:52] Energy consumed for All CPU : 0.003801 kWh\n[codecarbon INFO @ 12:40:52] Energy consumed for all GPUs : 0.005892 kWh. Total GPU Power : 66.70756545318656 W\n[codecarbon INFO @ 12:40:52] 0.011480 kWh of electricity used since the beginning.\nTraining Epoch 2:  66%|██████▌   | 152/230 [00:13<00:07, 10.98it/s, loss=0.613][codecarbon INFO @ 12:40:53] Energy consumed for RAM : 0.001792 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:53] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:53] Energy consumed for All CPU : 0.003813 kWh\n[codecarbon INFO @ 12:40:53] Energy consumed for all GPUs : 0.005911 kWh. Total GPU Power : 66.87575317647031 W\n[codecarbon INFO @ 12:40:53] 0.011516 kWh of electricity used since the beginning.\nTraining Epoch 2:  71%|███████▏  | 164/230 [00:14<00:06, 10.96it/s, loss=0.614][codecarbon INFO @ 12:40:54] Energy consumed for RAM : 0.001798 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:54] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:54] Energy consumed for All CPU : 0.003824 kWh\n[codecarbon INFO @ 12:40:54] Energy consumed for all GPUs : 0.005929 kWh. Total GPU Power : 67.07392822495389 W\n[codecarbon INFO @ 12:40:54] 0.011551 kWh of electricity used since the beginning.\nTraining Epoch 2:  76%|███████▌  | 174/230 [00:15<00:05, 10.96it/s, loss=0.61] [codecarbon INFO @ 12:40:55] Energy consumed for RAM : 0.001803 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:55] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:55] Energy consumed for All CPU : 0.003836 kWh\n[codecarbon INFO @ 12:40:55] Energy consumed for all GPUs : 0.005947 kWh. Total GPU Power : 66.7337692710847 W\n[codecarbon INFO @ 12:40:55] 0.011587 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:40:55] We do not have data for None, using world average.\n[codecarbon INFO @ 12:40:55] 0.016807 g.CO2eq/s mean an estimation of 530.0290217030912 kg.CO2eq/year\nTraining Epoch 2:  81%|████████  | 186/230 [00:16<00:04, 10.96it/s, loss=0.609][codecarbon INFO @ 12:40:56] Energy consumed for RAM : 0.001809 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:56] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:56] Energy consumed for All CPU : 0.003848 kWh\n[codecarbon INFO @ 12:40:56] Energy consumed for all GPUs : 0.005966 kWh. Total GPU Power : 66.51987995712334 W\n[codecarbon INFO @ 12:40:56] 0.011622 kWh of electricity used since the beginning.\nTraining Epoch 2:  85%|████████▌ | 196/230 [00:17<00:03, 10.95it/s, loss=0.607][codecarbon INFO @ 12:40:57] Energy consumed for RAM : 0.001814 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:57] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:57] Energy consumed for All CPU : 0.003860 kWh\n[codecarbon INFO @ 12:40:57] Energy consumed for all GPUs : 0.005984 kWh. Total GPU Power : 66.4395156638084 W\n[codecarbon INFO @ 12:40:57] 0.011658 kWh of electricity used since the beginning.\nTraining Epoch 2:  90%|█████████ | 208/230 [00:18<00:02, 10.97it/s, loss=0.605][codecarbon INFO @ 12:40:58] Energy consumed for RAM : 0.001820 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:58] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:58] Energy consumed for All CPU : 0.003871 kWh\n[codecarbon INFO @ 12:40:58] Energy consumed for all GPUs : 0.006001 kWh. Total GPU Power : 59.76391451329884 W\n[codecarbon INFO @ 12:40:58] 0.011692 kWh of electricity used since the beginning.\nTraining Epoch 2:  95%|█████████▍| 218/230 [00:19<00:01, 10.98it/s, loss=0.608][codecarbon INFO @ 12:40:59] Energy consumed for RAM : 0.001825 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:40:59] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:40:59] Energy consumed for All CPU : 0.003883 kWh\n[codecarbon INFO @ 12:40:59] Energy consumed for all GPUs : 0.006019 kWh. Total GPU Power : 66.17495187431008 W\n[codecarbon INFO @ 12:40:59] 0.011727 kWh of electricity used since the beginning.\nTraining Epoch 2: 100%|██████████| 230/230 [00:20<00:00, 10.99it/s, loss=0.609]\nEvaluating:   0%|          | 0/26 [00:00<?, ?it/s][codecarbon INFO @ 12:41:00] Energy consumed for RAM : 0.001831 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:00] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:00] Energy consumed for All CPU : 0.003895 kWh\n[codecarbon INFO @ 12:41:00] Energy consumed for all GPUs : 0.006037 kWh. Total GPU Power : 66.19193973657727 W\n[codecarbon INFO @ 12:41:00] 0.011763 kWh of electricity used since the beginning.\nTraining Epoch 3:   1%|          | 2/230 [00:00<00:20, 11.13it/s, loss=0.634][codecarbon INFO @ 12:41:01] Energy consumed for RAM : 0.001836 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:01] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:01] Energy consumed for All CPU : 0.003906 kWh\n[codecarbon INFO @ 12:41:01] Energy consumed for all GPUs : 0.006055 kWh. Total GPU Power : 66.53010413901889 W\n[codecarbon INFO @ 12:41:01] 0.011798 kWh of electricity used since the beginning.\nTraining Epoch 3:   6%|▌         | 14/230 [00:01<00:19, 11.04it/s, loss=0.591][codecarbon INFO @ 12:41:02] Energy consumed for RAM : 0.001842 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:02] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\nTraining Epoch 3:   6%|▌         | 14/230 [00:01<00:19, 11.04it/s, loss=0.598][codecarbon INFO @ 12:41:02] Energy consumed for All CPU : 0.003918 kWh\n[codecarbon INFO @ 12:41:02] Energy consumed for all GPUs : 0.006074 kWh. Total GPU Power : 66.53031723931332 W\n[codecarbon INFO @ 12:41:02] 0.011834 kWh of electricity used since the beginning.\nTraining Epoch 3:  10%|█         | 24/230 [00:02<00:18, 10.99it/s, loss=0.577][codecarbon INFO @ 12:41:03] Energy consumed for RAM : 0.001847 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:03] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:03] Energy consumed for All CPU : 0.003930 kWh\n[codecarbon INFO @ 12:41:03] Energy consumed for all GPUs : 0.006092 kWh. Total GPU Power : 66.50069878074362 W\n[codecarbon INFO @ 12:41:03] 0.011869 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:41:03] We do not have data for None, using world average.\n[codecarbon INFO @ 12:41:03] 0.016756 g.CO2eq/s mean an estimation of 528.4264328965411 kg.CO2eq/year\nTraining Epoch 3:  16%|█▌        | 36/230 [00:03<00:17, 11.00it/s, loss=0.586][codecarbon INFO @ 12:41:04] Energy consumed for RAM : 0.001853 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:04] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:04] Energy consumed for All CPU : 0.003942 kWh\nTraining Epoch 3:  16%|█▌        | 36/230 [00:03<00:17, 11.00it/s, loss=0.59] [codecarbon INFO @ 12:41:04] Energy consumed for all GPUs : 0.006110 kWh. Total GPU Power : 66.44920406842941 W\n[codecarbon INFO @ 12:41:04] 0.011905 kWh of electricity used since the beginning.\nTraining Epoch 3:  20%|██        | 46/230 [00:04<00:16, 10.97it/s, loss=0.6]  [codecarbon INFO @ 12:41:05] Energy consumed for RAM : 0.001858 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:05] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:05] Energy consumed for All CPU : 0.003953 kWh\n[codecarbon INFO @ 12:41:05] Energy consumed for all GPUs : 0.006129 kWh. Total GPU Power : 66.55169556996871 W\n[codecarbon INFO @ 12:41:05] 0.011940 kWh of electricity used since the beginning.\nTraining Epoch 3:  25%|██▌       | 58/230 [00:05<00:15, 10.99it/s, loss=0.6]  [codecarbon INFO @ 12:41:06] Energy consumed for RAM : 0.001864 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:06] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:06] Energy consumed for All CPU : 0.003965 kWh\nTraining Epoch 3:  25%|██▌       | 58/230 [00:05<00:15, 10.99it/s, loss=0.599][codecarbon INFO @ 12:41:06] Energy consumed for all GPUs : 0.006147 kWh. Total GPU Power : 66.71924909716012 W\n[codecarbon INFO @ 12:41:06] 0.011976 kWh of electricity used since the beginning.\nTraining Epoch 3:  30%|██▉       | 68/230 [00:06<00:14, 11.00it/s, loss=0.592][codecarbon INFO @ 12:41:07] Energy consumed for RAM : 0.001869 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:07] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:07] Energy consumed for All CPU : 0.003977 kWh\n[codecarbon INFO @ 12:41:07] Energy consumed for all GPUs : 0.006165 kWh. Total GPU Power : 66.40131614385471 W\n[codecarbon INFO @ 12:41:07] 0.012011 kWh of electricity used since the beginning.\nTraining Epoch 3:  35%|███▍      | 80/230 [00:07<00:13, 11.00it/s, loss=0.591][codecarbon INFO @ 12:41:08] Energy consumed for RAM : 0.001875 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:08] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:08] Energy consumed for All CPU : 0.003988 kWh\n[codecarbon INFO @ 12:41:08] Energy consumed for all GPUs : 0.006182 kWh. Total GPU Power : 59.80407600202834 W\nTraining Epoch 3:  35%|███▍      | 80/230 [00:07<00:13, 11.00it/s, loss=0.59] [codecarbon INFO @ 12:41:08] 0.012045 kWh of electricity used since the beginning.\nTraining Epoch 3:  39%|███▉      | 90/230 [00:08<00:12, 10.98it/s, loss=0.592][codecarbon INFO @ 12:41:09] Energy consumed for RAM : 0.001880 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:09] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:09] Energy consumed for All CPU : 0.004000 kWh\n[codecarbon INFO @ 12:41:09] Energy consumed for all GPUs : 0.006200 kWh. Total GPU Power : 66.67257983178067 W\n[codecarbon INFO @ 12:41:09] 0.012081 kWh of electricity used since the beginning.\nTraining Epoch 3:  44%|████▍     | 102/230 [00:09<00:11, 10.98it/s, loss=0.593][codecarbon INFO @ 12:41:10] Energy consumed for RAM : 0.001886 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:10] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:10] Energy consumed for All CPU : 0.004012 kWh\n[codecarbon INFO @ 12:41:10] Energy consumed for all GPUs : 0.006219 kWh. Total GPU Power : 66.45327072217269 W\n[codecarbon INFO @ 12:41:10] 0.012116 kWh of electricity used since the beginning.\nTraining Epoch 3:  49%|████▊     | 112/230 [00:10<00:10, 10.97it/s, loss=0.592][codecarbon INFO @ 12:41:11] Energy consumed for RAM : 0.001891 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:11] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:11] Energy consumed for All CPU : 0.004023 kWh\n[codecarbon INFO @ 12:41:11] Energy consumed for all GPUs : 0.006237 kWh. Total GPU Power : 66.50518806086404 W\n[codecarbon INFO @ 12:41:11] 0.012152 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:41:11] We do not have data for None, using world average.\n[codecarbon INFO @ 12:41:11] 0.016763 g.CO2eq/s mean an estimation of 528.6312263203424 kg.CO2eq/year\nTraining Epoch 3:  54%|█████▍    | 124/230 [00:11<00:09, 11.00it/s, loss=0.594][codecarbon INFO @ 12:41:12] Energy consumed for RAM : 0.001897 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:12] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:12] Energy consumed for All CPU : 0.004035 kWh\n[codecarbon INFO @ 12:41:12] Energy consumed for all GPUs : 0.006255 kWh. Total GPU Power : 66.20790916195432 W\n[codecarbon INFO @ 12:41:12] 0.012187 kWh of electricity used since the beginning.\nTraining Epoch 3:  58%|█████▊    | 134/230 [00:12<00:08, 10.95it/s, loss=0.593][codecarbon INFO @ 12:41:13] Energy consumed for RAM : 0.001902 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:13] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:13] Energy consumed for All CPU : 0.004047 kWh\n[codecarbon INFO @ 12:41:13] Energy consumed for all GPUs : 0.006274 kWh. Total GPU Power : 67.03653972891952 W\n[codecarbon INFO @ 12:41:13] 0.012223 kWh of electricity used since the beginning.\nTraining Epoch 3:  63%|██████▎   | 146/230 [00:13<00:07, 11.01it/s, loss=0.59] [codecarbon INFO @ 12:41:14] Energy consumed for RAM : 0.001908 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:14] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:14] Energy consumed for All CPU : 0.004058 kWh\n[codecarbon INFO @ 12:41:14] Energy consumed for all GPUs : 0.006292 kWh. Total GPU Power : 66.2238674449678 W\n[codecarbon INFO @ 12:41:14] 0.012258 kWh of electricity used since the beginning.\nTraining Epoch 3:  68%|██████▊   | 156/230 [00:14<00:06, 10.98it/s, loss=0.59] [codecarbon INFO @ 12:41:15] Energy consumed for RAM : 0.001913 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:15] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:15] Energy consumed for All CPU : 0.004070 kWh\n[codecarbon INFO @ 12:41:15] Energy consumed for all GPUs : 0.006310 kWh. Total GPU Power : 66.40927709742601 W\n[codecarbon INFO @ 12:41:15] 0.012294 kWh of electricity used since the beginning.\nTraining Epoch 3:  73%|███████▎  | 168/230 [00:15<00:05, 10.96it/s, loss=0.585][codecarbon INFO @ 12:41:16] Energy consumed for RAM : 0.001919 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:16] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:16] Energy consumed for All CPU : 0.004082 kWh\n[codecarbon INFO @ 12:41:16] Energy consumed for all GPUs : 0.006329 kWh. Total GPU Power : 66.39684252627217 W\n[codecarbon INFO @ 12:41:16] 0.012329 kWh of electricity used since the beginning.\nTraining Epoch 3:  77%|███████▋  | 178/230 [00:16<00:04, 10.99it/s, loss=0.585][codecarbon INFO @ 12:41:17] Energy consumed for RAM : 0.001924 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:17] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:17] Energy consumed for All CPU : 0.004094 kWh\n[codecarbon INFO @ 12:41:17] Energy consumed for all GPUs : 0.006345 kWh. Total GPU Power : 59.812934423606215 W\n[codecarbon INFO @ 12:41:17] 0.012363 kWh of electricity used since the beginning.\nTraining Epoch 3:  83%|████████▎ | 190/230 [00:17<00:03, 10.99it/s, loss=0.588][codecarbon INFO @ 12:41:18] Energy consumed for RAM : 0.001930 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:18] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:18] Energy consumed for All CPU : 0.004105 kWh\n[codecarbon INFO @ 12:41:18] Energy consumed for all GPUs : 0.006363 kWh. Total GPU Power : 66.42371515701706 W\n[codecarbon INFO @ 12:41:18] 0.012398 kWh of electricity used since the beginning.\nTraining Epoch 3:  87%|████████▋ | 200/230 [00:18<00:02, 10.96it/s, loss=0.59] [codecarbon INFO @ 12:41:19] Energy consumed for RAM : 0.001935 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:19] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:19] Energy consumed for All CPU : 0.004117 kWh\n[codecarbon INFO @ 12:41:19] Energy consumed for all GPUs : 0.006382 kWh. Total GPU Power : 66.2274121364538 W\n[codecarbon INFO @ 12:41:19] 0.012434 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:41:19] We do not have data for None, using world average.\n[codecarbon INFO @ 12:41:19] 0.016759 g.CO2eq/s mean an estimation of 528.5036448893626 kg.CO2eq/year\nTraining Epoch 3:  92%|█████████▏| 212/230 [00:19<00:01, 11.01it/s, loss=0.591][codecarbon INFO @ 12:41:20] Energy consumed for RAM : 0.001941 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:20] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:20] Energy consumed for All CPU : 0.004129 kWh\n[codecarbon INFO @ 12:41:20] Energy consumed for all GPUs : 0.006400 kWh. Total GPU Power : 66.46626876415722 W\n[codecarbon INFO @ 12:41:20] 0.012470 kWh of electricity used since the beginning.\nTraining Epoch 3:  97%|█████████▋| 222/230 [00:20<00:00, 10.98it/s, loss=0.594][codecarbon INFO @ 12:41:21] Energy consumed for RAM : 0.001946 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:21] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:21] Energy consumed for All CPU : 0.004140 kWh\n[codecarbon INFO @ 12:41:21] Energy consumed for all GPUs : 0.006418 kWh. Total GPU Power : 66.38920026934923 W\n[codecarbon INFO @ 12:41:21] 0.012505 kWh of electricity used since the beginning.\nTraining Epoch 3: 100%|██████████| 230/230 [00:20<00:00, 11.01it/s, loss=0.593]\nEvaluating:  46%|████▌     | 12/26 [00:00<00:00, 35.45it/s][codecarbon INFO @ 12:41:22] Energy consumed for RAM : 0.001952 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:22] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:22] Energy consumed for All CPU : 0.004152 kWh\nEvaluating:  62%|██████▏   | 16/26 [00:00<00:00, 35.00it/s][codecarbon INFO @ 12:41:22] Energy consumed for all GPUs : 0.006437 kWh. Total GPU Power : 66.20112607367997 W\n[codecarbon INFO @ 12:41:22] 0.012541 kWh of electricity used since the beginning.\nTraining Epoch 4:   3%|▎         | 6/230 [00:00<00:20, 10.91it/s, loss=0.593][codecarbon INFO @ 12:41:23] Energy consumed for RAM : 0.001957 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:23] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:23] Energy consumed for All CPU : 0.004164 kWh\n[codecarbon INFO @ 12:41:23] Energy consumed for all GPUs : 0.006455 kWh. Total GPU Power : 66.61451039649354 W\n[codecarbon INFO @ 12:41:23] 0.012576 kWh of electricity used since the beginning.\nTraining Epoch 4:   8%|▊         | 18/230 [00:01<00:19, 10.93it/s, loss=0.594][codecarbon INFO @ 12:41:24] Energy consumed for RAM : 0.001963 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:24] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:24] Energy consumed for All CPU : 0.004176 kWh\n[codecarbon INFO @ 12:41:24] Energy consumed for all GPUs : 0.006473 kWh. Total GPU Power : 66.56226479375883 W\n[codecarbon INFO @ 12:41:24] 0.012612 kWh of electricity used since the beginning.\nTraining Epoch 4:  12%|█▏        | 28/230 [00:02<00:18, 10.97it/s, loss=0.592][codecarbon INFO @ 12:41:25] Energy consumed for RAM : 0.001968 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:25] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:25] Energy consumed for All CPU : 0.004187 kWh\n[codecarbon INFO @ 12:41:25] Energy consumed for all GPUs : 0.006492 kWh. Total GPU Power : 66.55503531676602 W\n[codecarbon INFO @ 12:41:25] 0.012647 kWh of electricity used since the beginning.\nTraining Epoch 4:  17%|█▋        | 40/230 [00:03<00:17, 11.01it/s, loss=0.587][codecarbon INFO @ 12:41:26] Energy consumed for RAM : 0.001974 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:26] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:26] Energy consumed for All CPU : 0.004199 kWh\n[codecarbon INFO @ 12:41:26] Energy consumed for all GPUs : 0.006508 kWh. Total GPU Power : 59.87098721280981 W\n[codecarbon INFO @ 12:41:26] 0.012681 kWh of electricity used since the beginning.\nTraining Epoch 4:  22%|██▏       | 50/230 [00:04<00:16, 10.97it/s, loss=0.59] [codecarbon INFO @ 12:41:27] Energy consumed for RAM : 0.001979 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:27] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:27] Energy consumed for All CPU : 0.004211 kWh\n[codecarbon INFO @ 12:41:27] Energy consumed for all GPUs : 0.006527 kWh. Total GPU Power : 66.8893187420488 W\n[codecarbon INFO @ 12:41:27] 0.012716 kWh of electricity used since the beginning.\n[codecarbon WARNING @ 12:41:27] We do not have data for None, using world average.\n[codecarbon INFO @ 12:41:27] 0.016762 g.CO2eq/s mean an estimation of 528.6084823635376 kg.CO2eq/year\nTraining Epoch 4:  27%|██▋       | 62/230 [00:05<00:15, 10.96it/s, loss=0.587][codecarbon INFO @ 12:41:28] Energy consumed for RAM : 0.001985 kWh. RAM Power : 20.0 W\nTraining Epoch 4:  27%|██▋       | 62/230 [00:05<00:15, 10.96it/s, loss=0.588][codecarbon INFO @ 12:41:28] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:28] Energy consumed for All CPU : 0.004222 kWh\n[codecarbon INFO @ 12:41:28] Energy consumed for all GPUs : 0.006545 kWh. Total GPU Power : 66.23490383493223 W\n[codecarbon INFO @ 12:41:28] 0.012752 kWh of electricity used since the beginning.\nTraining Epoch 4:  31%|███▏      | 72/230 [00:06<00:14, 11.00it/s, loss=0.59] [codecarbon INFO @ 12:41:29] Energy consumed for RAM : 0.001990 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:29] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:29] Energy consumed for All CPU : 0.004234 kWh\n[codecarbon INFO @ 12:41:29] Energy consumed for all GPUs : 0.006563 kWh. Total GPU Power : 66.63535587576126 W\n[codecarbon INFO @ 12:41:29] 0.012787 kWh of electricity used since the beginning.\nTraining Epoch 4:  37%|███▋      | 84/230 [00:07<00:13, 10.99it/s, loss=0.592][codecarbon INFO @ 12:41:30] Energy consumed for RAM : 0.001996 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:30] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\nTraining Epoch 4:  37%|███▋      | 84/230 [00:07<00:13, 10.99it/s, loss=0.592][codecarbon INFO @ 12:41:30] Energy consumed for All CPU : 0.004246 kWh\n[codecarbon INFO @ 12:41:30] Energy consumed for all GPUs : 0.006581 kWh. Total GPU Power : 66.33989682834343 W\n[codecarbon INFO @ 12:41:30] 0.012823 kWh of electricity used since the beginning.\nTraining Epoch 4:  41%|████      | 94/230 [00:08<00:12, 10.97it/s, loss=0.593][codecarbon INFO @ 12:41:31] Energy consumed for RAM : 0.002001 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:31] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:31] Energy consumed for All CPU : 0.004257 kWh\n[codecarbon INFO @ 12:41:31] Energy consumed for all GPUs : 0.006600 kWh. Total GPU Power : 66.65728090061296 W\n[codecarbon INFO @ 12:41:31] 0.012859 kWh of electricity used since the beginning.\nTraining Epoch 4:  46%|████▌     | 106/230 [00:09<00:11, 10.98it/s, loss=0.588][codecarbon INFO @ 12:41:32] Energy consumed for RAM : 0.002007 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:32] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:32] Energy consumed for All CPU : 0.004269 kWh\nTraining Epoch 4:  46%|████▌     | 106/230 [00:09<00:11, 10.98it/s, loss=0.588][codecarbon INFO @ 12:41:32] Energy consumed for all GPUs : 0.006618 kWh. Total GPU Power : 66.9501857648665 W\n[codecarbon INFO @ 12:41:32] 0.012894 kWh of electricity used since the beginning.\nTraining Epoch 4:  50%|█████     | 116/230 [00:10<00:10, 10.99it/s, loss=0.585][codecarbon INFO @ 12:41:33] Energy consumed for RAM : 0.002012 kWh. RAM Power : 20.0 W\n[codecarbon INFO @ 12:41:33] Delta energy consumed for CPU with constant : 0.000012 kWh, power : 42.5 W\n[codecarbon INFO @ 12:41:33] Energy consumed for All CPU : 0.004281 kWh\n[codecarbon INFO @ 12:41:33] Energy consumed for all GPUs : 0.006637 kWh. Total GPU Power : 66.92272406803596 W\n","output_type":"stream"}],"execution_count":null}]}